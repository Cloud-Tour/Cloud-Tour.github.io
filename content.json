{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://Cloud-Tour.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-10-14T13:41:36.430Z","updated":"2022-10-06T02:48:58.718Z","comments":false,"path":"/404.html","permalink":"http://cloud-tour.github.io/404.html","excerpt":"","text":""},{"title":"分类","date":"2022-10-06T02:48:58.721Z","updated":"2022-10-06T02:48:58.721Z","comments":false,"path":"categories/index.html","permalink":"http://cloud-tour.github.io/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2022-10-06T02:48:58.720Z","updated":"2022-10-06T02:48:58.720Z","comments":false,"path":"books/index.html","permalink":"http://cloud-tour.github.io/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2022-10-06T02:48:58.720Z","updated":"2022-10-06T02:48:58.720Z","comments":false,"path":"about/index.html","permalink":"http://cloud-tour.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"标签","date":"2022-10-06T02:48:58.722Z","updated":"2022-10-06T02:48:58.722Z","comments":false,"path":"tags/index.html","permalink":"http://cloud-tour.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-10-06T04:17:29.443Z","updated":"2022-10-06T02:48:58.722Z","comments":false,"path":"repository/index.html","permalink":"http://cloud-tour.github.io/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-10-06T02:48:58.721Z","updated":"2022-10-06T02:48:58.721Z","comments":true,"path":"links/index.html","permalink":"http://cloud-tour.github.io/links/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka","slug":"kafka","date":"2023-03-11T11:18:45.210Z","updated":"2023-03-11T11:19:43.379Z","comments":true,"path":"2023/03/11/kafka/","link":"","permalink":"http://cloud-tour.github.io/2023/03/11/kafka/","excerpt":"","text":"1、介绍Kafka 简介 Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等。 作用 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输 同时支持离线数据处理和实时数据处理 应用场景 异步处理 可以将一些比较耗时的操作放在其他系统中，通过消息队列将需要进行处理的消息进行存储，其他系统可以消费消息队列中的数据 比较常见的：发送短信验证码、发送邮件 系统解耦 原先一个微服务是通过接口（HTTP）调用另一个微服务，这时候耦合很严重，只要接口发生变化就会导致系统不可用 使用消息队列可以将系统进行解耦合，现在第一个微服务可以将消息放入到消息队列中，另一个微服务可以从消息队列中把消息取出来进行处理。进行系统解耦 流量削峰 因为消息队列是低延迟、高可靠、高吞吐的，可以应对大量并发 日志处理 可以使用消息队列作为临时存储，或者一种通信管道 消息队列的两种模型 生产者、消费者模型 生产者负责将消息生产到MQ中 消费者负责从MQ中获取消息 生产者和消费者是解耦的，可能是生产者一个程序、消费者是另外一个程序 消息队列的模式 点对点：一个消费者消费一个消息。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。生产者发送一条消息到queue，只有一个消费者能收到。 发布订阅：多个消费者可以消费一个消息。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。 2、Kafka的重要组件 broker：kafka集群中包含一个或者多个服务实例（节点），这种服务实例被称为broker（一个broker就是一个节点/一个服务器） Kafka服务器进程，生产者、消费者都要连接broker。 一个集群由多个broker组成，功能实现Kafka集群的负载均衡、容错。 topic：每条发布到kafka集群的消息都属于某个类别，这个类别就叫做topic。 一个Kafka集群中，可以包含多个topic。一个topic可以包含多个分区 是一个逻辑结构，生产、消费消息都需要指定topic partition：partition(分区)是一个物理上的概念，每个topic包含一个或者多个partition 一个topic中的消息可以分布在topic中的不同partition中 offset：偏移量。相对消费者、partition来说，可以通过offset来拉取数据 消息在日志中的位置，可以理解是消息在 partition 上的偏移量，也是代表该消息的唯一序号。 同时也是主从之间的需要同步的信息 segment：一个partition当中存在多个segment文件段，每个segment分为两部分，.log文件和 .index 文件，其中 .index 文件是索引文件，主要用于快速查询， .log 文件当中数据的偏移量位置 replica： partition replicas（分区副本），实现Kafkaf集群的容错，实现partition的容错。一个topic至少应该包含大于1个的副本 producer：消息的生产者，负责发布消息到 kafka 的 broker 中 consumer：消息的消费者，向 kafka 的 broker 中读取消息的客户端 consumer group：消费者组，每一个 consumer 属于一个特定的 consumer group（可以为每个consumer指定 groupName） 一个消费者组中可以包含多个消费者，共同来消费topic中的数据 一个topic中如果只有一个分区，那么这个分区只能被某个组中的一个消费者消费 有多少个分区，那么就可以被同一个组内的多少个消费者消费 .log：存放数据文件 .index：存放.log文件的索引数据 涵盖关系图： 3、Kafka消息幂等性解决 消息幂等问题 拿http举例来说，一次或多次请求，得到地响应是一致的（网络超时等问题除外），换句话说，就是执行多次操作与执行一次操作的影响是一样的。 Kafka生产者生产消息到partition，如果直接发送消息，kafka会将消息保存到分区中，但Kafka会返回一个ack给生产者，表示当前操作是否成功，是否已经保存了这条消息。如果ack响应的过程失败了，此时生产者会重试，继续发送没有发送成功的消息，Kafka又会保存一条一模一样的消息 解决： 当Kafka的生产者生产消息时，会增加一个pid（生产者的唯一编号）和sequence number（针对消息的一个递增序列） PID：每个Producer在初始化时，都会分配一个唯一的PID，这个PID对用户来说，是透明的。 Sequence Number：针对每个生产者（对应PID）发送到指定主题分区的消息都对应一个从0开始递增的Sequence Number。 发送消息，会连着pid和sequence number一块发送 kafka接收到消息，会将消息和pid、sequence number一并保存下来 如果ack响应失败，生产者重试，再次发送消息时，Kafka会根据pid、sequence number是否需要再保存一条消息 判断条件：生产者发送过来的sequence number 是否小于等于 partition中消息对应的sequence 4、消费组Consumer Group Rebalance机制 再均衡：在某些情况下，消费者组中的消费者消费的分区会产生变化，会导致消费者分配不均匀（例如：有两个消费者消费3个，因为某个partition崩溃了，还有一个消费者当前没有分区要削峰），Kafka Consumer Group就会启用rebalance机制，重新平衡这个Consumer Group内的消费者消费的分区分配。 触发时机 消费者数量发生变化 某个消费者crash 新增消费者 topic的数量发生变化 某个topic被删除 partition的数量发生变化 删除partition 新增partition 不良影响 发生rebalance，group中的所有的consumer将不再工作，共同来参与再均衡，直到每个消费者都已经被成功分配所需要消费的分区为止（rebalance结束） 5、Kafka分区策略 生产者的分区写入策略 轮询（按照消息尽量保证每个分区的负载）策略，消息会均匀地分布到每个partition 写入消息的时候，key为null的时候，默认使用的是轮询策略 随机策略（不使用） 按key写入策略，key.hash() % 分区的数量(有可能会出现数据倾斜) 自定义分区策略（类似于MapReduce指定分区） 乱序问题 在Kafka中生产者是有写入策略，如果topic有多个分区，就会将数据分散在不同的partition中存储 当partition数量大于1的时候，数据（消息）会打散分布在不同的partition中 如果只有一个分区，消息是有序的 消费者的分区分配策略 分区分配策略：保障每个消费者尽量能够均衡地消费分区的数据，不能出现某个消费者消费分区的数量特别多，某个消费者消费的分区特别少 Range分配策略（范围分配策略）：Kafka默认的分配策略 n：分区的数量 / 消费者数量 m：分区的数量 % 消费者数量 前m个消费者消费n+1个分区 剩余的消费者消费n个分区 RoundRobin分配策略（轮询分配策略） 消费者挨个分配消费的分区 Striky粘性分配策略 在没有发生rebalance跟轮询分配策略是一致的 发生了rebalance，轮询分配策略，重新走一遍轮询分配的过程。而粘性会保证跟上一次的尽量一致，只是将新的需要分配的分区，均匀的分配到现有可用的消费者中即可 减少上下文的切换 6、副本的ACK策略 producer是不断地往Kafka中写入数据，写入数据会有一个返回结果，表示是否写入成功。这里对应有一个ACKs的配置。 acks = 0：生产者只管写入，不管是否写入成功，可能会数据丢失。性能是最好的 acks = 1：生产者会等到leader分区写入成功后，返回成功，接着发送下一条 acks = -1/all：确保消息写入到leader分区、还确保消息写入到对应副本都成功后，接着发送下一条，性能是最差的 根据业务情况来选择ack机制，是要求性能最高，一部分数据丢失影响不大，可以选择0/1。如果要求数据一定不能丢失，就得配置为-1/all。 分区中是有leader和follower的概念，为了确保消费者消费的数据是一致的，只能从分区leader去读写消息，follower做的事情就是同步数据，Backup。 7、leader和follower 在Kafka中，每个topic都可以配置多个分区以及多个副本。每个分区都有一个leader以及0个或者多个follower，在创建topic时，Kafka会将每个分区的leader均匀地分配在每个broker上。我们正常使用kafka是感觉不到leader、follower的存在的。但其实，所有的读写操作都是由leader处理，而所有的follower都复制leader的日志数据文件，如果leader出现故障时，follower就会被选举为leader。 Kafka中的leader和follower是相对于分区有意义，不是相对broker Kafka在创建topic的时候，会尽量分配分区的leader在不同的broker中，其实就是负载均衡 leader职责：读写数据 follower职责：同步数据、参与选举（leader crash之后，会选举一个follower重新成为分区的leader） 注意和ZooKeeper区分 ZK的leader负责读、写，follower可以读取 Kafka的leader负责读写、follower不能读写数据（确保每个消费者消费的数据是一致的），Kafka一个topic有多个分区leader，一样可以实现数据操作的负载均衡 AR\\ISR\\OSR AR：（Assigned Replicas——已分配的副本）表示一个topic下的所有副本 ISR：（In-Sync Replicas——在同步中的副本）所有与leader副本保持一定程度同步的副本（包括 leader 副本在内）组成 OSR：（Out of Sync Replicas——不再同步的副本）由于follower副本同步滞后过多的副本（不包括 leader 副本）组成 AR = ISR + OSR 正常情况下，所有的follower副本都应该与leader副本保持同步，即AR = ISR，OSR集合为空。 leader选举 Controller：controller是kafka集群的老大，前面leader和follower是针对partition，而controller是针对broker的 Controller是高可用的，是用过ZK来进行选举，一旦某个broker崩溃，其他的broker会重新注册为Controller Kafka启动时，会在所有的broker中选择一个controller 创建topic、或者添加分区、修改副本数量之类的管理任务都是由controller完成的 Kafka分区leader的选举，也是由controller决定的 Leader：是针对partition的一个角色 Leader是通过ISR来进行快速选举 Controller选举Leader 所有Partition的leader选举都由controller决定 controller会将leader的改变直接通过RPC的方式通知需为此作出响应的Broker controller读取到当前分区的ISR，只要有一个Replica还幸存，就选择其中一个作为leader；否则，则任意选这个一个Replica作为leader 如果该partition的所有Replica都已经宕机，则新的leader为-1 为什么不能通过ZK的方式来选举partition的leader？ Kafka集群如果业务很多的情况下，会有很多的partition 假设某个broker宕机，就会出现很多的partiton都需要重新选举leader 如果使用zookeeper选举leader，会给zookeeper带来巨大的压力。所以，kafka中leader的选举不能使用ZK来实现 leader负载均衡 leader的负载均衡 如果某个broker crash之后，就可能会导致partition的leader分布不均匀，就是一个broker上存在一个topic下不同partition的leader Kafka中引入了一个叫做「preferred-replica」的概念，意思就是：优先的Replica 在ISR列表中，第一个replica就是preferred-replica 第一个分区存放的broker，肯定就是preferred-replica 通过以下指令，可以将leader分配到优先的leader对应的broker，确保leader是均匀分配的 1bin/kafka-leader-election.sh --bootstrap-server node1.itcast.cn:9092 --topic test --partition=2 --election-type preferred kafka读写流程 写流程 通过ZooKeeper找partition对应的leader，leader是负责写的 producer开始写入数据 broker进程上的leader将消息写入到本地log中 follower从leader上拉取消息，写入到本地log，并向leader发送ACK leader接收到所有的ISR中的Replica的ACK后，并向生产者返回ACK 读流程 通过ZooKeeper找partition对应的leader，leader是负责读的 通过ZooKeeper找到消费者对应的offset 然后开始从offset往后顺序拉取数据 提交offset（自动提交——每隔多少秒提交一次offset、手动提交——放入到事务中提交） 物理存储 Kafka的数据组织结构 topic partition segment .log数据文件 .index（稀疏索引） .timeindex（根据时间做的索引） 每个日志文件的文件名为起始偏移量，因为每个分区的起始偏移量是0，所以，分区的日志文件都以0000000000000000000.log开始 默认的每个日志文件最大为「log.segment.bytes =102410241024」1G 为了简化根据offset查找消息，Kafka日志文件名设计为开始的偏移量 深入了解读数据的流程 消费者的offset是一个针对partition全局offset 可以根据这个offset找到segment段 接着需要将全局的offset转换成segment的局部offset 根据局部的offset，就可以从（.index稀疏索引）找到对应的数据位置 开始顺序读取 8、Kafka的数据不丢失性 broker消息不丢失：因为有副本relicas的存在，会不断地从leader中同步副本，所以，一个broker crash，不会导致数据丢失，除非是只有一个副本。 生产者消息不丢失: 生产者连接leader写入数据时，可以通过ACK机制来确保数据已经成功写入。ACK机制有三个可选配置 配置ACK响应要求为 -1 时 —— 表示所有的节点都收到数据(leader和follower都接收到数据） 配置ACK响应要求为 1 时 —— 表示leader收到数据 配置ACK影响要求为 0 时 —— 生产者只负责发送数据，不关心数据是否丢失（这种情况可能会产生数据丢失，但性能是最好的） 注意：如果broker端一直不返回ack状态，producer永远不知道是否成功；producer可以设置一个超时时间10s，超过时间认为失败。 生产者可以采用同步和异步两种方式发送数据 同步方式 发送一批数据给kafka后，等待kafka返回结果： 生产者等待10s，如果broker没有给出ack响应，就认为失败 生产者重试3次，如果还没有响应，就报错 异步方式 发送一批数据给kafka，只是提供一个回调函数： 先将数据保存在生产者端的buffer中。buffer大小是2万条 满足数据阈值或者数量阈值其中的一个条件就可以发送数据 发送一批数据的大小是500条 如果broker迟迟不给ack，而buﬀer又满了，开发者可以设置是否直接清空buﬀer中的数据 消费者消费不丢失： 在消费者消费数据的时候，只要每个消费者记录好oﬀset值即可，就能保证数据不丢失。 At-least once：一种数据可能会重复消费 Exactly-Once：仅被一次消费 9、Kafka的数据清理和配额限速 数据清理 Log Deletion（日志删除）：如果消息达到一定的条件（时间、日志大小、offset大小），Kafka就会自动将日志设置为待删除（segment端的后缀名会以 .delete结尾），日志管理程序会定期清理这些日志 默认是7天过期 Log Compaction（日志合并） 如果在一些key-value数据中，一个key可以对应多个不同版本的value 经过日志合并，就会只保留最新的一个版本 配额限速 可以限制Producer、Consumer的速率 防止Kafka的速度过快，占用整个服务器（broker）的所有IO资源 10、Kafka性能好的原因 顺序写磁盘 操作系统每次从磁盘读写数据的时候，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写，如果是机械硬盘，寻址就需要较长的时间。 kafka的设计中，数据其实是存储在磁盘上面，一般来说，会把数据存储在内存上面性能才会好。但是kafka用的是顺序写，追加数据是追加到末尾，磁盘顺序写的性能极高，在磁盘个数一定，转数达到一定的情况下，基本和内存速度一致。随机写的话是在文件的某个位置修改数据，性能会较低。 Page Cache Kafka 在 OS 系统方面使用了 Page Cache 而不是我们平常所用的 Buffer。Page Cache 其实不陌生，也不是什么新鲜事物 我们在 linux 上查看内存的时候，经常可以看到 buff/cache，两者都是用来加速 IO 读写用的，而 cache 是作用于读，也就是说，磁盘的内容可以读到 cache 里面这样，应用程序读磁盘就非常快；而 buff 是作用于写，我们开发写磁盘都是，一般如果写入一个 buff 里面再 flush 就非常快。而 kafka 正是把这两者发挥了极致： Kafka 虽然是 scala 写的，但是依旧在 Java 的虚拟机上运行，尽管如此，它尽量避开了 JVM 的限制，它利用了 Page cache 来存储，这样躲开了数据在 JVM 因为 GC 而发生的 STD。另一方面也是 Page Cache 使得它实现了零拷贝，具体下面会讲。 零拷贝 先来看看非零拷贝的情况： 可以看到数据的拷贝从内存拷贝到kafka服务进程那块，又拷贝到socket缓存那块，整个过程耗费的时间比较高，kafka利用了Linux的sendFile技术（NIO），省去了进程切换和一次数据拷贝，让性能变得更好。 传统的一次应用程请求数据的过程 这里大致可以发传统的方式发生了 4 次拷贝，2 次 DMA 和 2 次 CPU，而 CPU 发生了 4 次的切换。（DMA 简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情） 零拷贝的方式 通过优化我们可以发现，CPU 只发生了 2 次的上下文切换和 3 次数据拷贝。（linux 系统提供了系统事故调用函数“ sendfile()”，这样系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态） 分区分段 我们上面也介绍过了，kafka 采取了分区的模式，而每一个分区又对应到一个物理分段，而查找的时候可以根据二分查找快速定位。这样不仅提供了数据读的查询效率，也提供了并行操作的方式。 数据压缩 Kafka 对数据提供了：Gzip 和 Snappy 压缩协议等压缩协议，对消息结构体进行了压缩，一方面减少了带宽，也减少了数据传输的消耗。 11、日志如何分段存储 Kafka规定了一个分区内的.log文件最大为1G，做这个限制目的是为了方便把.log加载到内存去操作 123456789101100000000000000000000.index00000000000000000000.log00000000000000000000.timeindex00000000000005367851.index00000000000005367851.log00000000000005367851.timeindex00000000000009936472.index00000000000009936472.log00000000000009936472.timeindex 这个9936472之类的数字，就是代表了这个日志段文件里包含的起始offset，也就说明这个分区里至少都写入了接近1000万条数据了。Kafka broker有一个参数，log.segment.bytes，限定了每个日志段文件的大小，最大就是1GB，一个日志段文件满了，就自动开一个新的日志段文件来写入，避免单个文件过大，影响文件的读写性能，这个过程叫做log rolling，正在被写入的那个日志段文件，叫做active log segment。如果大家有看前面的两篇有关于HDFS的文章时，就会发现NameNode的edits log也会做出限制，所以这些框架都是会考虑到这些问题。 12、Kafka如何网络设计？ kafka的网络设计和Kafka的调优有关，这也是为什么它能支持高并发的原因： 首先客户端发送请求全部会先发送给一个Acceptor，broker里面会存在3个线程（默认是3个），这3个线程都是叫做processor，Acceptor不会对客户端的请求做任何的处理，直接封装成一个个socketChannel发送给这些processor形成一个队列，发送的方式是轮询，就是先给第一个processor发送，然后再给第二个，第三个，然后又回到第一个。消费者线程去消费这些socketChannel时，会获取一个个request请求，这些request请求中就会伴随着数据。 线程池里面默认有8个线程，这些线程是用来处理request的，解析请求，如果request是写请求，就写到磁盘里。读的话返回结果。 processor会从response中读取响应数据，然后再返回给客户端。这就是Kafka的网络三层架构。 所以如果我们需要对kafka进行增强调优，增加processor并增加线程池里面的处理线程，就可以达到效果。request和response那一块部分其实就是起到了一个缓存的效果，是考虑到processor们生成请求太快，线程数不够不能及时处理的问题。所以这就是一个加强版的reactor网络线程模型。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://cloud-tour.github.io/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://cloud-tour.github.io/tags/kafka/"}]},{"title":"基础","slug":"基础","date":"2023-03-07T10:42:48.718Z","updated":"2023-03-07T10:50:17.243Z","comments":true,"path":"2023/03/07/基础/","link":"","permalink":"http://cloud-tour.github.io/2023/03/07/%E5%9F%BA%E7%A1%80/","excerpt":"","text":"基础篇 基础篇要点：算法、数据结构、基础设计模式 1. 二分查找 要求 能够用自己语言描述二分查找算法 能够手写二分查找代码 能够解答一些变化后的考法 算法描述 前提：有已排序数组 A（假设已经做好） 定义左边界 L、右边界 R，确定搜索范围，循环执行二分查找（3、4两步） 获取中间索引 M = Floor((L+R) /2) 中间索引的值 A[M] 与待搜索的值 T 进行比较 ① A[M] == T 表示找到，返回中间索引 ② A[M] &gt; T，中间值右侧的其它元素都大于 T，无需比较，中间索引左边去找，M - 1 设置为右边界，重新查找 ③ A[M] &lt; T，中间值左侧的其它元素都小于 T，无需比较，中间索引右边去找， M + 1 设置为左边界，重新查找 当 L &gt; R 时，表示没有找到，应结束循环 更形象的描述请参考：binary_search.html 算法实现 1234567891011121314public static int binarySearch(int[] a, int t) &#123; int l = 0, r = a.length - 1, m; while (l &lt;= r) &#123; m = (l + r) / 2; if (a[m] == t) &#123; return m; &#125; else if (a[m] &gt; t) &#123; r = m - 1; &#125; else &#123; l = m + 1; &#125; &#125; return -1;&#125; 测试代码 123456public static void main(String[] args) &#123; int[] array = &#123;1, 5, 8, 11, 19, 22, 31, 35, 40, 45, 48, 49, 50&#125;; int target = 47; int idx = binarySearch(array, target); System.out.println(idx);&#125; 解决整数溢出问题 当 l 和 r 都较大时，l + r 有可能超过整数范围，造成运算错误，解决方法有两种： 1int m = l + (r - l) / 2; 还有一种是： 1int m = (l + r) &gt;&gt;&gt; 1; 其它考法 有一个有序表为 1,5,8,11,19,22,31,35,40,45,48,49,50 当二分查找值为 48 的结点时，查找成功需要比较的次数 使用二分法在序列 1,4,6,7,15,33,39,50,64,78,75,81,89,96 中查找元素 81 时，需要经过（ ）次比较 在拥有128个元素的数组中二分查找一个数，需要比较的次数最多不超过多少次 对于前两个题目，记得一个简要判断口诀：奇数二分取中间，偶数二分取中间靠左。对于后一道题目，需要知道公式： n=log2N=log10N/log102n = log_2N = log_{10}N/log_{10}2 n=log2​N=log10​N/log10​2 其中 n 为查找次数，N 为元素个数 2. 冒泡排序 要求 能够用自己语言描述冒泡排序算法 能够手写冒泡排序代码 了解一些冒泡排序的优化手段 算法描述 依次比较数组中相邻两个元素大小，若 a[j] &gt; a[j+1]，则交换两个元素，两两都比较一遍称为一轮冒泡，结果是让最大的元素排至最后 重复以上步骤，直到整个数组有序 更形象的描述请参考：bubble_sort.html 算法实现 123456789101112131415161718public static void bubble(int[] a) &#123; for (int j = 0; j &lt; a.length - 1; j++) &#123; // 一轮冒泡 boolean swapped = false; // 是否发生了交换 for (int i = 0; i &lt; a.length - 1 - j; i++) &#123; System.out.println(&quot;比较次数&quot; + i); if (a[i] &gt; a[i + 1]) &#123; Utils.swap(a, i, i + 1); swapped = true; &#125; &#125; System.out.println(&quot;第&quot; + j + &quot;轮冒泡&quot; + Arrays.toString(a)); if (!swapped) &#123; break; &#125; &#125;&#125; 优化点1：每经过一轮冒泡，内层循环就可以减少一次 优化点2：如果某一轮冒泡没有发生交换，则表示所有数据有序，可以结束外层循环 进一步优化 12345678910111213141516171819public static void bubble_v2(int[] a) &#123; int n = a.length - 1; while (true) &#123; int last = 0; // 表示最后一次交换索引位置 for (int i = 0; i &lt; n; i++) &#123; System.out.println(&quot;比较次数&quot; + i); if (a[i] &gt; a[i + 1]) &#123; Utils.swap(a, i, i + 1); last = i; &#125; &#125; n = last; System.out.println(&quot;第轮冒泡&quot; + Arrays.toString(a)); if (n == 0) &#123; break; &#125; &#125;&#125; 每轮冒泡时，最后一次交换索引可以作为下一轮冒泡的比较次数，如果这个值为零，表示整个数组有序，直接退出外层循环即可 3. 选择排序 要求 能够用自己语言描述选择排序算法 能够比较选择排序与冒泡排序 理解非稳定排序与稳定排序 算法描述 将数组分为两个子集，排序的和未排序的，每一轮从未排序的子集中选出最小的元素，放入排序子集 重复以上步骤，直到整个数组有序 更形象的描述请参考：selection_sort.html 算法实现 123456789101112131415public static void selection(int[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; // i 代表每轮选择最小元素要交换到的目标索引 int s = i; // 代表最小元素的索引 for (int j = s + 1; j &lt; a.length; j++) &#123; if (a[s] &gt; a[j]) &#123; // j 元素比 s 元素还要小, 更新 s s = j; &#125; &#125; if (s != i) &#123; swap(a, s, i); &#125; System.out.println(Arrays.toString(a)); &#125;&#125; 优化点：为减少交换次数，每一轮可以先找最小的索引，在每轮最后再交换元素 与冒泡排序比较 二者平均时间复杂度都是 O(n2)O(n^2)O(n2) 选择排序一般要快于冒泡，因为其交换次数少 但如果集合有序度高，冒泡优于选择 冒泡属于稳定排序算法，而选择属于不稳定排序 稳定排序指，按对象中不同字段进行多次排序，不会打乱同值元素的顺序 不稳定排序则反之 稳定排序与不稳定排序 123456789101112131415System.out.println(&quot;=================不稳定================&quot;);Card[] cards = getStaticCards();System.out.println(Arrays.toString(cards));selection(cards, Comparator.comparingInt((Card a) -&gt; a.sharpOrder).reversed());System.out.println(Arrays.toString(cards));selection(cards, Comparator.comparingInt((Card a) -&gt; a.numberOrder).reversed());System.out.println(Arrays.toString(cards));System.out.println(&quot;=================稳定=================&quot;);cards = getStaticCards();System.out.println(Arrays.toString(cards));bubble(cards, Comparator.comparingInt((Card a) -&gt; a.sharpOrder).reversed());System.out.println(Arrays.toString(cards));bubble(cards, Comparator.comparingInt((Card a) -&gt; a.numberOrder).reversed());System.out.println(Arrays.toString(cards)); 都是先按照花色排序（♠♥♣♦），再按照数字排序（AKQJ…） 不稳定排序算法按数字排序时，会打乱原本同值的花色顺序 12[[♠7], [♠2], [♠4], [♠5], [♥2], [♥5]][[♠7], [♠5], [♥5], [♠4], [♥2], [♠2]] 原来 ♠2 在前 ♥2 在后，按数字再排后，他俩的位置变了 稳定排序算法按数字排序时，会保留原本同值的花色顺序，如下所示 ♠2 与 ♥2 的相对位置不变 12[[♠7], [♠2], [♠4], [♠5], [♥2], [♥5]][[♠7], [♠5], [♥5], [♠4], [♠2], [♥2]] 4. 插入排序 要求 能够用自己语言描述插入排序算法 能够比较插入排序与选择排序 算法描述 将数组分为两个区域，排序区域和未排序区域，每一轮从未排序区域中取出第一个元素，插入到排序区域（需保证顺序） 重复以上步骤，直到整个数组有序 更形象的描述请参考：insertion_sort.html 算法实现 12345678910111213141516171819// 修改了代码与希尔排序一致public static void insert(int[] a) &#123; // i 代表待插入元素的索引 for (int i = 1; i &lt; a.length; i++) &#123; int t = a[i]; // 代表待插入的元素值 int j = i; System.out.println(j); while (j &gt;= 1) &#123; if (t &lt; a[j - 1]) &#123; // j-1 是上一个元素索引，如果 &gt; t，后移 a[j] = a[j - 1]; j--; &#125; else &#123; // 如果 j-1 已经 &lt;= t, 则 j 就是插入位置 break; &#125; &#125; a[j] = t; System.out.println(Arrays.toString(a) + &quot; &quot; + j); &#125;&#125; 与选择排序比较 二者平均时间复杂度都是 O(n2)O(n^2)O(n2) 大部分情况下，插入都略优于选择 有序集合插入的时间复杂度为 O(n)O(n)O(n) 插入属于稳定排序算法，而选择属于不稳定排序 提示 插入排序通常被同学们所轻视，其实它的地位非常重要。小数据量排序，都会优先选择插入排序 5. 希尔排序 要求 能够用自己语言描述希尔排序算法 算法描述 首先选取一个间隙序列，如 (n/2，n/4 … 1)，n 为数组长度 每一轮将间隙相等的元素视为一组，对组内元素进行插入排序，目的有二 ① 少量元素插入排序速度很快 ② 让组内值较大的元素更快地移动到后方 当间隙逐渐减少，直至为 1 时，即可完成排序 更形象的描述请参考：shell_sort.html 算法实现 123456789101112131415161718192021private static void shell(int[] a) &#123; int n = a.length; for (int gap = n / 2; gap &gt; 0; gap /= 2) &#123; // i 代表待插入元素的索引 for (int i = gap; i &lt; n; i++) &#123; int t = a[i]; // 代表待插入的元素值 int j = i; while (j &gt;= gap) &#123; // 每次与上一个间隙为 gap 的元素进行插入排序 if (t &lt; a[j - gap]) &#123; // j-gap 是上一个元素索引，如果 &gt; t，后移 a[j] = a[j - gap]; j -= gap; &#125; else &#123; // 如果 j-1 已经 &lt;= t, 则 j 就是插入位置 break; &#125; &#125; a[j] = t; System.out.println(Arrays.toString(a) + &quot; gap:&quot; + gap); &#125; &#125;&#125; 参考资料 https://en.wikipedia.org/wiki/Shellsort 6. 快速排序 要求 能够用自己语言描述快速排序算法 掌握手写单边循环、双边循环代码之一 能够说明快排特点 了解洛穆托与霍尔两种分区方案的性能比较 算法描述 每一轮排序选择一个基准点（pivot）进行分区 让小于基准点的元素的进入一个分区，大于基准点的元素的进入另一个分区 当分区完成时，基准点元素的位置就是其最终位置 在子分区内重复以上过程，直至子分区元素个数少于等于 1，这体现的是分而治之的思想 （divide-and-conquer） 从以上描述可以看出，一个关键在于分区算法，常见的有洛穆托分区方案、双边循环分区方案、霍尔分区方案 更形象的描述请参考：quick_sort.html 单边循环快排（lomuto 洛穆托分区方案） 选择最右元素作为基准点元素 j 指针负责找到比基准点小的元素，一旦找到则与 i 进行交换 i 指针维护小于基准点元素的边界，也是每次交换的目标索引 最后基准点与 i 交换，i 即为分区位置 123456789101112131415161718192021222324252627public static void quick(int[] a, int l, int h) &#123; if (l &gt;= h) &#123; return; &#125; int p = partition(a, l, h); // p 索引值 quick(a, l, p - 1); // 左边分区的范围确定 quick(a, p + 1, h); // 左边分区的范围确定&#125;private static int partition(int[] a, int l, int h) &#123; int pv = a[h]; // 基准点元素 int i = l; for (int j = l; j &lt; h; j++) &#123; if (a[j] &lt; pv) &#123; if (i != j) &#123; swap(a, i, j); &#125; i++; &#125; &#125; if (i != h) &#123; swap(a, h, i); &#125; System.out.println(Arrays.toString(a) + &quot; i=&quot; + i); // 返回值代表了基准点元素所在的正确索引，用它确定下一轮分区的边界 return i;&#125; 双边循环快排（不完全等价于 hoare 霍尔分区方案） 选择最左元素作为基准点元素 j 指针负责从右向左找比基准点小的元素，i 指针负责从左向右找比基准点大的元素，一旦找到二者交换，直至 i，j 相交 最后基准点与 i（此时 i 与 j 相等）交换，i 即为分区位置 要点 基准点在左边，并且要先 j 后 i while( i &lt; j &amp;&amp; a[j] &gt; pv ) j– while ( i &lt; j &amp;&amp; a[i] &lt;= pv ) i++ 12345678910111213141516171819202122232425262728private static void quick(int[] a, int l, int h) &#123; if (l &gt;= h) &#123; return; &#125; int p = partition(a, l, h); quick(a, l, p - 1); quick(a, p + 1, h);&#125;private static int partition(int[] a, int l, int h) &#123; int pv = a[l]; int i = l; int j = h; while (i &lt; j) &#123; // j 从右找小的 while (i &lt; j &amp;&amp; a[j] &gt; pv) &#123; j--; &#125; // i 从左找大的 while (i &lt; j &amp;&amp; a[i] &lt;= pv) &#123; i++; &#125; swap(a, i, j); &#125; swap(a, l, j); System.out.println(Arrays.toString(a) + &quot; j=&quot; + j); return j;&#125; 快排特点 平均时间复杂度是 O(nlog2⁡n)O(nlog_2⁡n )O(nlog2​⁡n)，最坏时间复杂度 O(n2)O(n^2)O(n2) 数据量较大时，优势非常明显 属于不稳定排序 洛穆托分区方案 vs 霍尔分区方案 霍尔的移动次数平均来讲比洛穆托少3倍 https://qastack.cn/cs/11458/quicksort-partitioning-hoare-vs-lomuto 补充代码说明 day01.sort.QuickSort3 演示了空穴法改进的双边快排，比较次数更少 day01.sort.QuickSortHoare 演示了霍尔分区的实现 day01.sort.LomutoVsHoare 对四种分区实现的移动次数比较 7. ArrayList 要求 掌握 ArrayList 扩容规则 扩容规则 ArrayList() 会使用长度为零的数组 ArrayList(int initialCapacity) 会使用指定容量的数组 public ArrayList(Collection&lt;? extends E&gt; c) 会使用 c 的大小作为数组容量 add(Object o) 首次扩容为 10，再次扩容为上次容量的 1.5 倍 addAll(Collection c) 没有元素时，扩容为 Math.max(10, 实际元素个数)，有元素时为 Math.max(原容量 1.5 倍, 实际元素个数) 其中第 4 点必须知道，其它几点视个人情况而定 提示 测试代码见 day01.list.TestArrayList ，这里不再列出 要注意的是，示例中用反射方式来更直观地反映 ArrayList 的扩容特征，但从 JDK 9 由于模块化的影响，对反射做了较多限制，需要在运行测试代码时添加 VM 参数 --add-opens java.base/java.util=ALL-UNNAMED 方能运行通过，后面的例子都有相同问题 代码说明 day01.list.TestArrayList#arrayListGrowRule 演示了 add(Object) 方法的扩容规则，输入参数 n 代表打印多少次扩容后的数组长度 8. Iterator 要求 掌握什么是 Fail-Fast、什么是 Fail-Safe Fail-Fast 与 Fail-Safe ArrayList 是 fail-fast 的典型代表，遍历的同时不能修改，尽快失败 CopyOnWriteArrayList 是 fail-safe 的典型代表，遍历的同时可以修改，原理是读写分离 提示 测试代码见 day01.list.FailFastVsFailSafe，这里不再列出 9. LinkedList 要求 能够说清楚 LinkedList 对比 ArrayList 的区别，并重视纠正部分错误的认知 LinkedList 基于双向链表，无需连续内存 随机访问慢（要沿着链表遍历） 头尾插入删除性能高 占用内存多 ArrayList 基于数组，需要连续内存 随机访问快（指根据下标访问） 尾部插入、删除性能可以，其它部分插入、删除都会移动数据，因此性能会低 可以利用 cpu 缓存，局部性原理 代码说明 day01.list.ArrayListVsLinkedList#randomAccess 对比随机访问性能 day01.list.ArrayListVsLinkedList#addMiddle 对比向中间插入性能 day01.list.ArrayListVsLinkedList#addFirst 对比头部插入性能 day01.list.ArrayListVsLinkedList#addLast 对比尾部插入性能 day01.list.ArrayListVsLinkedList#linkedListSize 打印一个 LinkedList 占用内存 day01.list.ArrayListVsLinkedList#arrayListSize 打印一个 ArrayList 占用内存 10. HashMap 要求 掌握 HashMap 的基本数据结构 掌握树化 理解索引计算方法、二次 hash 的意义、容量对索引计算的影响 掌握 put 流程、扩容、扩容因子 理解并发使用 HashMap 可能导致的问题 理解 key 的设计 1）基本数据结构 1.7 数组 + 链表 1.8 数组 + （链表 | 红黑树） 更形象的演示，见资料中的 hash-demo.jar，运行需要 jdk14 以上环境，进入 jar 包目录，执行下面命令 1java -jar --add-exports java.base/jdk.internal.misc=ALL-UNNAMED hash-demo.jar 2）树化与退化 树化意义 红黑树用来避免 DoS 攻击，防止链表超长时性能下降，树化应当是偶然情况，是保底策略 hash 表的查找，更新的时间复杂度是 O(1)O(1)O(1)，而红黑树的查找，更新的时间复杂度是 O(log2⁡n)O(log_2⁡n )O(log2​⁡n)，TreeNode 占用空间也比普通 Node 的大，如非必要，尽量还是使用链表 hash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，树化阈值选择 8 就是为了让树化几率足够小 树化规则 当链表长度超过树化阈值 8 时，先尝试扩容来减少链表长度，如果数组容量已经 &gt;=64，才会进行树化 退化规则 情况1：在扩容时如果拆分树时，树元素个数 &lt;= 6 则会退化链表 情况2：remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表 3）索引计算 索引计算方法 首先，计算对象的 hashCode() 再进行调用 HashMap 的 hash() 方法进行二次哈希 二次 hash() 是为了综合高位数据，让哈希分布更为均匀 最后 &amp; (capacity – 1) 得到索引 数组容量为何是 2 的 n 次幂 计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模 扩容时重新计算索引效率更高： hash &amp; oldCap == 0 的元素留在原来位置 ，否则新位置 = 旧位置 + oldCap 注意 二次 hash 是为了配合 容量是 2 的 n 次幂 这一设计前提，如果 hash 表的容量不是 2 的 n 次幂，则不必二次 hash 容量是 2 的 n 次幂 这一设计计算索引效率更好，但 hash 的分散性就不好，需要二次 hash 来作为补偿，没有采用这一设计的典型例子是 Hashtable 4）put 与扩容 put 流程 HashMap 是懒惰创建数组的，首次使用才创建数组 计算索引（桶下标） 如果桶下标还没人占用，创建 Node 占位返回 如果桶下标已经有人占用 已经是 TreeNode 走红黑树的添加或更新逻辑 是普通 Node，走链表的添加或更新逻辑，如果链表长度超过树化阈值，走树化逻辑 返回前检查容量是否超过阈值，一旦超过进行扩容 1.7 与 1.8 的区别 链表插入节点时，1.7 是头插法，1.8 是尾插法 1.7 是大于等于阈值且没有空位时才扩容，而 1.8 是大于阈值就扩容 1.8 在扩容计算 Node 索引时，会优化 扩容（加载）因子为何默认是 0.75f 在空间占用与查询时间之间取得较好的权衡 大于这个值，空间节省了，但链表就会比较长影响性能 小于这个值，冲突减少了，但扩容就会更频繁，空间占用也更多 5）并发问题 扩容死链（1.7 会存在） 1.7 源码如下： 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; e 和 next 都是局部变量，用来指向当前节点和下一个节点 线程1（绿色）的临时变量 e 和 next 刚引用了这俩节点，还未来得及移动节点，发生了线程切换，由线程2（蓝色）完成扩容和迁移 线程2 扩容完成，由于头插法，链表顺序颠倒。但线程1 的临时变量 e 和 next 还引用了这俩节点，还要再来一遍迁移 第一次循环 循环接着线程切换前运行，注意此时 e 指向的是节点 a，next 指向的是节点 b e 头插 a 节点，注意图中画了两份 a 节点，但事实上只有一个（为了不让箭头特别乱画了两份） 当循环结束是 e 会指向 next 也就是 b 节点 第二次循环 next 指向了节点 a e 头插节点 b 当循环结束时，e 指向 next 也就是节点 a 第三次循环 next 指向了 null e 头插节点 a，a 的 next 指向了 b（之前 a.next 一直是 null），b 的 next 指向 a，死链已成 当循环结束时，e 指向 next 也就是 null，因此第四次循环时会正常退出 数据错乱（1.7，1.8 都会存在） 代码参考 day01.map.HashMapMissData，具体调试步骤参考视频 补充代码说明 day01.map.HashMapDistribution 演示 map 中链表长度符合泊松分布 day01.map.DistributionAffectedByCapacity 演示容量及 hashCode 取值对分布的影响 day01.map.DistributionAffectedByCapacity#hashtableGrowRule 演示了 Hashtable 的扩容规律 day01.sort.Utils#randomArray 如果 hashCode 足够随机，容量是否是 2 的 n 次幂影响不大 day01.sort.Utils#lowSameArray 如果 hashCode 低位一样的多，容量是 2 的 n 次幂会导致分布不均匀 day01.sort.Utils#evenArray 如果 hashCode 偶数的多，容量是 2 的 n 次幂会导致分布不均匀 由此得出对于容量是 2 的 n 次幂的设计来讲，二次 hash 非常重要 day01.map.HashMapVsHashtable 演示了对于同样数量的单词字符串放入 HashMap 和 Hashtable 分布上的区别 6）key 的设计 key 的设计要求 HashMap 的 key 可以为 null，但 Map 的其他实现则不然 作为 key 的对象，必须实现 hashCode 和 equals，并且 key 的内容不能修改（不可变） key 的 hashCode 应该有良好的散列性 如果 key 可变，例如修改了 age 会导致再次查询时查询不到 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class HashMapMutableKey &#123; public static void main(String[] args) &#123; HashMap&lt;Student, Object&gt; map = new HashMap&lt;&gt;(); Student stu = new Student(&quot;张三&quot;, 18); map.put(stu, new Object()); System.out.println(map.get(stu)); stu.age = 19; System.out.println(map.get(stu)); &#125; static class Student &#123; String name; int age; public Student(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return age == student.age &amp;&amp; Objects.equals(name, student.name); &#125; @Override public int hashCode() &#123; return Objects.hash(name, age); &#125; &#125;&#125; String 对象的 hashCode() 设计 目标是达到较为均匀的散列效果，每个字符串的 hashCode 足够独特 字符串中的每个字符都可以表现为一个数字，称为 SiS_iSi​，其中 i 的范围是 0 ~ n - 1 散列公式为： S0∗31(n−1)+S1∗31(n−2)+…Si∗31(n−1−i)+…S(n−1)∗310S_0∗31^{(n-1)}+ S_1∗31^{(n-2)}+ … S_i ∗ 31^{(n-1-i)}+ …S_{(n-1)}∗31^0S0​∗31(n−1)+S1​∗31(n−2)+…Si​∗31(n−1−i)+…S(n−1)​∗310 31 代入公式有较好的散列特性，并且 31 * h 可以被优化为 即 $32 ∗h -h $ 即 25∗h−h2^5 ∗h -h25∗h−h 即 h≪5−hh≪5 -hh≪5−h 11. 单例模式 要求 掌握五种单例模式的实现方式 理解为何 DCL 实现时要使用 volatile 修饰静态变量 了解 jdk 中用到单例的场景 饿汉式 12345678910111213141516171819202122public class Singleton1 implements Serializable &#123; private Singleton1() &#123; if (INSTANCE != null) &#123; throw new RuntimeException(&quot;单例对象不能重复创建&quot;); &#125; System.out.println(&quot;private Singleton1()&quot;); &#125; private static final Singleton1 INSTANCE = new Singleton1(); public static Singleton1 getInstance() &#123; return INSTANCE; &#125; public static void otherMethod() &#123; System.out.println(&quot;otherMethod()&quot;); &#125; public Object readResolve() &#123; return INSTANCE; &#125;&#125; 构造方法抛出异常是防止反射破坏单例 readResolve() 是防止反序列化破坏单例 枚举饿汉式 1234567891011121314151617181920public enum Singleton2 &#123; INSTANCE; private Singleton2() &#123; System.out.println(&quot;private Singleton2()&quot;); &#125; @Override public String toString() &#123; return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); &#125; public static Singleton2 getInstance() &#123; return INSTANCE; &#125; public static void otherMethod() &#123; System.out.println(&quot;otherMethod()&quot;); &#125;&#125; 枚举饿汉式能天然防止反射、反序列化破坏单例 懒汉式 1234567891011121314151617181920public class Singleton3 implements Serializable &#123; private Singleton3() &#123; System.out.println(&quot;private Singleton3()&quot;); &#125; private static Singleton3 INSTANCE = null; // Singleton3.class public static synchronized Singleton3 getInstance() &#123; if (INSTANCE == null) &#123; INSTANCE = new Singleton3(); &#125; return INSTANCE; &#125; public static void otherMethod() &#123; System.out.println(&quot;otherMethod()&quot;); &#125;&#125; 其实只有首次创建单例对象时才需要同步，但该代码实际上每次调用都会同步 因此有了下面的双检锁改进 双检锁懒汉式 12345678910111213141516171819202122public class Singleton4 implements Serializable &#123; private Singleton4() &#123; System.out.println(&quot;private Singleton4()&quot;); &#125; private static volatile Singleton4 INSTANCE = null; // 可见性，有序性 public static Singleton4 getInstance() &#123; if (INSTANCE == null) &#123; synchronized (Singleton4.class) &#123; if (INSTANCE == null) &#123; INSTANCE = new Singleton4(); &#125; &#125; &#125; return INSTANCE; &#125; public static void otherMethod() &#123; System.out.println(&quot;otherMethod()&quot;); &#125;&#125; 为何必须加 volatile： INSTANCE = new Singleton4() 不是原子的，分成 3 步：创建对象、调用构造、给静态变量赋值，其中后两步可能被指令重排序优化，变成先赋值、再调用构造 如果线程1 先执行了赋值，线程2 执行到第一个 INSTANCE == null 时发现 INSTANCE 已经不为 null，此时就会返回一个未完全构造的对象 内部类懒汉式 1234567891011121314151617public class Singleton5 implements Serializable &#123; private Singleton5() &#123; System.out.println(&quot;private Singleton5()&quot;); &#125; private static class Holder &#123; static Singleton5 INSTANCE = new Singleton5(); &#125; public static Singleton5 getInstance() &#123; return Holder.INSTANCE; &#125; public static void otherMethod() &#123; System.out.println(&quot;otherMethod()&quot;); &#125;&#125; 避免了双检锁的缺点 JDK 中单例的体现 Runtime 体现了饿汉式单例 Console 体现了双检锁懒汉式单例 Collections 中的 EmptyNavigableSet 内部类懒汉式单例 ReverseComparator.REVERSE_ORDER 内部类懒汉式单例 Comparators.NaturalOrderComparator.INSTANCE 枚举饿汉式单例 并发篇 1. 线程状态 要求 掌握 Java 线程六种状态 掌握 Java 线程状态转换 能理解五种状态与六种状态两种说法的区别 六种状态及转换 分别是 新建 当一个线程对象被创建，但还未调用 start 方法时处于新建状态 此时未与操作系统底层线程关联 可运行 调用了 start 方法，就会由新建进入可运行 此时与底层线程关联，由操作系统调度执行 终结 线程内代码已经执行完毕，由可运行进入终结 此时会取消与底层线程关联 阻塞 当获取锁失败后，由可运行进入 Monitor 的阻塞队列阻塞，此时不占用 cpu 时间 当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的阻塞线程，唤醒后的线程进入可运行状态 等待 当获取锁成功后，但由于条件不满足，调用了 wait() 方法，此时从可运行状态释放锁进入 Monitor 等待集合等待，同样不占用 cpu 时间 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的等待线程，恢复为可运行状态 有时限等待 当获取锁成功后，但由于条件不满足，调用了 wait(long) 方法，此时从可运行状态释放锁进入 Monitor 等待集合进行有时限等待，同样不占用 cpu 时间 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的有时限等待线程，恢复为可运行状态，并重新去竞争锁 如果等待超时，也会从有时限等待状态恢复为可运行状态，并重新去竞争锁 还有一种情况是调用 sleep(long) 方法也会从可运行状态进入有时限等待状态，但与 Monitor 无关，不需要主动唤醒，超时时间到自然恢复为可运行状态 其它情况（只需了解） 可以用 interrupt() 方法打断等待、有时限等待的线程，让它们恢复为可运行状态 park，unpark 等方法也可以让线程等待和唤醒 五种状态 五种状态的说法来自于操作系统层面的划分 运行态：分到 cpu 时间，能真正执行线程内代码的 就绪态：有资格分到 cpu 时间，但还未轮到它的 阻塞态：没资格分到 cpu 时间的 涵盖了 java 状态中提到的阻塞、等待、有时限等待 多出了阻塞 I/O，指线程在调用阻塞 I/O 时，实际活由 I/O 设备完成，此时线程无事可做，只能干等 新建与终结态：与 java 中同名状态类似，不再啰嗦 2. 线程池 要求 掌握线程池的 7 大核心参数 七大参数 corePoolSize 核心线程数目 - 池中会保留的最多线程数 maximumPoolSize 最大线程数目 - 核心线程+救急线程的最大数目 keepAliveTime 生存时间 - 救急线程的生存时间，生存时间内没有新任务，此线程资源会释放 unit 时间单位 - 救急线程的生存时间单位，如秒、毫秒等 workQueue - 当没有空闲核心线程时，新来任务会加入到此队列排队，队列满会创建救急线程执行任务 threadFactory 线程工厂 - 可以定制线程对象的创建，例如设置线程名字、是否是守护线程等 handler 拒绝策略 - 当所有线程都在繁忙，workQueue 也放满时，会触发拒绝策略 AbortPolicy：抛异常 java.util.concurrent.ThreadPoolExecutor.AbortPolicy CallerRunsPolicy：由调用者执行任务 java.util.concurrent.ThreadPoolExecutor.CallerRunsPolicy DiscardPolicy：丢弃任务 java.util.concurrent.ThreadPoolExecutor.DiscardPolicy DiscardOldestPolicy：丢弃最早排队任务 java.util.concurrent.ThreadPoolExecutor.DiscardOldestPolicy 代码说明 day02.TestThreadPoolExecutor 以较为形象的方式演示了线程池的核心组成 3. wait vs sleep 要求 能够说出二者区别 一个共同点，三个不同点 共同点 wait() ，wait(long) 和 sleep(long) 的效果都是让当前线程暂时放弃 CPU 的使用权，进入阻塞状态 不同点 方法归属不同 sleep(long) 是 Thread 的静态方法 而 wait()，wait(long) 都是 Object 的成员方法，每个对象都有 醒来时机不同 执行 sleep(long) 和 wait(long) 的线程都会在等待相应毫秒后醒来 wait(long) 和 wait() 还可以被 notify 唤醒，wait() 如果不唤醒就一直等下去 它们都可以被打断唤醒 锁特性不同（重点） wait 方法的调用必须先获取 wait 对象的锁，而 sleep 则无此限制 wait 方法执行后会释放对象锁，允许其它线程获得该对象锁（我放弃 cpu，但你们还可以用） 而 sleep 如果在 synchronized 代码块中执行，并不会释放对象锁（我放弃 cpu，你们也用不了） 4. lock vs synchronized 要求 掌握 lock 与 synchronized 的区别 理解 ReentrantLock 的公平、非公平锁 理解 ReentrantLock 中的条件变量 三个层面 不同点 语法层面 synchronized 是关键字，源码在 jvm 中，用 c++ 语言实现 Lock 是接口，源码由 jdk 提供，用 java 语言实现 使用 synchronized 时，退出同步代码块锁会自动释放，而使用 Lock 时，需要手动调用 unlock 方法释放锁 功能层面 二者均属于悲观锁、都具备基本的互斥、同步、锁重入功能 Lock 提供了许多 synchronized 不具备的功能，例如获取等待状态、公平锁、可打断、可超时、多条件变量 Lock 有适合不同场景的实现，如 ReentrantLock， ReentrantReadWriteLock 性能层面 在没有竞争时，synchronized 做了很多优化，如偏向锁、轻量级锁，性能不赖 在竞争激烈时，Lock 的实现通常会提供更好的性能 公平锁 公平锁的公平体现 已经处在阻塞队列中的线程（不考虑超时）始终都是公平的，先进先出 公平锁是指未处于阻塞队列中的线程来争抢锁，如果队列不为空，则老实到队尾等待 非公平锁是指未处于阻塞队列中的线程来争抢锁，与队列头唤醒的线程去竞争，谁抢到算谁的 公平锁会降低吞吐量，一般不用 条件变量 ReentrantLock 中的条件变量功能类似于普通 synchronized 的 wait，notify，用在当线程获得锁后，发现条件不满足时，临时等待的链表结构 与 synchronized 的等待集合不同之处在于，ReentrantLock 中的条件变量可以有多个，可以实现更精细的等待、唤醒控制 代码说明 day02.TestReentrantLock 用较为形象的方式演示 ReentrantLock 的内部结构 5. volatile 要求 掌握线程安全要考虑的三个问题 掌握 volatile 能解决哪些问题 原子性 起因：多线程下，不同线程的指令发生了交错导致的共享变量的读写混乱 解决：用悲观锁或乐观锁解决，volatile 并不能解决原子性 可见性 起因：由于编译器优化、或缓存优化、或 CPU 指令重排序优化导致的对共享变量所做的修改另外的线程看不到 解决：用 volatile 修饰共享变量，能够防止编译器等优化发生，让一个线程对共享变量的修改对另一个线程可见 有序性 起因：由于编译器优化、或缓存优化、或 CPU 指令重排序优化导致指令的实际执行顺序与编写顺序不一致 解决：用 volatile 修饰共享变量会在读、写共享变量时加入不同的屏障，阻止其他读写操作越过屏障，从而达到阻止重排序的效果 注意： volatile 变量写加的屏障是阻止上方其它写操作越过屏障排到 volatile 变量写之下 volatile 变量读加的屏障是阻止下方其它读操作越过屏障排到 volatile 变量读之上 volatile 读写加入的屏障只能防止同一线程内的指令重排 代码说明 day02.threadsafe.AddAndSubtract 演示原子性 day02.threadsafe.ForeverLoop 演示可见性 注意：本例经实践检验是编译器优化导致的可见性问题 day02.threadsafe.Reordering 演示有序性 需要打成 jar 包后测试 请同时参考视频讲解 6. 悲观锁 vs 乐观锁 要求 掌握悲观锁和乐观锁的区别 对比悲观锁与乐观锁 悲观锁的代表是 synchronized 和 Lock 锁 其核心思想是【线程只有占有了锁，才能去操作共享变量，每次只有一个线程占锁成功，获取锁失败的线程，都得停下来等待】 线程从运行到阻塞、再从阻塞到唤醒，涉及线程上下文切换，如果频繁发生，影响性能 实际上，线程在获取 synchronized 和 Lock 锁时，如果锁已被占用，都会做几次重试操作，减少阻塞的机会 乐观锁的代表是 AtomicInteger，使用 cas 来保证原子性 其核心思想是【无需加锁，每次只有一个线程能成功修改共享变量，其它失败的线程不需要停止，不断重试直至成功】 由于线程一直运行，不需要阻塞，因此不涉及线程上下文切换 它需要多核 cpu 支持，且线程数不应超过 cpu 核数 代码说明 day02.SyncVsCas 演示了分别使用乐观锁和悲观锁解决原子赋值 请同时参考视频讲解 7. Hashtable vs ConcurrentHashMap 要求 掌握 Hashtable 与 ConcurrentHashMap 的区别 掌握 ConcurrentHashMap 在不同版本的实现区别 更形象的演示，见资料中的 hash-demo.jar，运行需要 jdk14 以上环境，进入 jar 包目录，执行下面命令 1java -jar --add-exports java.base/jdk.internal.misc=ALL-UNNAMED hash-demo.jar Hashtable 对比 ConcurrentHashMap Hashtable 与 ConcurrentHashMap 都是线程安全的 Map 集合 Hashtable 并发度低，整个 Hashtable 对应一把锁，同一时刻，只能有一个线程操作它 ConcurrentHashMap 并发度高，整个 ConcurrentHashMap 对应多把锁，只要线程访问的是不同锁，那么不会冲突 ConcurrentHashMap 1.7 数据结构：Segment(大数组) + HashEntry(小数组) + 链表，每个 Segment 对应一把锁，如果多个线程访问不同的 Segment，则不会冲突 并发度：Segment 数组大小即并发度，决定了同一时刻最多能有多少个线程并发访问。Segment 数组不能扩容，意味着并发度在 ConcurrentHashMap 创建时就固定了 索引计算 假设大数组长度是 2m2^m2m，key 在大数组内的索引是 key 的二次 hash 值的高 m 位 假设小数组长度是 2n2^n2n，key 在小数组内的索引是 key 的二次 hash 值的低 n 位 扩容：每个小数组的扩容相对独立，小数组在超过扩容因子时会触发扩容，每次扩容翻倍 Segment[0] 原型：首次创建其它小数组时，会以此原型为依据，数组长度，扩容因子都会以原型为准 ConcurrentHashMap 1.8 数据结构：Node 数组 + 链表或红黑树，数组的每个头节点作为锁，如果多个线程访问的头节点不同，则不会冲突。一个位置上首次生成头节点时如果发生竞争，利用 cas 而非 syncronized，进一步提升性能。后续继续在这个位置上添加节点才会利用syncronized。 并发度：Node 数组有多大，并发度就有多大，与 1.7 不同，Node 数组可以扩容 扩容条件：Node 数组满 3/4 时就会扩容 扩容单位：以链表为单位从后向前迁移链表，迁移完成的将旧数组头节点替换为 ForwardingNode 扩容时并发 get 根据是否为 ForwardingNode 来决定是在新数组查找还是在旧数组查找，不会阻塞 如果链表长度超过 1，则需要对节点进行复制（创建新节点），怕的是节点迁移后 next 指针改变 如果链表最后几个元素扩容后索引不变，则节点无需复制 扩容时并发 put 如果 put 的线程与扩容线程操作的链表是同一个，put 线程会阻塞 如果 put 的线程操作的链表还未迁移完成，即头节点不是 ForwardingNode，则可以并发执行 如果 put 的线程操作的链表已经迁移完成，即头结点是 ForwardingNode，则可以协助扩容 与 1.7 相比是懒惰初始化 capacity 代表预估的元素个数，capacity / factory 来计算出初始数组大小，需要贴近 2n2^n2n loadFactor 只在计算初始数组大小时被使用，之后扩容固定为 3/4 超过树化阈值时的扩容问题，如果容量已经是 64，直接树化，否则在原来容量基础上做 3 轮扩容 8. ThreadLocal 要求 掌握 ThreadLocal 的作用与原理 掌握 ThreadLocal 的内存释放时机 作用 ThreadLocal 可以实现【资源对象】的线程隔离，让每个线程各用各的【资源对象】，避免争用引发的线程安全问题 ThreadLocal 同时实现了线程内的资源共享 原理 每个线程内有一个 ThreadLocalMap 类型的成员变量，用来存储资源对象 调用 set 方法，就是以 ThreadLocal 自己作为 key，资源对象作为 value，放入当前线程的 ThreadLocalMap 集合中 调用 get 方法，就是以 ThreadLocal 自己作为 key，到当前线程中查找关联的资源值 调用 remove 方法，就是以 ThreadLocal 自己作为 key，移除当前线程关联的资源值 ThreadLocalMap 的一些特点 key 的 hash 值统一分配 初始容量 16，扩容因子 2/3，扩容容量翻倍 key 索引冲突后用开放寻址法解决冲突 弱引用 key ThreadLocalMap 中的 key 被设计为弱引用，原因如下 Thread 可能需要长时间运行（如线程池中的线程），如果 key 不再使用，需要在内存不足（GC）时释放其占用的内存 内存释放时机 被动 GC 释放 key 仅是让 key 的内存释放，关联 value 的内存并不会释放 懒惰被动释放 value get key 时，发现是 null key，则释放其 value 内存 set key 时，会使用启发式扫描，清除临近的 null key 的 value 内存，启发次数与元素个数，是否发现 null key 有关 主动 remove 释放 key，value 会同时释放 key，value 的内存，也会清除临近的 null key 的 value 内存 推荐使用它，因为一般使用 ThreadLocal 时都把它作为静态变量（即强引用），因此无法被动依靠 GC 回收 虚拟机篇 1. JVM 内存结构 要求 掌握 JVM 内存结构划分 尤其要知道方法区、永久代、元空间的关系 结合一段 java 代码的执行理解内存划分 执行 javac 命令编译源代码为字节码 执行 java 命令 创建 JVM，调用类加载子系统加载 class，将类的信息存入方法区 创建 main 线程，使用的内存区域是 JVM 虚拟机栈，开始执行 main 方法代码 如果遇到了未见过的类，会继续触发类加载过程，同样会存入方法区 需要创建对象，会使用堆内存来存储对象 不再使用的对象，会由垃圾回收器在内存不足时回收其内存 调用方法时，方法内的局部变量、方法参数所使用的是 JVM 虚拟机栈中的栈帧内存 调用方法时，先要到方法区获得到该方法的字节码指令，由解释器将字节码指令解释为机器码执行 调用方法时，会将要执行的指令行号读到程序计数器，这样当发生了线程切换，恢复时就可以从中断的位置继续 对于非 java 实现的方法调用，使用内存称为本地方法栈（见说明） 对于热点方法调用，或者频繁的循环代码，由 JIT 即时编译器将这些代码编译成机器码缓存，提高执行性能 说明 加粗字体代表了 JVM 虚拟机组件 对于 Oracle 的 Hotspot 虚拟机实现，不区分虚拟机栈和本地方法栈 会发生内存溢出的区域 不会出现内存溢出的区域 – 程序计数器 出现 OutOfMemoryError 的情况 堆内存耗尽 – 对象越来越多，又一直在使用，不能被垃圾回收 方法区内存耗尽 – 加载的类越来越多，很多框架都会在运行期间动态产生新的类 虚拟机栈累积 – 每个线程最多会占用 1 M 内存，线程个数越来越多，而又长时间运行不销毁时 出现 StackOverflowError 的区域 JVM 虚拟机栈，原因有方法递归调用未正确结束、反序列化 json 时循环引用 方法区、永久代、元空间 方法区是 JVM 规范中定义的一块内存区域，用来存储类元数据、方法字节码、即时编译器需要的信息等 永久代是 Hotspot 虚拟机对 JVM 规范的实现（1.8 之前） 元空间是 Hotspot 虚拟机对 JVM 规范的另一种实现（1.8 以后），使用本地内存作为这些信息的存储空间 从这张图学到三点 当第一次用到某个类是，由类加载器将 class 文件的类元信息读入，并存储于元空间 X，Y 的类元信息是存储于元空间中，无法直接访问 可以用 X.class，Y.class 间接访问类元信息，它们俩属于 java 对象，我们的代码中可以使用 从这张图可以学到 堆内存中：当一个类加载器对象，这个类加载器对象加载的所有类对象，这些类对象对应的所有实例对象都没人引用时，GC 时就会对它们占用的对内存进行释放 元空间中：内存释放以类加载器为单位，当堆中类加载器内存释放时，对应的元空间中的类元信息也会释放 2. JVM 内存参数 要求 熟悉常见的 JVM 参数，尤其和大小相关的 堆内存，按大小设置 解释： -Xms 最小堆内存（包括新生代和老年代） -Xmx 最大对内存（包括新生代和老年代） 通常建议将 -Xms 与 -Xmx 设置为大小相等，即不需要保留内存，不需要从小到大增长，这样性能较好 -XX:NewSize 与 -XX:MaxNewSize 设置新生代的最小与最大值，但一般不建议设置，由 JVM 自己控制 -Xmn 设置新生代大小，相当于同时设置了 -XX:NewSize 与 -XX:MaxNewSize 并且取值相等 保留是指，一开始不会占用那么多内存，随着使用内存越来越多，会逐步使用这部分保留内存。下同 堆内存，按比例设置 解释： -XX:NewRatio=2:1 表示老年代占两份，新生代占一份 -XX:SurvivorRatio=4:1 表示新生代分成六份，伊甸园占四份，from 和 to 各占一份 元空间内存设置 解释： class space 存储类的基本信息，最大值受 -XX:CompressedClassSpaceSize 控制 non-class space 存储除类的基本信息以外的其它信息（如方法字节码、注解等） class space 和 non-class space 总大小受 -XX:MaxMetaspaceSize 控制 注意： 这里 -XX:CompressedClassSpaceSize 这段空间还与是否开启了指针压缩有关，这里暂不深入展开，可以简单认为指针压缩默认开启 代码缓存内存设置 解释： 如果 -XX:ReservedCodeCacheSize &lt; 240m，所有优化机器代码不加区分存在一起 否则，分成三个区域（图中笔误 mthod 拼写错误，少一个 e） non-nmethods - JVM 自己用的代码 profiled nmethods - 部分优化的机器码 non-profiled nmethods - 完全优化的机器码 线程内存设置 官方参考文档 https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE 3. JVM 垃圾回收 要求 掌握垃圾回收算法 掌握分代回收思想 理解三色标记及漏标处理 了解常见垃圾回收器 三种垃圾回收算法 标记清除法 解释： 找到 GC Root 对象，即那些一定不会被回收的对象，如正执行方法内局部变量引用的对象、静态变量引用的对象 标记阶段：沿着 GC Root 对象的引用链找，直接或间接引用到的对象加上标记 清除阶段：释放未加标记的对象占用的内存 要点： 标记速度与存活对象线性关系 清除速度与内存大小线性关系 缺点是会产生内存碎片 标记整理法 解释： 前面的标记阶段、清理阶段与标记清除法类似 多了一步整理的动作，将存活对象向一端移动，可以避免内存碎片产生 特点： 标记速度与存活对象线性关系 清除与整理速度与内存大小成线性关系 缺点是性能上较慢 标记复制法 解释： 将整个内存分成两个大小相等的区域，from 和 to，其中 to 总是处于空闲，from 存储新创建的对象 标记阶段与前面的算法类似 在找出存活对象后，会将它们从 from 复制到 to 区域，复制的过程中自然完成了碎片整理 复制完成后，交换 from 和 to 的位置即可 特点： 标记与复制速度与存活对象成线性关系 缺点是会占用成倍的空间 GC 与分代回收算法 GC 的目的在于实现无用对象内存自动释放，减少内存碎片、加快分配速度 GC 要点： 回收区域是堆内存，不包括虚拟机栈 判断无用对象，使用可达性分析算法，三色标记法标记存活对象，回收未标记对象 GC 具体的实现称为垃圾回收器 GC 大都采用了分代回收思想 理论依据是大部分对象朝生夕灭，用完立刻就可以回收，另有少部分对象会长时间存活，每次很难回收 根据这两类对象的特性将回收区域分为新生代和老年代，新生代采用标记复制法、老年代一般采用标记整理法 根据 GC 的规模可以分成 Minor GC，Mixed GC，Full GC 分代回收 伊甸园 eden，最初对象都分配到这里，与幸存区 survivor（分成 from 和 to）合称新生代， 当伊甸园内存不足，标记伊甸园与 from（现阶段没有）的存活对象 将存活对象采用复制算法复制到 to 中，复制完毕后，伊甸园和 from 内存都得到释放 将 from 和 to 交换位置 经过一段时间后伊甸园的内存又出现不足 标记伊甸园与 from（现阶段没有）的存活对象 将存活对象采用复制算法复制到 to 中 复制完毕后，伊甸园和 from 内存都得到释放 将 from 和 to 交换位置 老年代 old，当幸存区对象熬过几次回收（最多15次），晋升到老年代（幸存区内存不足或大对象会导致提前晋升） GC 规模 Minor GC 发生在新生代的垃圾回收，暂停时间短 Mixed GC 新生代 + 老年代部分区域的垃圾回收，G1 收集器特有 Full GC 新生代 + 老年代完整垃圾回收，暂停时间长，应尽力避免 三色标记 即用三种颜色记录对象的标记状态 黑色 – 已标记 灰色 – 标记中 白色 – 还未标记 起始的三个对象还未处理完成，用灰色表示 该对象的引用已经处理完成，用黑色表示，黑色引用的对象变为灰色 依次类推 沿着引用链都标记了一遍 最后为标记的白色对象，即为垃圾 并发漏标问题 比较先进的垃圾回收器都支持并发标记，即在标记过程中，用户线程仍然能工作。但这样带来一个新的问题，如果用户线程修改了对象引用，那么就存在漏标问题。例如： 如图所示标记工作尚未完成 用户线程同时在工作，断开了第一层 3、4 两个对象之间的引用，这时对于正在处理 3 号对象的垃圾回收线程来讲，它会将 4 号对象当做是白色垃圾 但如果其他用户线程又建立了 2、4 两个对象的引用，这时因为 2 号对象是黑色已处理对象了，因此垃圾回收线程不会察觉到这个引用关系的变化，从而产生了漏标 如果用户线程让黑色对象引用了一个新增对象，一样会存在漏标问题 因此对于并发标记而言，必须解决漏标问题，也就是要记录标记过程中的变化。有两种解决方法： Incremental Update 增量更新法，CMS 垃圾回收器采用 思路是拦截每次赋值动作，只要赋值发生，被赋值的对象就会被记录下来，在重新标记阶段再确认一遍 Snapshot At The Beginning，SATB 原始快照法，G1 垃圾回收器采用 思路也是拦截每次赋值动作，不过记录的对象不同，也需要在重新标记阶段对这些对象二次处理 新加对象会被记录 被删除引用关系的对象也被记录 垃圾回收器 - Parallel GC eden 内存不足发生 Minor GC，采用标记复制算法，需要暂停用户线程 old 内存不足发生 Full GC，采用标记整理算法，需要暂停用户线程 注重吞吐量 垃圾回收器 - ConcurrentMarkSweep GC 它是工作在 old 老年代，支持并发标记的一款回收器，采用并发清除算法 并发标记时不需暂停用户线程 重新标记时仍需暂停用户线程 如果并发失败（即回收速度赶不上创建新对象速度），会触发 Full GC 注重响应时间 垃圾回收器 - G1 GC 响应时间与吞吐量兼顾 划分成多个区域，每个区域都可以充当 eden，survivor，old， humongous，其中 humongous 专为大对象准备 分成三个阶段：新生代回收、并发标记、混合收集 如果并发失败（即回收速度赶不上创建新对象速度），会触发 Full GC G1 回收阶段 - 新生代回收 初始时，所有区域都处于空闲状态 创建了一些对象，挑出一些空闲区域作为伊甸园区存储这些对象 当伊甸园需要垃圾回收时，挑出一个空闲区域作为幸存区，用复制算法复制存活对象，需要暂停用户线程 复制完成，将之前的伊甸园内存释放 随着时间流逝，伊甸园的内存又有不足 将伊甸园以及之前幸存区中的存活对象，采用复制算法，复制到新的幸存区，其中较老对象晋升至老年代 释放伊甸园以及之前幸存区的内存 G1 回收阶段 - 并发标记与混合收集 当老年代占用内存超过阈值后，触发并发标记，这时无需暂停用户线程 并发标记之后，会有重新标记阶段解决漏标问题，此时需要暂停用户线程。这些都完成后就知道了老年代有哪些存活对象，随后进入混合收集阶段。此时不会对所有老年代区域进行回收，而是根据暂停时间目标优先回收价值高（存活对象少）的区域（这也是 Gabage First 名称的由来）。 混合收集阶段中，参与复制的有 eden、survivor、old，下图显示了伊甸园和幸存区的存活对象复制 下图显示了老年代和幸存区晋升的存活对象的复制 复制完成，内存得到释放。进入下一轮的新生代回收、并发标记、混合收集 4. 内存溢出 要求 能够说出几种典型的导致内存溢出的情况 典型情况 误用线程池导致的内存溢出 参考 day03.TestOomThreadPool 任务队列使用无界队列时，重复多次往队列中添加任务，占满内存时会出现内存溢出 任务队列使用有界队列，但救急线程数过多时，创建过多救急线程将内存占满也会出现内存溢出 查询数据量太大导致的内存溢出 参考 day03.TestOomTooManyObject 动态生成类导致的内存溢出 参考 day03.TestOomTooManyClass 5. 类加载 要求 掌握类加载阶段 掌握类加载器 理解双亲委派机制 类加载过程的三个阶段 加载 将类的字节码载入方法区，并创建类.class 对象 如果此类的父类没有加载，先加载父类 加载是懒惰执行 链接 验证 – 验证类是否符合 Class 规范，合法性、安全性检查 准备 – 为 static 变量分配空间，设置默认值 解析 – 将常量池的符号引用解析为直接引用 初始化 静态代码块、static 修饰的变量赋值、static final 修饰的引用类型变量赋值，会被合并成一个 &lt;cinit&gt; 方法，在初始化时被调用 static final 修饰的基本类型变量赋值，在链接阶段就已完成 初始化是懒惰执行 验证手段 使用 jps 查看进程号 使用 jhsdb 调试，执行命令 jhsdb.exe hsdb 打开它的图形界面 Class Browser 可以查看当前 jvm 中加载了哪些类 控制台的 universe 命令查看堆内存范围 控制台的 g1regiondetails 命令查看 region 详情 scanoops 起始地址 结束地址 对象类型 可以根据类型查找某个区间内的对象地址 控制台的 inspect 地址 指令能够查看这个地址对应的对象详情 使用 javap 命令可以查看 class 字节码 代码说明 day03.loader.TestLazy - 验证类的加载是懒惰的，用到时才触发类加载 day03.loader.TestFinal - 验证使用 final 修饰的变量不会触发类加载 jdk 8 的类加载器 名称 加载哪的类 说明 Bootstrap ClassLoader JAVA_HOME/jre/lib 无法直接访问 ClassLoader JAVA_HOME/jre/lib/ext 上级为 Bootstrap，显示为 null Application ClassLoader classpath 上级为 Extension 自定义类加载器 自定义 上级为 Application 双亲委派机制 所谓的双亲委派，就是指优先委派上级类加载器进行加载，如果上级类加载器 能找到这个类，由上级加载，加载后该类也对下级加载器可见 找不到这个类，则下级类加载器才有资格执行加载 双亲委派的目的有两点 让上级类加载器中的类对下级共享（反之不行），即能让你的类能依赖到 jdk 提供的核心类 让类的加载有优先次序，保证核心类优先加载 对双亲委派的误解 下面面试题的回答是错误的 错在哪了？ 自己编写类加载器就能加载一个假冒的 java.lang.System 吗? 答案是不行。 假设你自己的类加载器用双亲委派，那么优先由启动类加载器加载真正的 java.lang.System，自然不会加载假冒的 假设你自己的类加载器不用双亲委派，那么你的类加载器加载假冒的 java.lang.System 时，它需要先加载父类 java.lang.Object，而你没有用委派，找不到 java.lang.Object 所以加载会失败 以上也仅仅是假设。事实上操作你就会发现，自定义类加载器加载以 java. 打头的类时，会抛安全异常，在 jdk9 以上版本这些特殊包名都与模块进行了绑定，更连编译都过不了 代码说明 day03.loader.TestJdk9ClassLoader - 演示类加载器与模块的绑定关系 6. 四种引用 要求 掌握四种引用 强引用 普通变量赋值即为强引用，如 A a = new A(); 通过 GC Root 的引用链，如果强引用不到该对象，该对象才能被回收 软引用（SoftReference） 例如：SoftReference a = new SoftReference(new A()); 如果仅有软引用该对象时，首次垃圾回收不会回收该对象，如果内存仍不足，再次回收时才会释放对象 软引用自身需要配合引用队列来释放 典型例子是反射数据 弱引用（WeakReference） 例如：WeakReference a = new WeakReference(new A()); 如果仅有弱引用引用该对象时，只要发生垃圾回收，就会释放该对象 弱引用自身需要配合引用队列来释放 典型例子是 ThreadLocalMap 中的 Entry 对象 虚引用（PhantomReference） 例如： PhantomReference a = new PhantomReference(new A(), referenceQueue); 必须配合引用队列一起使用，当虚引用所引用的对象被回收时，由 Reference Handler 线程将虚引用对象入队，这样就可以知道哪些对象被回收，从而对它们关联的资源做进一步处理 典型例子是 Cleaner 释放 DirectByteBuffer 关联的直接内存 代码说明 day03.reference.TestPhantomReference - 演示虚引用的基本用法 day03.reference.TestWeakReference - 模拟 ThreadLocalMap, 采用引用队列释放 entry 内存 7. finalize 要求 掌握 finalize 的工作原理与缺点 finalize 它是 Object 中的一个方法，如果子类重写它，垃圾回收时此方法会被调用，可以在其中进行资源释放和清理工作 将资源释放和清理放在 finalize 方法中非常不好，非常影响性能，严重时甚至会引起 OOM，从 Java9 开始就被标注为 @Deprecated，不建议被使用了 finalize 原理 对 finalize 方法进行处理的核心逻辑位于 java.lang.ref.Finalizer 类中，它包含了名为 unfinalized 的静态变量（双向链表结构），Finalizer 也可被视为另一种引用对象（地位与软、弱、虚相当，只是不对外，无法直接使用） 当重写了 finalize 方法的对象，在构造方法调用之时，JVM 都会将其包装成一个 Finalizer 对象，并加入 unfinalized 链表中 Finalizer 类中还有另一个重要的静态变量，即 ReferenceQueue 引用队列，刚开始它是空的。当狗对象可以被当作垃圾回收时，就会把这些狗对象对应的 Finalizer 对象加入此引用队列 但此时 Dog 对象还没法被立刻回收，因为 unfinalized -&gt; Finalizer 这一引用链还在引用它嘛，为的是【先别着急回收啊，等我调完 finalize 方法，再回收】 FinalizerThread 线程会从 ReferenceQueue 中逐一取出每个 Finalizer 对象，把它们从链表断开并真正调用 finallize 方法 由于整个 Finalizer 对象已经从 unfinalized 链表中断开，这样没谁能引用到它和狗对象，所以下次 gc 时就被回收了 finalize 缺点 无法保证资源释放：FinalizerThread 是守护线程，代码很有可能没来得及执行完，线程就结束了 无法判断是否发生错误：执行 finalize 方法时，会吞掉任意异常（Throwable） 内存释放不及时：重写了 finalize 方法的对象在第一次被 gc 时，并不能及时释放它占用的内存，因为要等着 FinalizerThread 调用完 finalize，把它从 unfinalized 队列移除后，第二次 gc 时才能真正释放内存 可以想象gc本就因为内存不足引起，finalize调用又很慢（两个队列的移除操作，都是串行执行的，用来释放连接类的资源也应该不快），不能及时释放内存，对象释放不及时就会逐渐移入老年代，老年代垃圾积累过多就会容易full gc，full gc后释放速度如果仍跟不上创建新 对象的速度，就会OOM 有的文章提到【Finalizer 线程会和我们的主线程进行竞争，不过由于它的优先级较低，获取到的CPU时间较少，因此它永远也赶不上主线程的步伐】这个显然是错误的，FinalizerThread 的优先级较普通线程更高，原因应该是 finalize 串行执行慢等原因综合导致 代码说明 day03.reference.TestFinalize - finalize 的测试代码 框架篇 1. Spring refresh 流程 要求 掌握 refresh 的 12 个步骤 Spring refresh 概述 refresh 是 AbstractApplicationContext 中的一个方法，负责初始化 ApplicationContext 容器，容器必须调用 refresh 才能正常工作。它的内部主要会调用 12 个方法，我们把它们称为 refresh 的 12 个步骤： prepareRefresh obtainFreshBeanFactory prepareBeanFactory postProcessBeanFactory invokeBeanFactoryPostProcessors registerBeanPostProcessors initMessageSource initApplicationEventMulticaster onRefresh registerListeners finishBeanFactoryInitialization finishRefresh 功能分类 1 为准备环境 2 3 4 5 6 为准备 BeanFactory 7 8 9 10 12 为准备 ApplicationContext 11 为初始化 BeanFactory 中非延迟单例 bean 1. prepareRefresh 这一步创建和准备了 Environment 对象，它作为 ApplicationContext 的一个成员变量 Environment 对象的作用之一是为后续 @Value，值注入时提供键值 Environment 分成三个主要部分 systemProperties - 保存 java 环境键值 systemEnvironment - 保存系统环境键值 自定义 PropertySource - 保存自定义键值，例如来自于 *.properties 文件的键值 2. obtainFreshBeanFactory 这一步获取（或创建） BeanFactory，它也是作为 ApplicationContext 的一个成员变量 BeanFactory 的作用是负责 bean 的创建、依赖注入和初始化，bean 的各项特征由 BeanDefinition 定义 BeanDefinition 作为 bean 的设计蓝图，规定了 bean 的特征，如单例多例、依赖关系、初始销毁方法等 BeanDefinition 的来源有多种多样，可以是通过 xml 获得、配置类获得、组件扫描获得，也可以是编程添加 所有的 BeanDefinition 会存入 BeanFactory 中的 beanDefinitionMap 集合 3. prepareBeanFactory 这一步会进一步完善 BeanFactory，为它的各项成员变量赋值 beanExpressionResolver 用来解析 SpEL，常见实现为 StandardBeanExpressionResolver propertyEditorRegistrars 会注册类型转换器 它在这里使用了 ResourceEditorRegistrar 实现类 并应用 ApplicationContext 提供的 Environment 完成 ${ } 解析 registerResolvableDependency 来注册 beanFactory 以及 ApplicationContext，让它们也能用于依赖注入 beanPostProcessors 是 bean 后处理器集合，会工作在 bean 的生命周期各个阶段，此处会添加两个： ApplicationContextAwareProcessor 用来解析 Aware 接口 ApplicationListenerDetector 用来识别容器中 ApplicationListener 类型的 bean 4. postProcessBeanFactory 这一步是空实现，留给子类扩展。 一般 Web 环境的 ApplicationContext 都要利用它注册新的 Scope，完善 Web 下的 BeanFactory 这里体现的是模板方法设计模式 5. invokeBeanFactoryPostProcessors 这一步会调用 beanFactory 后处理器 beanFactory 后处理器，充当 beanFactory 的扩展点，可以用来补充或修改 BeanDefinition 常见的 beanFactory 后处理器有 ConfigurationClassPostProcessor – 解析 @Configuration、@Bean、@Import、@PropertySource 等 PropertySourcesPlaceHolderConfigurer – 替换 BeanDefinition 中的 ${ } MapperScannerConfigurer – 补充 Mapper 接口对应的 BeanDefinition 6. registerBeanPostProcessors 这一步是继续从 beanFactory 中找出 bean 后处理器，添加至 beanPostProcessors 集合中 bean 后处理器，充当 bean 的扩展点，可以工作在 bean 的实例化、依赖注入、初始化阶段，常见的有： AutowiredAnnotationBeanPostProcessor 功能有：解析 @Autowired，@Value 注解 CommonAnnotationBeanPostProcessor 功能有：解析 @Resource，@PostConstruct，@PreDestroy AnnotationAwareAspectJAutoProxyCreator 功能有：为符合切点的目标 bean 自动创建代理 7. initMessageSource 这一步是为 ApplicationContext 添加 messageSource 成员，实现国际化功能 去 beanFactory 内找名为 messageSource 的 bean，如果没有，则提供空的 MessageSource 实现 8. initApplicationContextEventMulticaster 这一步为 ApplicationContext 添加事件广播器成员，即 applicationContextEventMulticaster 它的作用是发布事件给监听器 去 beanFactory 找名为 applicationEventMulticaster 的 bean 作为事件广播器，若没有，会创建默认的事件广播器 之后就可以调用 ApplicationContext.publishEvent(事件对象) 来发布事件 9. onRefresh 这一步是空实现，留给子类扩展 SpringBoot 中的子类在这里准备了 WebServer，即内嵌 web 容器 体现的是模板方法设计模式 10. registerListeners 这一步会从多种途径找到事件监听器，并添加至 applicationEventMulticaster 事件监听器顾名思义，用来接收事件广播器发布的事件，有如下来源 事先编程添加的 来自容器中的 bean 来自于 @EventListener 的解析 要实现事件监听器，只需要实现 ApplicationListener 接口，重写其中 onApplicationEvent(E e) 方法即可 11. finishBeanFactoryInitialization 这一步会将 beanFactory 的成员补充完毕，并初始化所有非延迟单例 bean conversionService 也是一套转换机制，作为对 PropertyEditor 的补充 embeddedValueResolvers 即内嵌值解析器，用来解析 @Value 中的 ${ }，借用的是 Environment 的功能 singletonObjects 即单例池，缓存所有单例对象 对象的创建都分三个阶段，每一阶段都有不同的 bean 后处理器参与进来，扩展功能 12. finishRefresh 这一步会为 ApplicationContext 添加 lifecycleProcessor 成员，用来控制容器内需要生命周期管理的 bean 如果容器中有名称为 lifecycleProcessor 的 bean 就用它，否则创建默认的生命周期管理器 准备好生命周期管理器，就可以实现 调用 context 的 start，即可触发所有实现 LifeCycle 接口 bean 的 start 调用 context 的 stop，即可触发所有实现 LifeCycle 接口 bean 的 stop 发布 ContextRefreshed 事件，整个 refresh 执行完成 总结 2. Spring bean 生命周期 要求 掌握 Spring bean 的生命周期 bean 生命周期 概述 bean 的生命周期从调用 beanFactory 的 getBean 开始，到这个 bean 被销毁，可以总结为以下七个阶段： 处理名称，检查缓存 处理父子容器 处理 dependsOn 选择 scope 策略 创建 bean ​ ① 创建 bean 实例- @Autowired，唯一带参构造，默认构造 ​ ② 依赖注入-@Autowired @Value,@Resource,ByName ByType,精确指定 ​ ③ 初始化-Aware接口处理,@PostConstruct,InitializingBean,initMethod ​ ④ 创建代理 ​ ⑤ 登记可销毁 bean ​ 6. 类型转换处理 ​ 7. 销毁 bean 注意 划分的阶段和名称并不重要，重要的是理解整个过程中做了哪些事情 1. 处理名称，检查缓存 这一步会处理别名，将别名解析为实际名称 对 FactoryBean 也会特殊处理，如果以 &amp; 开头表示要获取 FactoryBean 本身，否则表示要获取其产品 这里针对单例对象会检查一级、二级、三级缓存 singletonFactories 三级缓存，存放单例工厂对象 earlySingletonObjects 二级缓存，存放单例工厂的产品对象 如果发生循环依赖，产品是代理；无循环依赖，产品是原始对象 singletonObjects 一级缓存，存放单例成品对象 2. 处理父子容器 如果当前容器根据名字找不到这个 bean，此时若父容器存在，则执行父容器的 getBean 流程 父子容器的 bean 名称可以重复 3. 处理 dependsOn 如果当前 bean 有通过 dependsOn 指定了非显式依赖的 bean，这一步会提前创建这些 dependsOn 的 bean 所谓非显式依赖，就是指两个 bean 之间不存在直接依赖关系，但需要控制它们的创建先后顺序 4. 选择 scope 策略 对于 singleton scope，首先到单例池去获取 bean，如果有则直接返回，没有再进入创建流程 对于 prototype scope，每次都会进入创建流程 对于自定义 scope，例如 request，首先到 request 域获取 bean，如果有则直接返回，没有再进入创建流程 5.1 创建 bean - 创建 bean 实例 要点 总结 有自定义 TargetSource 的情况 由 AnnotationAwareAspectJAutoProxyCreator 创建代理返回 Supplier 方式创建 bean 实例 为 Spring 5.0 新增功能，方便编程方式创建 bean 实例 FactoryMethod 方式 创建 bean 实例 ① 分成静态工厂与实例工厂；② 工厂方法若有参数，需要对工厂方法参数进行解析，利用 resolveDependency；③ 如果有多个工厂方法候选者，还要进一步按权重筛选 AutowiredAnnotationBeanPostProcessor ① 优先选择带 @Autowired 注解的构造；② 若有唯一的带参构造，也会入选 mbd.getPreferredConstructors 选择所有公共构造，这些构造之间按权重筛选 采用默认构造 如果上面的后处理器和 BeanDefiniation 都没找到构造，采用默认构造，即使是私有的 5.2 创建 bean - 依赖注入 要点 总结 AutowiredAnnotationBeanPostProcessor(注解匹配) 识别 @Autowired 及 @Value 标注的成员，封装为 InjectionMetadata 进行依赖注入 CommonAnnotationBeanPostProcessor（注解匹配） 识别 @Resource 标注的成员，封装为 InjectionMetadata 进行依赖注入 resolveDependency 用来查找要装配的值，可以识别：① Optional；② ObjectFactory 及 ObjectProvider；③ @Lazy 注解；④ @Value 注解（${ }, #{ }, 类型转换）；⑤ 集合类型（Collection，Map，数组等）；⑥ 泛型和 @Qualifier（用来区分类型歧义）；⑦ primary 及名字匹配（用来区分类型歧义） AUTOWIRE_BY_NAME（根据名字匹配） 根据成员名字找 bean 对象，修改 mbd 的 propertyValues，不会考虑简单类型的成员 AUTOWIRE_BY_TYPE（根据类型匹配） 根据成员类型执行 resolveDependency 找到依赖注入的值，修改 mbd 的 propertyValues applyPropertyValues（即xml中&lt;property name ref|value/&gt;）(精确指定) 根据 mbd 的 propertyValues 进行依赖注入（即xml中 `&lt;property name ref 5.3 创建 bean - 初始化 要点 总结 内置 Aware 接口的装配 包括 BeanNameAware，BeanFactoryAware 等 扩展 Aware 接口的装配 由 ApplicationContextAwareProcessor 解析，执行时机在 postProcessBeforeInitialization @PostConstruct 由 CommonAnnotationBeanPostProcessor 解析，执行时机在 postProcessBeforeInitialization InitializingBean 通过接口回调执行初始化 initMethod 根据 BeanDefinition 得到的初始化方法执行初始化，即 &lt;bean init-method&gt; 或 @Bean(initMethod) 创建 aop 代理 由 AnnotationAwareAspectJAutoProxyCreator 创建，执行时机在 postProcessAfterInitialization 5.4 创建 bean - 注册可销毁 bean 在这一步判断并登记可销毁 bean 判断依据 如果实现了 DisposableBean 或 AutoCloseable 接口，则为可销毁 bean 如果自定义了 destroyMethod，则为可销毁 bean 如果采用 @Bean 没有指定 destroyMethod，则采用自动推断方式获取销毁方法名（close，shutdown） 如果有 @PreDestroy 标注的方法 存储位置 singleton scope 的可销毁 bean 会存储于 beanFactory 的成员当中 自定义 scope 的可销毁 bean 会存储于对应的域对象当中 prototype scope 不会存储，需要自己找到此对象销毁 存储时都会封装为 DisposableBeanAdapter 类型对销毁方法的调用进行适配 6. 类型转换处理 如果 getBean 的 requiredType 参数与实际得到的对象类型不同，会尝试进行类型转换 7. 销毁 bean 销毁时机 singleton bean 的销毁在 ApplicationContext.close 时，此时会找到所有 DisposableBean 的名字，逐一销毁 自定义 scope bean 的销毁在作用域对象生命周期结束时 prototype bean 的销毁可以通过自己手动调用 AutowireCapableBeanFactory.destroyBean 方法执行销毁 同一 bean 中不同形式销毁方法的调用次序 优先后处理器销毁，即 @PreDestroy 其次 DisposableBean 接口销毁 最后 destroyMethod 销毁（包括自定义名称，推断名称，AutoCloseable 接口 多选一） 3. Spring bean 循环依赖 要求 掌握单例 set 方式循环依赖的原理 掌握其它循环依赖的解决方法 循环依赖的产生 首先要明白，bean 的创建要遵循一定的步骤，必须是创建、注入、初始化三步，这些顺序不能乱 set 方法（包括成员变量）的循环依赖如图所示 可以在【a 创建】和【a set 注入 b】之间加入 b 的整个流程来解决 【b set 注入 a】 时可以成功，因为之前 a 的实例已经创建完毕 a 的顺序，及 b 的顺序都能得到保障 构造方法的循环依赖如图所示，显然无法用前面的方法解决 构造循环依赖的解决(同时也能解决多例循环依赖) 思路1 a 注入 b 的代理对象，这样能够保证 a 的流程走通 后续需要用到 b 的真实对象时，可以通过代理间接访问 思路2 a 注入 b 的工厂对象，让 b 的实例创建被推迟，这样能够保证 a 的流程先走通 后续需要用到 b 的真实对象时，再通过 ObjectFactory 工厂间接访问 示例1：用 @Lazy 为构造方法参数生成代理 1234567891011121314151617181920212223242526272829303132333435363738394041public class App60_1 &#123; static class A &#123; private static final Logger log = LoggerFactory.getLogger(&quot;A&quot;); private B b; public A(@Lazy B b) &#123; log.debug(&quot;A(B b) &#123;&#125;&quot;, b.getClass()); this.b = b; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; static class B &#123; private static final Logger log = LoggerFactory.getLogger(&quot;B&quot;); private A a; public B(A a) &#123; log.debug(&quot;B(&#123;&#125;)&quot;, a); this.a = a; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; public static void main(String[] args) &#123; GenericApplicationContext context = new GenericApplicationContext(); context.registerBean(&quot;a&quot;, A.class); context.registerBean(&quot;b&quot;, B.class); AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory()); context.refresh(); System.out.println(); &#125;&#125; 示例2：用 ObjectProvider 延迟依赖对象的创建 12345678910111213141516171819202122232425262728293031323334353637383940414243public class App60_2 &#123; static class A &#123; private static final Logger log = LoggerFactory.getLogger(&quot;A&quot;); private ObjectProvider&lt;B&gt; b; public A(ObjectProvider&lt;B&gt; b) &#123; log.debug(&quot;A(&#123;&#125;)&quot;, b); this.b = b; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; static class B &#123; private static final Logger log = LoggerFactory.getLogger(&quot;B&quot;); private A a; public B(A a) &#123; log.debug(&quot;B(&#123;&#125;)&quot;, a); this.a = a; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; public static void main(String[] args) &#123; GenericApplicationContext context = new GenericApplicationContext(); context.registerBean(&quot;a&quot;, A.class); context.registerBean(&quot;b&quot;, B.class); AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory()); context.refresh(); System.out.println(context.getBean(A.class).b.getObject()); System.out.println(context.getBean(B.class)); &#125;&#125; 示例3：用 @Scope 产生代理 12345678910public class App60_3 &#123; public static void main(String[] args) &#123; GenericApplicationContext context = new GenericApplicationContext(); ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(context.getDefaultListableBeanFactory()); scanner.scan(&quot;com.itheima.app60.sub&quot;); context.refresh(); System.out.println(); &#125;&#125; 123456789101112131415@Componentclass A &#123; private static final Logger log = LoggerFactory.getLogger(&quot;A&quot;); private B b; public A(B b) &#123; log.debug(&quot;A(B b) &#123;&#125;&quot;, b.getClass()); this.b = b; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125;&#125; 12345678910111213141516@Scope(proxyMode = ScopedProxyMode.TARGET_CLASS)@Componentclass B &#123; private static final Logger log = LoggerFactory.getLogger(&quot;B&quot;); private A a; public B(A a) &#123; log.debug(&quot;B(&#123;&#125;)&quot;, a); this.a = a; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125;&#125; 示例4：用 Provider 接口解决，原理上与 ObjectProvider 一样，Provider 接口是独立的 jar 包，需要加入依赖 12345&lt;dependency&gt; &lt;groupId&gt;javax.inject&lt;/groupId&gt; &lt;artifactId&gt;javax.inject&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243public class App60_4 &#123; static class A &#123; private static final Logger log = LoggerFactory.getLogger(&quot;A&quot;); private Provider&lt;B&gt; b; public A(Provider&lt;B&gt; b) &#123; log.debug(&quot;A(&#123;&#125;&#125;)&quot;, b); this.b = b; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; static class B &#123; private static final Logger log = LoggerFactory.getLogger(&quot;B&quot;); private A a; public B(A a) &#123; log.debug(&quot;B(&#123;&#125;&#125;)&quot;, a); this.a = a; &#125; @PostConstruct public void init() &#123; log.debug(&quot;init()&quot;); &#125; &#125; public static void main(String[] args) &#123; GenericApplicationContext context = new GenericApplicationContext(); context.registerBean(&quot;a&quot;, A.class); context.registerBean(&quot;b&quot;, B.class); AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory()); context.refresh(); System.out.println(context.getBean(A.class).b.get()); System.out.println(context.getBean(B.class)); &#125;&#125; 解决 set 循环依赖的原理 一级缓存 作用是保证单例对象仅被创建一次 第一次走 getBean(&quot;a&quot;) 流程后，最后会将成品 a 放入 singletonObjects 一级缓存 后续再走 getBean(&quot;a&quot;) 流程时，先从一级缓存中找，这时已经有成品 a，就无需再次创建 一级缓存与循环依赖 一级缓存无法解决循环依赖问题，分析如下 无论是获取 bean a 还是获取 bean b，走的方法都是同一个 getBean 方法，假设先走 getBean(&quot;a&quot;) 当 a 的实例对象创建，接下来执行 a.setB() 时，需要走 getBean(&quot;b&quot;) 流程，红色箭头 1 当 b 的实例对象创建，接下来执行 b.setA() 时，又回到了 getBean(&quot;a&quot;) 的流程，红色箭头 2 但此时 singletonObjects 一级缓存内没有成品的 a，陷入了死循环 二级缓存 解决思路如下： 再增加一个 singletonFactories 缓存 在依赖注入前，即 a.setB() 以及 b.setA() 将 a 及 b 的半成品对象（未完成依赖注入和初始化）放入此缓存 执行依赖注入时，先看看 singletonFactories 缓存中是否有半成品的对象，如果有拿来注入，顺利走完流程 对于上面的图 a = new A() 执行之后就会把这个半成品的 a 放入 singletonFactories 缓存，即 factories.put(a) 接下来执行 a.setB()，走入 getBean(&quot;b&quot;) 流程，红色箭头 3 这回再执行到 b.setA() 时，需要一个 a 对象，有没有呢？有！ factories.get() 在 singletonFactories 缓存中就可以找到，红色箭头 4 和 5 b 的流程能够顺利走完，将 b 成品放入 singletonObject 一级缓存，返回到 a 的依赖注入流程，红色箭头 6 二级缓存与创建代理 二级缓存无法正确处理循环依赖并且包含有代理创建的场景，分析如下 spring 默认要求，在 a.init 完成之后才能创建代理 pa = proxy(a) 由于 a 的代理创建时机靠后，在执行 factories.put(a) 向 singletonFactories 中放入的还是原始对象 接下来箭头 3、4、5 这几步 b 对象拿到和注入的都是原始对象 三级缓存 简单分析的话，只需要将代理的创建时机放在依赖注入之前即可，但 spring 仍然希望代理的创建时机在 init 之后，只有出现循环依赖时，才会将代理的创建时机提前。所以解决思路稍显复杂： 图中 factories.put(fa) 放入的既不是原始对象，也不是代理对象而是工厂对象 fa 当检查出发生循环依赖时，fa 的产品就是代理 pa，没有发生循环依赖，fa 的产品是原始对象 a 假设出现了循环依赖，拿到了 singletonFactories 中的工厂对象，通过在依赖注入前获得了 pa，红色箭头 5 这回 b.setA() 注入的就是代理对象，保证了正确性，红色箭头 7 还需要把 pa 存入新加的 earlySingletonObjects 缓存，红色箭头 6 a.init 完成后，无需二次创建代理，从哪儿找到 pa 呢？earlySingletonObjects 已经缓存，蓝色箭头 9 当成品对象产生，放入 singletonObject 后，singletonFactories 和 earlySingletonObjects 就中的对象就没有用处，清除即可 4. Spring 事务失效 要求 掌握事务失效的八种场景 1. 抛出检查异常导致事务不能正确回滚 12345678910111213141516@Servicepublic class Service1 &#123; @Autowired private AccountMapper accountMapper; @Transactional public void transfer(int from, int to, int amount) throws FileNotFoundException &#123; int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); new FileInputStream(&quot;aaa&quot;); accountMapper.update(to, amount); &#125; &#125;&#125; 原因：Spring 默认只会回滚非检查异常 解法：配置 rollbackFor 属性 @Transactional(rollbackFor = Exception.class) 2. 业务方法内自己 try-catch 异常导致事务不能正确回滚 1234567891011121314151617181920@Servicepublic class Service2 &#123; @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) &#123; try &#123; int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); new FileInputStream(&quot;aaa&quot;); accountMapper.update(to, amount); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 原因：事务通知只有捉到了目标抛出的异常，才能进行后续的回滚处理，如果目标自己处理掉异常，事务通知无法知悉 解法1：异常原样抛出 在 catch 块添加 throw new RuntimeException(e); 解法2：手动设置 TransactionStatus.setRollbackOnly() 在 catch 块添加 TransactionInterceptor.currentTransactionStatus().setRollbackOnly(); 3. aop 切面顺序导致导致事务不能正确回滚 12345678910111213141516@Servicepublic class Service3 &#123; @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) throws FileNotFoundException &#123; int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); new FileInputStream(&quot;aaa&quot;); accountMapper.update(to, amount); &#125; &#125;&#125; 12345678910111213@Aspectpublic class MyAspect &#123; @Around(&quot;execution(* transfer(..))&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; LoggerUtils.get().debug(&quot;log:&#123;&#125;&quot;, pjp.getTarget()); try &#123; return pjp.proceed(); &#125; catch (Throwable e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 原因：事务切面优先级最低，但如果自定义的切面优先级和他一样，则还是自定义切面在内层，这时若自定义切面没有正确抛出异常… 解法1、2：同情况2 中的解法:1、2 解法3：调整切面顺序，在 MyAspect 上添加 @Order(Ordered.LOWEST_PRECEDENCE - 1) （不推荐） 4. 非 public 方法导致的事务失效 123456789101112131415@Servicepublic class Service4 &#123; @Autowired private AccountMapper accountMapper; @Transactional void transfer(int from, int to, int amount) throws FileNotFoundException &#123; int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); &#125; &#125;&#125; 原因：Spring 为方法创建代理、添加事务通知、前提条件都是该方法是 public 的 解法1：改为 public 方法 解法2：添加 bean 配置如下（不推荐） 1234@Beanpublic TransactionAttributeSource transactionAttributeSource() &#123; return new AnnotationTransactionAttributeSource(false);&#125; 5. 父子容器导致的事务失效 12345678910111213141516171819package day04.tx.app.service;// ...@Servicepublic class Service5 &#123; @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) throws FileNotFoundException &#123; int fromBalance = accountMapper.findBalanceBy(from); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); &#125; &#125;&#125; 控制器类 1234567891011121314package day04.tx.app.controller;// ...@Controllerpublic class AccountController &#123; @Autowired public Service5 service; public void transfer(int from, int to, int amount) throws FileNotFoundException &#123; service.transfer(from, to, amount); &#125;&#125; App 配置类 1234567@Configuration@ComponentScan(&quot;day04.tx.app.service&quot;)@EnableTransactionManagement// ...public class AppConfig &#123; // ... 有事务相关配置&#125; Web 配置类 123456@Configuration@ComponentScan(&quot;day04.tx.app&quot;)// ...public class WebConfig &#123; // ... 无事务配置&#125; 现在配置了父子容器，WebConfig 对应子容器，AppConfig 对应父容器，发现事务依然失效 原因：子容器扫描范围过大，把未加事务配置的 service 扫描进来 解法1：各扫描各的，不要图简便 解法2：不要用父子容器，所有 bean 放在同一容器 6. 调用本类方法导致传播行为失效 1234567891011121314@Servicepublic class Service6 &#123; @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;foo&quot;); bar(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;bar&quot;); &#125;&#125; 原因：本类方法调用不经过代理，因此无法增强 解法1：依赖注入自己（代理）来调用 解法2：通过 AopContext 拿到代理对象，来调用 解法3：通过 CTW，LTW 实现功能增强 解法1 123456789101112131415161718@Servicepublic class Service6 &#123; @Autowired private Service6 proxy; // 本质上是一种循环依赖 @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;foo&quot;); System.out.println(proxy.getClass()); proxy.bar(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;bar&quot;); &#125;&#125; 解法2，还需要在 AppConfig 上添加 @EnableAspectJAutoProxy(exposeProxy = true) 1234567891011121314@Servicepublic class Service6 &#123; @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void foo() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;foo&quot;); ((Service6) AopContext.currentProxy()).bar(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class) public void bar() throws FileNotFoundException &#123; LoggerUtils.get().debug(&quot;bar&quot;); &#125;&#125; 7. @Transactional 没有保证原子行为 12345678910111213141516171819202122@Servicepublic class Service7 &#123; private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public void transfer(int from, int to, int amount) &#123; int fromBalance = accountMapper.findBalanceBy(from); logger.debug(&quot;更新前查询余额为: &#123;&#125;&quot;, fromBalance); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); &#125; &#125; public int findBalance(int accountNo) &#123; return accountMapper.findBalanceBy(accountNo); &#125;&#125; 上面的代码实际上是有 bug 的，假设 from 余额为 1000，两个线程都来转账 1000，可能会出现扣减为负数的情况 原因：事务的原子性仅涵盖 insert、update、delete、select … for update 语句，select 方法并不阻塞 如上图所示，红色线程和蓝色线程的查询都发生在扣减之前，都以为自己有足够的余额做扣减 8. @Transactional 方法导致的 synchronized 失效 针对上面的问题，能否在方法上加 synchronized 锁来解决呢？ 12345678910111213141516171819202122@Servicepublic class Service7 &#123; private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class) public synchronized void transfer(int from, int to, int amount) &#123; int fromBalance = accountMapper.findBalanceBy(from); logger.debug(&quot;更新前查询余额为: &#123;&#125;&quot;, fromBalance); if (fromBalance - amount &gt;= 0) &#123; accountMapper.update(from, -1 * amount); accountMapper.update(to, amount); &#125; &#125; public int findBalance(int accountNo) &#123; return accountMapper.findBalanceBy(accountNo); &#125;&#125; 答案是不行，原因如下： synchronized 保证的仅是目标方法的原子性，环绕目标方法的还有 commit 等操作，它们并未处于 sync 块内 可以参考下图发现，蓝色线程的查询只要在红色线程提交之前执行，那么依然会查询到有 1000 足够余额来转账 解法1：synchronized 范围应扩大至代理方法调用 解法2：使用 select … for update 替换 select 5. Spring MVC 执行流程 要求 掌握 Spring MVC 的执行流程 了解 Spring MVC 的重要组件的作用 概要 我把整个流程分成三个阶段 准备阶段 匹配阶段 执行阶段 准备阶段 在 Web 容器第一次用到 DispatcherServlet 的时候，会创建其对象并执行 init 方法 init 方法内会创建 Spring Web 容器，并调用容器 refresh 方法 refresh 过程中会创建并初始化 SpringMVC 中的重要组件， 例如 MultipartResolver，HandlerMapping，HandlerAdapter，HandlerExceptionResolver、ViewResolver 等 容器初始化后，会将上一步初始化好的重要组件，赋值给 DispatcherServlet 的成员变量，留待后用 匹配阶段 用户发送的请求统一到达前端控制器 DispatcherServlet DispatcherServlet 遍历所有 HandlerMapping ，找到与路径匹配的处理器 ① HandlerMapping 有多个，每个 HandlerMapping 会返回不同的处理器对象，谁先匹配，返回谁的处理器。其中能识别 @RequestMapping 的优先级最高 ② 对应 @RequestMapping 的处理器是 HandlerMethod，它包含了控制器对象和控制器方法信息 ③ 其中路径与处理器的映射关系在 HandlerMapping 初始化时就会建立好 将 HandlerMethod 连同匹配到的拦截器，生成调用链对象 HandlerExecutionChain 返回 遍历HandlerAdapter 处理器适配器，找到能处理 HandlerMethod 的适配器对象，开始调用 调用阶段 执行拦截器 preHandle 由 HandlerAdapter 调用 HandlerMethod ① 调用前处理不同类型的参数 ② 调用后处理不同类型的返回值 第 2 步没有异常 ① 返回 ModelAndView ② 执行拦截器 postHandle 方法 ③ 解析视图，得到 View 对象，进行视图渲染 第 2 步有异常，进入 HandlerExceptionResolver 异常处理流程 最后都会执行拦截器的 afterCompletion 方法 如果控制器方法标注了 @ResponseBody 注解，则在第 2 步，就会生成 json 结果，并标记 ModelAndView 已处理，这样就不会执行第 3 步的视图渲染 6. Spring 注解 要求 掌握 Spring 常见注解 提示 注解的详细列表请参考：面试题-spring-注解.xmind 下面列出了视频中重点提及的注解，考虑到大部分注解同学们已经比较熟悉了，仅对个别的作简要说明 事务注解 @EnableTransactionManagement，会额外加载 4 个 bean BeanFactoryTransactionAttributeSourceAdvisor 事务切面类 TransactionAttributeSource 用来解析事务属性 TransactionInterceptor 事务拦截器 TransactionalEventListenerFactory 事务监听器工厂 @Transactional 核心 @Order 控制bean的执行顺序 切面 @EnableAspectJAutoProxy -------（切面相关） 会加载 AnnotationAwareAspectJAutoProxyCreator，它是一个 bean 后处理器，用来创建代理 如果没有配置 @EnableAspectJAutoProxy，又需要用到代理（如事务）则会使用 InfrastructureAdvisorAutoProxyCreator 这个 bean 后处理器 组件扫描与配置类 @Component @Controller @Service @Repository @ComponentScan @Conditional @Configuration 配置类其实相当于一个工厂, 标注 @Bean 注解的方法相当于工厂方法 @Bean 不支持方法重载, 如果有多个重载方法, 仅有一个能入选为工厂方法 @Configuration 默认会为标注的类生成代理, 其目的是保证 @Bean 方法相互调用时, 仍然能保证其单例特性 @Configuration 中如果含有 BeanFactory 后处理器, 则实例工厂方法会导致 MyConfig 提前创建, 造成其依赖注入失败，解决方法是改用静态工厂方法或直接为 @Bean 的方法参数依赖注入, 针对 Mapper 扫描可以改用注解方式 @Bean @Import 四种用法 ① 引入单个 bean ② 引入一个配置类 ③ 通过 Selector 引入多个类 ④ 通过 beanDefinition 注册器 解析规则 同一配置类中, @Import 先解析 @Bean 后解析 同名定义, 默认后面解析的会覆盖前面解析的 不允许覆盖的情况下, 如何能够让 MyConfig(主配置类) 的配置优先? (虽然覆盖方式能解决) 采用 DeferredImportSelector，因为它最后工作, 可以简单认为先解析 @Bean, 再 Import @Lazy 加在类上，表示此类延迟实例化、初始化 加在方法参数上，此参数会以代理方式注入 @PropertySource ---- 读取properties文件键值信息 依赖注入 @Autowired @Qualifier @Value mvc mapping @RequestMapping，可以派生多个注解如 @GetMapping 等 mvc rest @RequestBody 将请求体中的json数据转化为java对象 @ResponseBody，将java对象转为json数据组成响应体，组合 @Controller =&gt; @RestController @ResponseStatus，控制响应的状态码 mvc 统一处理 @ControllerAdvice，组合 @ResponseBody =&gt; @RestControllerAdvice @ExceptionHandler mvc 参数 @PathVariable mvc ajax @CrossOrigin，解决跨域问题 boot auto @SpringBootApplication @EnableAutoConfiguration @SpringBootConfiguration boot condition @ConditionalOnClass，classpath 下存在某个 class 时，条件才成立 @ConditionalOnMissingBean，beanFactory 内不存在某个 bean 时，条件才成立 @ConditionalOnProperty，配置文件中存在某个 property（键、值）时，条件才成立 boot properties @ConfigurationProperties，会将当前 bean 的属性与配置文件中的键值进行绑定 @EnableConfigurationProperties，会添加两个较为重要的 bean ConfigurationPropertiesBindingPostProcessor，bean 后处理器，在 bean 初始化前调用下面的 binder ConfigurationPropertiesBinder，真正执行绑定操作 7. SpringBoot 自动配置原理 要求 掌握 SpringBoot 自动配置原理 自动配置原理 @SpringBootApplication是一个组合注解，由 @ComponentScan、@EnableAutoConfiguration 和 @SpringBootConfiguration 组成 @SpringBootConfiguration 与普通 @Configuration 相比，唯一区别是前者要求整个 app 中只出现一次 @ComponentScan excludeFilters - 用来在组件扫描时进行排除，也会排除自动配置类 @EnableAutoConfiguration 也是一个组合注解，由下面注解组成 @AutoConfigurationPackage – 用来记住扫描的起始包 @Import(AutoConfigurationImportSelector.class) 用来加载 META-INF/spring.factories 中的自动配置类 为什么不使用 @Import 直接引入自动配置类 有两个原因： 让主配置类和自动配置类变成了强耦合，主配置类不应该知道有哪些从属配置 直接用 @Import(自动配置类.class)，引入的配置解析优先级较高，自动配置类的解析应该在主配置没提供时作为默认配置 因此，采用了 @Import(AutoConfigurationImportSelector.class) 由 AutoConfigurationImportSelector.class 去读取 META-INF/spring.factories 中的自动配置类，实现了弱耦合。 另外 AutoConfigurationImportSelector.class 实现了 DeferredImportSelector 接口，让自动配置的解析晚于主配置的解析 8. Spring 中的设计模式 要求 掌握 Spring 中常见的设计模式 1. Spring 中的 Singleton 请大家区分 singleton pattern 与 Spring 中的 singleton bean 根据单例模式的目的 Ensure a class only has one instance, and provide a global point of access to it 显然 Spring 中的 singleton bean 并非实现了单例模式，singleton bean 只能保证每个容器内，相同 id 的 bean 单实例 当然 Spring 中也用到了单例模式，例如 org.springframework.transaction.TransactionDefinition#withDefaults org.springframework.aop.TruePointcut#INSTANCE org.springframework.aop.interceptor.ExposeInvocationInterceptor#ADVISOR org.springframework.core.annotation.AnnotationAwareOrderComparator#INSTANCE org.springframework.core.OrderComparator#INSTANCE 2. Spring 中的 Builder（建造器模式） 定义 Separate the construction of a complex object from its representation so that the same construction process can create different representations 它的主要亮点有三处： 较为灵活的构建产品对象 在不执行最后 build 方法前，产品对象都不可用 构建过程采用链式调用，看起来比较爽 Spring 中体现 Builder 模式的地方： org.springframework.beans.factory.support.BeanDefinitionBuilder org.springframework.web.util.UriComponentsBuilder org.springframework.http.ResponseEntity.HeadersBuilder org.springframework.http.ResponseEntity.BodyBuilder 3. Spring 中的 Factory Method 定义 Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses 根据上面的定义，Spring 中的 ApplicationContext 与 BeanFactory 中的 getBean 都可以视为工厂方法，它隐藏了 bean （产品）的创建过程和具体实现 Spring 中其它工厂： org.springframework.beans.factory.FactoryBean @Bean 标注的静态方法及实例方法 ObjectFactory 及 ObjectProvider 前两种工厂主要封装第三方的 bean 的创建过程，后两种工厂可以推迟 bean 创建，解决循环依赖及单例注入多例等问题 4. Spring 中的 Adapter 定义 Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn’t otherwise because of incompatible interfaces 典型的实现有两处： org.springframework.web.servlet.HandlerAdapter – 因为控制器实现有各种各样，比如有 大家熟悉的 @RequestMapping 标注的控制器实现 传统的基于 Controller 接口（不是 @Controller注解啊）的实现 较新的基于 RouterFunction 接口的实现 它们的处理方法都不一样，为了统一调用，必须适配为 HandlerAdapter 接口 org.springframework.beans.factory.support.DisposableBeanAdapter – 因为销毁方法多种多样，因此都要适配为 DisposableBean 来统一调用销毁方法 5. Spring 中的 Composite（组合模式） 定义 Compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly 典型实现有： org.springframework.web.method.support.HandlerMethodArgumentResolverComposite org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite org.springframework.web.servlet.handler.HandlerExceptionResolverComposite org.springframework.web.servlet.view.ViewResolverComposite composite 对象的作用是，将分散的调用集中起来，统一调用入口，它的特征是，与具体干活的实现实现同一个接口，当调用 composite 对象的接口方法时，其实是委托具体干活的实现来完成 6. Spring 中的 Decorator 定义 Attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality 典型实现： org.springframework.web.util.ContentCachingRequestWrapper 7. Spring 中的 Proxy 定义 Provide a surrogate or placeholder for another object to control access to it 装饰器模式注重的是功能增强，避免子类继承方式进行功能扩展，而代理模式更注重控制目标的访问 典型实现： org.springframework.aop.framework.JdkDynamicAopProxy org.springframework.aop.framework.ObjenesisCglibAopProxy 8. Spring 中的 Chain of Responsibility 定义 Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it 典型实现： org.springframework.web.servlet.HandlerInterceptor 9. Spring 中的 Observer 定义 Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically 典型实现： org.springframework.context.ApplicationListener org.springframework.context.event.ApplicationEventMulticaster org.springframework.context.ApplicationEvent 10. Spring 中的 Strategy 定义 Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it 典型实现： org.springframework.beans.factory.support.InstantiationStrategy org.springframework.core.annotation.MergedAnnotations.SearchStrategy org.springframework.boot.autoconfigure.condition.SearchStrategy 11. Spring 中的 Template Method 定义 Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure 典型实现： 大部分以 Template 命名的类，如 JdbcTemplate，TransactionTemplate 很多以 Abstract 命名的类，如 AbstractApplicationContext","categories":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"},{"name":"juc","slug":"juc","permalink":"http://cloud-tour.github.io/tags/juc/"},{"name":"jvm","slug":"jvm","permalink":"http://cloud-tour.github.io/tags/jvm/"},{"name":"ssm","slug":"ssm","permalink":"http://cloud-tour.github.io/tags/ssm/"}]},{"title":"redis","slug":"redis","date":"2023-02-17T13:22:50.541Z","updated":"2023-02-17T13:23:58.698Z","comments":true,"path":"2023/02/17/redis/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/redis/","excerpt":"","text":"1.讲一下Redis的网络模型 io多路复用 ​ I/O多路复用的I/O是指网络I/O，多路指多个TCP连接(即socket或者channel），复用指复用一个或几个线程。意思说一个或一组线程处理多个TCP连接。最大优势是减少系统开销小，不必创建过多的进程/线程，也不必维护这些进程/线程。 I/O多路复用使用两个系统调用(select/poll/epoll和recvfrom)，blocking I/O只调用了recvfrom； select机制 ​ 客户端操作服务器时就会产生这三种文件描述符(简称fd)：writefds(写)、readfds(读)、和exceptfds(异常)。select会阻塞住监视3类文件描述符，等有数据、可读、可写、出异常 或超时、就会返回；返回后通过遍历fdset整个数组来找到就绪的描述符fd，然后进行对应的I/O操作 io流程： 1.接收监听的数据，执行select函数，将整个fd发给内核态 2.内核态遍历传过来的数据，若无就绪数据就休眠 3.有数据准备好，即唤醒，再次遍历，然后将fd集合写给用户态 4.用户态收到后会再次进行遍历，然后找到对应准备好的数据节点 poll机制 ​ 基本原理与select一致，也是轮询+遍历；唯一的区别就是poll没有最大文件描述符限制（使用链表的方式存储fd） io流程： 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限 内核遍历fd，判断是否就绪 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n 用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd epoll机制 ​ 没有fd个数限制，用户态拷贝到内核态只需要一次，使用时间通知机制来触发。通过epoll_ctl注册fd，一旦fd就绪就会通过callback回调机制来激活对应fd，进行相关的io操作。 重要函数： epoll_create()：系统启动时，在Linux内核里面申请一个红黑树结构文件系统，返回epoll对象，也是一个fd epoll_ctl()：每新建一个连接，都通过该函数操作epoll对象，在这个对象里面修改添加删除对应的链接fd, 绑定一个callback函数 epoll_wait()：轮训所有的callback集合，并完成对应的IO操作 执行流程： 1.系统启动，先调用epoll_create()函数，在内核空间创建epoll(包含一颗红黑树，一个list_head链表)实例，并返回一个句柄（唯一标识） 2.紧接着调用epoll_ctl()操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听函数，这个函数会在fd数据就绪时触发，就是准备好了，就把fd数据添加到list_head中去 3.调用epoll_wait()函数,就去等待就绪，该函数会在用户态创建一个空的events数组，当调用这个函数的时候，在相应的等待时间内会去检查list_head，如果在此过程中，检查到了list_head中有数据会将数据拷贝到events数组中，并且返回对应的操作的数量，用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据。 各种机制的优劣 select模式存在的三个问题： 能监听的FD最大不超过1024 每次select都需要把所有要监听的FD都拷贝到内核空间 每次都要遍历所有FD来判断就绪状态 poll模式的问题： poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有FD，如果监听较多，性能会下降 epoll模式中如何解决这些问题的？ 基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增删改查效率都非常高 每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝FD到内核空间 利用ep_poll_callback机制来监听FD状态，无需遍历所有FD，因此性能不会随监听的FD数量增多而下降 2.Redis是单线程吗 如果仅仅聊Redis的核心业务部分（命令处理），答案是单线程 如果是聊整个Redis，那么答案就是多线程 在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持： Redis v4.0：引入多线程异步处理一些耗时较久的任务，例如异步删除命令unlink Redis v6.0：在核心网络模型中引入 多线程，进一步提高对于多核CPU的利用率 因此，对于Redis的核心网络模型，在Redis 6.0之前确实都是单线程。是利用epoll（Linux系统）这样的IO多路复用技术在事件循环中不断处理客户端情况。 3.Redis是单线程模型为什么效率还这么高（为何要选单线程）？ 纯内存操作：数据存放在内存中，内存的响应时间大约是100纳秒，这是Redis每秒万亿级别访问的重要基础 非阻塞的I/O多路复用机制：Redis采用epoll做为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接，读写，关闭都转换为了时间，不在I/O上浪费过多的时间 C语言实现：距离操作系统更近，执行速度会更快 单线程避免切换开销：单线程避免了多线程上下文切换的时间开销，预防了多线程可能产生的竞争问题 4.讲一下Redis中的基本数据类型以及使用场景 String： 缓存、计数器、分布式锁等 List： 链表、队列、微博关注人时间轴列表等 Hash： 用户信息、Hash 表等 Set： 去重、赞、踩、共同好友等 Zset： 访问量排行榜、点击量排行榜等 5.讲一下Redis中的特殊数据结构以及使用场景 HyperLogLog（基数统计）：HyperLogLog 主要的应用场景就是进行基数统计。实际上不会存储每个元素的值，它使用的是概率算法，通过存储元素的hash值的第一个1的位置，来计算元素数量。HyperLogLog 可用极小空间完成独立数统计。 应用案例 如何统计 Google 主页面每天被多少个不同的账户访问过？ 对于 Google 这种访问量巨大的网页而言，其实统计出有十亿的访问量或十亿零十万的访问量其实是没有太多的区别的，因此，在这种业务场景下，为了节省成本，其实可以只计算出一个大概的值，而没有必要计算出精准的值。 HyperLogLog：存在一定误差，占用内存少，稳定占用 12k 左右内存，可以统计 2^64 个元素，对于上面举例的应用场景，建议使用 Geo（地理空间信息）：Geo主要用于存储地理位置信息，并对存储的信息进行操作（添加、获取、计算两位置之间距离、获取指定范围内位置集合、获取某地点指定范围内集合）。 Pub/Sub（发布订阅）：发布订阅类似于广播功能。redis发布订阅包括 发布者、订阅者、Channel Bitmap（位图）：Bitmap就是位图，其实也就是字节数组（byte array），用一串连续的2进制数字（0或1）表示，每一位所在的位置为偏移(offset)，位图就是用每一个二进制位来存放或者标记某个元素对应的值。通常是用来判断某个数据存不存在的，因为是用bit为单位来存储所以Bitmap本身会极大的节省储存空间 应用案例 有1亿用户，5千万登陆用户，那么统计每日用户的登录数。每一位标识一个用户ID，当某个用户访问我们的网站就在Bitmap中把标识此用户的位设置为1。 用户在线状态 用户签到状态 统计独立用户 6.布隆过滤器的工作原理 ​ 当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点（使用多个哈希函数对元素key (bloom中不存value) 进行哈希，算出一个整数索引值，然后对位数组长度进行取模运算得到一个位置，每个无偏哈希函数都会得到一个不同的位置），把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了： 如果这些点有任何一个为0，则被检元素一定不在 如果都是1，并不能完全说明这个元素就一定存在其中，有可能这些位置为1是因为其他元素的存在，这就是布隆过滤器会出现误判的原因 应用场景 解决缓存穿透：事先把存在的key都放到redis的Bloom Filter 中，他的用途就是存在性检测，如果 BloomFilter 中不存在，那么数据一定不存在；如果 BloomFilter 中存在，实际数据也有可能会不存 黑名单校验：假设黑名单的数量是数以亿计的，存放起来就是非常耗费存储空间的，布隆过滤器则是一个较好的解决方案。把所有黑名单都放在布隆过滤器中，再收到邮件时，判断邮件地址是否在布隆过滤器中即可 Web拦截器：用户第一次请求，将请求参数放入布隆过滤器中，当第二次请求时，先判断请求参数是否被布隆过滤器命中，从而提高缓存命中率 7.Redis的持久化方式及优缺点 RDB RDB（Redis Database Backup File，Redis数据备份文件）持久化方式：是指用数据集快照的方式半持久化模式记录 Redis 数据库的所有键值对，在某个时间点将数据写入一个临时文件，持久化结束后，用这个临时文件替换上次持久化的文件，达到数据恢复。 原理 bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作。 优缺点 优点 RDB快照是一个压缩过的非常紧凑的文件。保存着某个时间点的数据集，适合做数据的备份，灾难恢复 可最大化Redis的的性能。在保存RDB文件，服务器进程只需要fork一个子进程来完成RDB文件创建，父进程不需要做IO操作 与AOF相比，恢复大数据集的时候会更快 缺点 RDB的数据安全性是不如AOF的，保存整个数据集的过程是比繁重的，根据配置可能要几分钟才快照一次，如果服务器宕机，那么就可能丢失几分钟的数据 Redis数据集较大时，fork的子进程要完成快照会比较耗CPU、耗时 AOF AOF（Append Only File，追加日志文件）持久化方式：是指所有的命令行记录以 Redis 命令请求协议的格式完全持久化存储保存为 aof 文件。Redis 是先执行命令，把数据写入内存，然后才记录日志。因为该模式是只追加的方式，所以没有任何磁盘寻址的开销，所以很快，有点像 Mysql 中的binlog，AOF更适合做热备。 AOF的命令记录的频率也可以通过redis.conf文件来配： 123456# 表示每执行一次写命令，立即记录到AOF文件appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘appendfsync no 三种策略对比： AOF文件重写 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。 如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。 所以重写命令后，AOF文件内容就是：mset name jack num 666 优缺点 优点 数据更完整，安全性更高，秒级数据丢失（取决fsync策略，如果是everysec，最多丢失1秒的数据） AOF文件是一个只进行追加的日志文件，且写入操作是以Redis协议的格式保存的，内容是可读的，适合误删紧急恢复 缺点 对于相同的数据集，AOF文件的体积要大于RDB文件，数据恢复也会比较慢 根据所使用的fsync策略，AOF的速度可能会慢于RDB。 不过在一般情况下，每秒fsync的性能依然非常高 RDB和AOF对比 RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 8.Redis的淘汰策略 Redis淘汰机制的存在是为了更好的使用内存，用一定的缓存丢失来换取内存的使用效率。当Redis内存快耗尽时，Redis会启动内存淘汰机制，将部分key清掉以腾出内存。当达到内存使用上限超过maxmemory时，可在配置文件redis.conf中指定 maxmemory-policy 的清理缓存方式。 1234# 配置最大内存限制maxmemory 1000mb# 配置淘汰策略maxmemory-policy volatile-lru LRU(最近最少使用) volatile-lru：从已设置过期时间的key中，挑选**最近最少使用(最长时间没有使用)**的key进行淘汰 allkeys-lru：从所有key中，挑选最近最少使用的数据淘汰 LFU(最近最不经常使用) volatile-lfu：从已设置过期时间的key中，挑选**最近最不经常使用(使用次数最少)**的key进行淘汰 allkeys-lfu：从所有key中，选择某段时间内内最近最不经常使用的数据淘汰 Random(随机淘汰) volatile-random：从已设置过期时间的key中，任意选择数据淘汰 allkeys-random：从所有key中，任意选择数据淘汰 TTL(过期时间) volatile-ttl：从已设置过期时间的key中，挑选将要过期的数据淘汰 allkeys-random：从所有key中，任意选择数据淘汰 No-Enviction(驱逐) noenviction（驱逐）：当达到最大内存时直接返回错误，不覆盖或逐出任何数据 9.Redis主从同步原理 全量同步 主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程： 这里有一个问题，master如何得知salve是第一次来连接呢？？ 有几个概念，可以作为判断依据： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。 master判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。 master会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。 如图： 完整流程描述： slave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave slave执行接收到的命令，保持与master之间的同步 增量同步 全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。 什么是增量同步？就是只更新slave与master存在差异的部分数据。如图： 那么master怎么知道slave与自己的数据差异在哪里呢? repl_backlog原理 master怎么知道slave与自己的数据差异在哪里呢? 这就要说到全量同步时的repl_baklog文件了。 这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。 repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset： slave与master的offset之间的差异，就是salve需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 直到数组被填满： 此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。 但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset： 如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖： 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。 主从同步优化 主从同步可以保证主从数据的一致性，非常重要。 可以从以下几个方面来优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图： 小结 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 10.解释一下Redis中的缓存击穿、雪崩、穿透问题，并指出解决方案 缓存穿透 缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。 常见的解决方案有两种： 缓存空对象 优点：实现简单，维护方便 缺点： 额外的内存消耗 可能造成短期的不一致 布隆过滤 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 **缓存空对象思路分析：**哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到缓存了 **布隆过滤：**布隆过滤器其实采用的是哈希思想(Bigmap)来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，不过此时可能会有误判。 假设布隆过滤器判断这个数据不存在，那么这个数据一定不存在则直接返回 这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要哈希思想，就可能存在哈希冲突 缓存雪崩 缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。 解决方案： 给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 缓存击穿（热点key问题） 缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且缓存重建业务较复杂的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大的冲击。 常见的解决方案有两种： 互斥锁 逻辑过期 逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大 解决方案一、使用锁来解决： 因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。 假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。 解决方案二、逻辑过期方案 方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。 我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。 这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。 进行对比 **互斥锁方案：**由于保证了互斥性，所以数据一致，且实现简单，因为仅仅只需要加一把锁而已，也没其他的事情需要操心，所以没有额外的内存消耗，缺点在于有锁就有死锁问题的发生，且只能串行执行性能肯定受到影响 逻辑过期方案： 线程读取过程中不需要等待，性能好，有一个额外的线程持有锁去进行重构数据，但是在重构数据完成前，其他的线程只能返回之前的数据，且实现起来麻烦 11.讲一下Redis缓存同步问题 大多数情况下，浏览器查询到的都是缓存数据，如果缓存数据与数据库数据存在较大差异，可能会产生比较严重的后果。 所以我们必须保证数据库数据、缓存数据的一致性，这就是缓存与数据库的同步。 缓存数据同步的常见方式有三种： 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新 优势：简单、方便 缺点：时效性差，缓存过期之前可能不一致 场景：更新频率较低，时效性要求低的业务 同步双写：在修改数据库的同时，直接修改缓存 优势：时效性强，缓存与数据库强一致 缺点：有代码侵入，耦合度高； 场景：对一致性、时效性要求较高的缓存数据 **异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据 优势：低耦合，可以同时通知多个缓存服务 缺点：时效性一般，可能存在中间不一致状态 场景：时效性要求一般，有多个服务需要同步 而异步实现又可以基于MQ或者Canal来实现： 1）基于MQ的异步通知： 解读： 商品服务完成对数据的修改后，只需要发送一条消息到MQ中。 缓存服务监听MQ消息，然后完成对缓存的更新 依然有少量的代码侵入。 2）基于Canal的通知 解读： 商品服务完成商品修改后，业务直接结束，没有任何代码侵入 Canal监听MySQL变化，当发现变化后，立即通知缓存服务 缓存服务接收到canal通知，更新缓存 代码零侵入 12.如何用redis实现分布式锁 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。 分布式锁的核心思想就是让大家都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住线程，不让线程进行，让程序串行执行，这就是分布式锁的核心思路 Redis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁 实现思路 实现分布式锁时需要实现的两个基本方法： 获取锁： 互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁： 手动释放 超时释放：获取锁时添加一个超时时间 核心思路： 我们利用redis 的setNx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试即可 关于锁误删情况的改进 逻辑说明： 持有锁的线程在锁的内部出现了阻塞，导致他的锁自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明 解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。 核心逻辑：在存入锁时，放入自己线程的标识，在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。 原子性问题 用java代码无法保证两条语句的原子性 例如：线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时他的锁到期了，那么此时线程2进来，但是线程1他会接着往后执行，当他卡顿结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生 解决方案：使用Lua脚本，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性 redission 基于setnx实现的分布式锁存在下面的问题： 重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，比如HashTable这样的代码中，他的方法都是使用synchronized修饰的，假如他在一个方法内，调用另一个方法，那么此时如果是不可重入的，不就死锁了吗？所以可重入锁他的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的。 不可重试：是指目前的分布式只能尝试一次，我们认为合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。 **超时释放：**我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟没有锁住，有安全隐患 主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。 那么什么是Redission呢 Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。 Redission提供了分布式锁的多种多样的功能 Redission执行流程如下：（只要线程一加锁成功，就会启动一个watch dog看门狗，它是一个后台线程，会每隔10秒检查一下（锁续命周期就是设置的超时时间的三分之一），如果线程还持有锁，就会不断的延长锁key的生存时间。因此，Redis就是使用Redisson解决了锁过期释放，业务没执行完问题。当业务执行完，释放锁后，再关闭守护线程， 可重入原理：在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state=0，假如有人持有这把锁，那么state=1，如果持有这把锁的人再次持有这把锁，那么state就会+1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成0 时，表示当前这把锁没有被人持有。 redission锁重试与看门狗机制 抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同 1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null 2、判断当前这把锁是否是属于当前线程，如果是，则返回null 所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，同学们可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁 123456long threadId = Thread.currentThread().getId();Long ttl = tryAcquire(-1, leaseTime, unit, threadId);// lock acquiredif (ttl == null) &#123; return;&#125; 接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带带参数传入的值是-1，如果传入参数，则leaseTime是他本身，所以如果传入了参数，此时leaseTime != -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑 123if (leaseTime != -1) &#123; return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG);&#125; 如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout() ttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程 1234567891011121314RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);ttlRemainingFuture.onComplete((ttlRemaining, e) -&gt; &#123; if (e != null) &#123; return; &#125; // lock acquired if (ttlRemaining == null) &#123; scheduleExpirationRenewal(threadId); &#125;&#125;);return ttlRemainingFuture; 此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法 Method( new TimerTask() {},参数2 ，参数3 ) 指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情 因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约 那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。 1234567891011121314151617181920212223242526272829303132333435private void renewExpiration() &#123; ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ee == null) &#123; return; &#125; Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ent == null) &#123; return; &#125; Long threadId = ent.getFirstThreadId(); if (threadId == null) &#123; return; &#125; RFuture&lt;Boolean&gt; future = renewExpirationAsync(threadId); future.onComplete((res, e) -&gt; &#123; if (e != null) &#123; log.error(&quot;Can&#x27;t update lock &quot; + getName() + &quot; expiration&quot;, e); return; &#125; if (res) &#123; // reschedule itself renewExpiration(); &#125; &#125;); &#125; &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task);&#125; redission锁的MutiLock原理 为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例 此时我们去写命令，写在主机上， 主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。 为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。 那么MutiLock 加锁原理是什么呢？笔者画了一幅图来说明 当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环去不停去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功， 那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试. 13.redis的集群方式（待完成）","categories":[{"name":"redis","slug":"redis","permalink":"http://cloud-tour.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://cloud-tour.github.io/tags/redis/"}]},{"title":"mysql","slug":"mysql","date":"2023-02-17T13:21:00.159Z","updated":"2023-02-17T13:22:31.373Z","comments":true,"path":"2023/02/17/mysql/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/mysql/","excerpt":"","text":"1.sql的执行流程 执行流程： 1、在打开客户端后，最初需要和sql服务器建立连接，账号认证和校验权限。 2、认证后，客户端发生查询sql脚本给服务器 3、服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 4、服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。 5、MySQL根据优化器生成的执行计划，再调用存储引擎的API来执行查询。 6、将结果返回给客户端 各大器件说明： **连接器：**Client与Server建立连接、进行鉴权、保持连接、管理连接。 **解析器：**1、先通过词法分析：从左到右一个字符、一个字符地输入，然后根据构词规则识别单词。 ​ 2、接下来，进行语法解析，生成解析树，判断输入的这个 SQL 语句是否满足 MySQL 语法. ​ 3、语义解析：若SQL 语句符合语法上的定义的话，则服务器进程接下去会对语句中涉及的表、索引、视图等对象进行解析，并 对照数据字典检查这些对象的名称以及相关结构，看看这些字段、表、视图等是否在数据库中。 **预处理器：**绝大多数情况下，某需求某一条 SQL 语句可能会被反复调用执行，或者每次执行的时候只有个别的值不同（比如 select 的 where 子句值不同，update 的 set 子句值不同，insert 的 values 值不同）。如果每次都需要经过词法语义解析、语句优 化、制定执行计划等，则效率就明显不行了。 ​ 所谓预编译语句就是将此类 SQL 语句中的值用占位符替代，可以视为将 SQL 语句模板化或者说参数化，一般称这类语句叫 Prepared Statements。 ​ 预编译语句的优势在于归纳为：一次编译、多次运行，省去了解析优化等过程；此外预编译语句能防止 SQL 注入。 **查询优化器：**优化器的目的是按照一定原则来得到她认为的目标SQL在当前情形下最有效的执行路径,优化器的目的是为了得到目标SQL的 执行计划。传统关系型数据库里面的优化器分为CBO和RBO两种。 ​ CBO—Cost_Based Potimizer 基于成本的优化器: CBO在会从目标诸多的执行路径中选择一个成本最小的执行路径来作为 执行计划。这里的成本他实际代表了MySQL根据相关统计信息计算出来目标SQL对应的步骤的IO，CPU等消耗。也就是意 味着数据库里的成本实际上就是对于执行目标SQL所需要IO,CPU等资源的一个估计值。而成本值是根据索引，表，行的统 计信息计算出来的。(计算过程比较复杂) 2.innodb中的页分裂现象 innodb的逻辑存储结构： .ibd的文件。这个文件由多个段（segments）组成，每个段和一个索引相关。 文件的结构是不会随着数据行的删除而变化的，但段则会跟着构成它的更小一级单位——区的变化而变化。区仅存在于段内，并且每个区都是固定的1MB大小（页体积默认的情况下）。页则是区的下一级构成单位，默认体积为16KB。 按这样算，一个区可以容纳最多64个页，一个页可以容纳2-N个行。行的数量取决于它的大小，由你的表结构定义。InnoDB要求页至少要有两个行，因此可以算出行的大小最多为8000 bytes。 页的内部原理 页可以空或者填充满（100%），行记录会按照主键顺序来排列。例如在使用AUTO_INCREMENT时，你会有顺序的ID 1、2、3、4等。 页还有另一个重要的属性：MERGE_THRESHOLD。该参数的默认值是50%页的大小，它在InnoDB的合并操作中扮演了很重要的角色。 当你插入数据时，如果数据（大小）能够放的进页中的话，那他们是按顺序将页填满的。 若当前页满，则下一行记录会被插入下一页（NEXT）中。 根据B树的特性，它可以自顶向下遍历，但也可以在各叶子节点水平遍历。因为每个叶子节点都有着一个指向包含下一条（顺序）记录的页的指针。 例如，页#5有指向页#6的指针，页#6有指向前一页（#5）的指针和后一页（#7）的指针。 页合并 当你删了一行记录时，实际上记录并没有被物理删除，记录被标记（flaged）为删除并且它的空间变得允许被其他记录声明使用。 当页中删除的记录达到MERGE_THRESHOLD（默认页体积的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。 在示例中，页#6使用了不到一半的空间，页#5又有足够的删除数量，现在同样处于50%使用以下。从InnoDB的角度来看，它们能够进行合并。 合并操作使得页#5保留它之前的数据，并且容纳来自页#6的数据。页#6变成一个空页，可以接纳新数据。 如果我们在UPDATE操作中让页中数据体积达到类似的阈值点，InnoDB也会进行一样的操作。 页分裂 前面提到，页可能填充至100%，在页填满了之后，下一页会继续接管新的记录。但如果有下面这种情况呢？ 页#10没有足够空间去容纳新（或更新）的记录。根据“下一页”的逻辑，记录应该由页#11负责。然而： 页#11也同样满了，数据也不可能不按顺序地插入。怎么办？ 还记得之前说的链表吗（译注：指B+树的每一层都是双向链表）？页#10有指向页#9和页#11的指针。 InnoDB的做法是（简化版）： 创建新页 判断当前页（页#10）可以从哪里进行分裂（记录行层面） 移动记录行 重新定义页之间的关系 新的页#12被创建： 页#11保持原样，只有页之间的关系发生了改变： 页#10相邻的前一页为页#9，后一页为页#12 页#12相邻的前一页为页#10，后一页为页#11 页#11相邻的前一页为页#10，后一页为页#13 （译注：页#13可能本来就有，这里意思为页#10与页#11之间插入了页#12） 这样B树水平方向的一致性仍然满足，因为满足原定的顺序排列逻辑。然而从物理存储上讲页是乱序的，而且大概率会落到不同的区。 规律总结：页分裂会发生在插入或更新，并且造成页的错位（dislocation，落入不同的区） InnoDB用INFORMATION_SCHEMA.INNODB_METRICS表来跟踪页的分裂数。可以查看其中的index_page_splits和index_page_reorg_attempts/successful统计。 一旦创建分裂的页，唯一（译注：实则仍有其他方法，见下文）将原先顺序恢复的办法就是新分裂出来的页因为低于合并阈值（merge threshold）被删掉。这时候InnoDB用页合并将数据合并回来。 另一种方式就是用OPTIMIZE重新整理表。这可能是个很重量级和耗时的过程，但可能是唯一将大量分布在不同区的页理顺的方法。 另一方面，要记住在合并和分裂的过程，InnoDB会在索引树上加写锁（x-latch）。在操作频繁的系统中这可能会是个隐患。它可能会导致索引的锁争用（index latch contention）。如果表中没有合并和分裂（也就是写操作）的操作，称为“乐观”更新，只需要使用读锁（S）。带有合并也分裂操作则称为“悲观”更新，使用写锁（X）。 3.join原理 MySQL是只支持一种JOIN算法Nested-Loop Join（嵌套循环链接）而对于这个算法有很多变种，能够帮助MySQL更高效的执行JOIN操作： （1）Simple Nested-Loop Join 这个算法相对来说就是很简单了，从驱动表中取出R1匹配S表所有列，然后R2，R3,直到将R表中的所有数据匹配完（笛卡尔集），然后合并数据，可以看到这种算法要对S表进行RN次访问，虽然简单，但是相对来说开销还是太大了，对于两表联接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍 假设R为驱动表，S被驱动表，用伪代码表示一下这个过程就是这样： 1234for r in R # 扫描R表（驱动表） for s in S # 扫描S表（被驱动表） if (r and s satisfy the join condition) # 如果r和s满足join条件 output result # 返回结果集 所以如果R有1万条数据，S有1万条数据，那么数据比较的次数1万 * 1万 =1亿次，这种查询效率会非常慢。 （2）Index Nested-Loop Join 由于内表上有索引，所以比较的时候不再需要一条条记录进行比较，而可以通过索引来减少比较，从而加速查询。整个过程如下图所示： 这种算法在链接查询的时候，驱动表会根据关联字段的索引进行查找，当在索引上找到了符合的值，再回表进行查询，也就是只有当匹配到索引以后才会进行回表。至于驱动表的选择，MySQL优化器一般情况下是会选择记录数少的作为驱动表，但是当SQL特别复杂的时候不排除会出现错误选择。 大部分人诟病MySQL的INLJ慢，主要是因为在进行Join的时候可能用到的索引并不是主键的聚集索引，而是辅助索引，这时INLJ的过程又需要多一步Fetch（即回表执行）的过程，由于访问的是辅助索引，如果查询需要访问聚集索引上的列，那么必要需要进行回表取数据，看似每条记录只是多了一次回表操作，但这才是INLJ算法最大的弊端。 （3）Block Nested-Loop Join 在有索引的情况下，MySQL会尝试去使用Index Nested-Loop Join算法，在有些情况下，可能Join的列就是没有索引，那么这时MySQL的选择绝对不会是最先介绍的Simple Nested-Loop Join算法，而是会优先使用Block Nested-Loop Join的算法。 Block Nested-Loop Join对比Simple Nested-Loop Join多了一个中间处理的过程，也就是join buffer，使用join buffer将驱动表的查询JOIN相关列都给缓冲到了JOIN BUFFER当中，然后批量与非驱动表进行比较，这也来实现的话，可以将多次比较合并到一次，降低了非驱动表的访问频率。也就是只需要访问一次S表。这样来说的话，就不会出现多次访问非驱动表的情况了，也只有这种情况下才会访问join buffer。 在MySQL当中，我们可以通过参数join_buffer_size来设置join buffer的值，然后再进行操作。默认情况下join_buffer_size=256K，在查找的时候MySQL会将所有的需要的列缓存到join buffer当中，包括select的列，而不是仅仅只缓存关联列。在一个有N个JOIN关联的SQL当中会在执行时候分配N-1个join buffer。 4.InnoDB主键索引的B+tree高度 InnoDB主键索引的B+tree高度为多高呢? 假设: 一行数据大小为1k，一页中可以存储16行这样的数据。InnoDB的指针占用6个字节的空 间，主键即使为bigint，占用字节数为8。 高度为2： n * 8 + (n + 1) * 6 = 16*1024 算出n约为 1170 1171* 16 = 18736 也就是说，如果树的高度为2，则可以存储 18000 多条记录。 高度为3： 1171 * 1171 * 16 = 21939856 也就是说，如果树的高度为3，则可以存储 2200w 左右的记录。 5.mysql主从复制(同步)原理 可以看到mysql主从复制需要三个线程：master（binlog dump thread）、slave（I/O thread 、SQL thread） binlog dump线程： 主库中有数据更新时，根据设置的binlog格式，将更新的事件类型写入到主库的binlog文件中，并创建log dump线程通知slave有数据更新。当I/O线程请求日志内容时，将此时的binlog名称和当前更新的位置同时传给slave的I/O线程 I/O线程： 该线程会连接到master，向log dump线程请求一份指定binlog文件位置的副本，并将请求回来的binlog存到本地的relay log中 SQL线程： 该线程检测到relay log有更新后，会读取并在本地做redo操作，将发生在主库的事件在本地重新执行一遍，来保证主从数据同步 基本过程总结 主库写入数据并且生成binlog文件。该过程中MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的 在事件写入二进制日志完成后，master通知存储引擎提交事务 从库服务器上的IO线程连接Master服务器，请求从执行binlog日志文件中的指定位置开始读取binlog至从库 主库接收到从库IO线程请求后，其上复制的IO线程会根据Slave的请求信息分批读取binlog文件然后返回给从库的IO线程 Slave服务器的IO线程获取到Master服务器上IO线程发送的日志内容、日志文件及位置点后，会将binlog日志内容依次写到Slave端自身的Relay Log（即中继日志）文件的最末端，并将新的binlog文件名和位置记录到master-info文件中，以便下一次读取master端新binlog日志时能告诉Master服务器从新binlog日志的指定文件及位置开始读取新的binlog日志内容 从库服务器的SQL线程会实时监测到本地Relay Log中新增了日志内容，然后把RelayLog中的日志翻译成SQL并且按照顺序执行SQL来更新从库的数据 从库在relay-log.info中记录当前应用中继日志的文件名和位置点以便下一次数据复制 biglog binlog是什么？有什么作用？ 用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。可以简单理解为记录的就是sql语句。binlog 是 mysql 的逻辑日志，并且由 Server层进行记录，使用任何存储引擎的 mysql 数据库都会记录 binlog 日志。在实际应用中， binlog 的主要使用场景有两个： 用于主从复制，在主从结构中，binlog 作为操作记录从 master 被发送到 slave，slave服务器从 master 接收到的日志保存到 relay log 中 用于数据备份，在数据库备份文件生成后，binlog保存了数据库备份后的详细信息，以便下一次备份能从备份点开始 日志格式 binlog日志有三种格式，分别为STATMENT 、 ROW 和 MIXED。在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 STATMENT ：基于 SQL 语句的复制，每一条会修改数据的sql语句会记录到 binlog 中 ROW ：基于行的复制 MIXED ：基于 STATMENT 和 ROW 两种模式的混合复制，比如一般的数据操作使用 row 格式保存，有些表结构的变更语句，使用 statement 来记录 6.数据库三大范式 第一范式(1NF) 所有字段的值都是不可分解的原子值。即实体中的某个属性有多个值时，必须拆分为不同的属性。例如： 用户信息表 编号 姓名 年龄 地址 1 小王 23 浙江省杭州市拱墅区湖州街51号 当实际需求对地址没有特定的要求下，这个用户信息表的每一列都是不可分割的。但是当实际需求对省份或者城市有特别要求时，这个用户信息表中的地址就是可以分割的，改为： 用户信息表 编号 姓名 年龄 省份 城市 区县 详细地址 1 小王 23 浙江省 杭州市 拱墅区 湖州街51号 好处 表结构相对清晰 易于查询 第二范式(2NF) 所有非主键列必须全部依赖于全部主键。符合与不符合的场景如下： 案例如下： 学生课程表 学生编号 课程编号 学生名称 课程名称 所在班级 班主任 S1 C1 小王 计算机导论 计算机3班 陈老师 S1 C2 小王 数据结构 计算机3班 陈老师 S2 C1 小马 计算机导论 软件1班 李老师 将学生编号和课程编号作为主键，能确定唯一一条数据，但是学生名称只跟学生编号有关，跟课程编号无关，即不满足完全函数依赖，改为： 学生表 学生编号 学生名称 所在班级 班主任 S1 小王 计算机3班 陈老师 S2 小马 软件1班 李老师 课程表 课程编号 课程名称 C1 计算机导论 C2 数据结构 学生课程关系表 学生编号 课程编号 S1 C1 S1 C2 S2 C1 好处 相对节约空间，当学生表和课程表属性越多，效果越明显 解决插入异常，当新增一门课程时，原表因为没有学生选课，导致无法插入数据 解决更新繁琐，当更改一门课程名称时，原表要更改多条数据 解决删除异常，当学生学完一门课，原表若要清空学生上课信息，课程编号与课程名称的关系可能会丢失 第三范式(3NF) 在满足1NF和2NF的前提下，非主键列之间不存在间接依赖关系（消除传递函数依赖）。 案例如下： 学生表 学生编号 学生名称 班级编号 班级名字 S1 小王 001 计算机1班 S2 小马 003 计算机3班 学生编号作为主键满足第二范式（2NF）。通过学生编号 ⇒⇒ 班级编号 ⇒⇒ 班级名字，所以班级编号和班级名字之间存在依赖关系，改为： 学生表 学生编号 学生名称 班级编号 S1 小王 001 S2 小马 003 班级表 班级编号 班级名称 001 计算机1班 003 计算机3班 好处 相对节约空间 解决更新繁琐 解决插入异常，当班级分配了老师，还没分配学生的时候，原表将不可插入数据 解决删除异常，当学生毕业后，若要清空学生信息，班级和老师的关系可能会丢失 7.添加索引性能无法提升的可能原因 一条 SQL 语句执行的很慢，那是每次执行都很慢呢？还是大多数情况下是正常的，偶尔出现很慢呢？所以我觉得，我们还得分以下两种情况来讨论。 大多数情况是正常的，只是偶尔会出现很慢的情况。 在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。 针对这两种情况，我们来分析下可能是哪些原因导致的。 一、针对偶尔很慢的情况 一条 SQL 大多数情况正常，偶尔才能出现很慢的情况，针对这种情况，我觉得这条SQL语句的书写本身是没什么问题的，而是其他原因导致的，那会是什么原因呢？ 1、数据库在刷新脏页 当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。 2、拿不到锁 这个就比较容易想到了，我们要执行的这条语句，刚好这条语句涉及到的表，别人在用，并且加锁了，我们拿不到锁，只能慢慢等待别人释放锁了。或者，表没有加锁，但要使用到的某个一行被加锁了，这个时候，我也没办法啊。 二、针对一直都这么慢的情况 1、没用到索引，索引失效 2、没有使用到覆盖索引，导致回表查询 3、数据库自己选错索引 8.sql字段冗余的解决方法 冗余字段的使用在多表联合查询都是大数据量的表的情况下，确实是个不错的选择，有效的减少了IO操作。但结合已有的项目产品来看，冗余字段确实是双刃剑。尤其是大项目的开发，如果忽略某个表的冗余字段的更新，那么后果是灾难性的。如何有效的管理冗余字段是开发组内必须解决的问题。 解决方案1：使用专门的表来管理冗余字段。 例如article表有以下冗余字段 fromUserName、toUserName 如何管理这两个字段呢？通过建立一个表，表结构如下 id、objTable、objName、sourceTable、sourceId、level、isUpdate 其中objTable=目标表，objName= 目标字段，sourceTable=源表,sourceId=源表ID,level=是否需要立即更新,isUpdate=是否已更新 其中，level字段很有必要，有些冗余字段并不需要在源表修改后立即更新，那么可以通过一个定期更新策略来更新。 通过库表的管理，配合一个合理的存储过程，冗余字段的使用将不再是难题。 举例，如果上面两个字段发生变化，则使用触发器或者调用这个存储过程来检查是否有需要立即更新的冗余字段，需要则立即更新，不需要则isUpdate置0,等到周期性的策略来更新同时isUpdate=1。 解决方案2：平滑剔除字段。 流程： 1.梳理确认的要剔除冗余字段 2.排查出此字段所有关联的业务场景 3.开发停止对该字段写入和读取（统一操作保留字段） 4.强烈建议按照模块逐步进行，确保系统稳定（比如一次只改一个模块的相关表） 5.定期查该字段时候有新数据进入，如果有及时查找，一切正常后合适的时间节点删除该字段 6.平滑剔除三个月后对字段进行删除(保留三个月是避免因为排查遗漏造成线上错误) 9.锁问题 死锁 MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 当两个及以上的事务，双方都在等待对方释放已经持有的锁或因为加锁顺序不一致造成循环等待锁资源，就会出现“死锁”。常见的报错信息为 ” Deadlock found when trying to get lock...”。 避免死锁 三种常见的避免死锁方式： 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率 预防死锁 innodb_lock_wait_timeout 等待锁超时回滚事务 直观方法是在两个事务相互等待时，当一个等待时间超过设置的某一阀值时，对其中一个事务进行回滚，另一个事务就能继续执行。 wait-for graph算法来主动进行死锁检测 每当加锁请求无法立即满足需要并进入等待时，wait-for graph算法都会被触发。wait-for graph要求数据库保存以下两种信息： 锁的信息链表 事务等待链表 解决死锁 等待事务超时，主动回滚 进行死锁检查，主动回滚某条事务，让别的事务能继续走下去 下面提供一种方法，解决死锁的状态: 12-- 查看正在被锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX; 12--上图trx_mysql_thread_id列的值kill trx_mysql_thread_id; 10.数据切分 水平切分 水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。 垂直切分 垂直切分是将一张表按列分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不通的库中，例如将原来电商数据部署库垂直切分称商品数据库、用户数据库等。 Sharding策略 哈希取模：hash(key)%N 范围：可以是 ID 范围也可以是时间范围 映射表：使用单独的一个数据库来存储映射关系 Sharding存在的问题 事务问题：使用分布式事务来解决，比如 XA 接口 连接：可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。 唯一性 使用全局唯一 ID （GUID） 为每个分片指定一个 ID 范围 分布式 ID 生成器（如 Twitter 的 Snowflake 算法） 使用redis生成全局增长主键","categories":[{"name":"mysql","slug":"mysql","permalink":"http://cloud-tour.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://cloud-tour.github.io/tags/mysql/"}]},{"title":"ssb","slug":"ssb","date":"2023-02-17T13:19:19.502Z","updated":"2023-02-17T13:20:23.268Z","comments":true,"path":"2023/02/17/ssb/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/ssb/","excerpt":"","text":"1.Spring、SpringMVC、SpringBoot的相关概念 Spring Spring是一个开源容器框架，可以接管web层，业务层，dao层，持久层的组件，并且可以配置各种bean，和维护bean与bean之间的关系。其核心就是控制反转(IOC)，和面向切面(AOP)，简单的说就是一个分层的轻量级开源框架。 SpringMVC SpringMVC是一种web层mvc框架，用于替代servlet（处理|响应请求，获取表单参数，表单校验等。SpringMVC是一个MVC的开源框架，SpringMVC = struts2 + spring，springMVC就相当于是Struts2加上Spring的整合。 SpringBoot Springboot是一个微服务框架，延续了spring框架的核心思想IOC和AOP，简化了应用的开发和部署。Spring Boot是为了简化Spring应用的创建、运行、调试、部署等而出现的，使用它可以做到专注于Spring应用的开发，而无需过多关注XML的配置。提供了一堆依赖打包，并已经按照使用习惯解决了依赖问题—&gt;习惯大于约定。 Spring Boot基本上是Spring框架的扩展，它消除了设置Spring应用程序所需的XML配置，为更快，更高效的开发生态系统铺平了道路。Spring Boot中的一些特点： 创建独立的spring应用 嵌入Tomcat, JettyUndertow 而且不需要部署他们 提供的“starters” poms来简化Maven配置 尽可能自动配置spring应用 提供生产指标,健壮检查和外部化配置 绝对没有代码生成和XML配置要求 2.IOC与AOP IoC IOC就是控制反转，指创建对象的控制权转移给Spring框架进行管理，并由Spring根据配置文件去创建实例和管理各个实例之间的依赖关系，对象与对象之间松散耦合，也利于功能的复用。DI依赖注入，和控制反转是同一个概念的不同角度的描述，即 应用程序在运行时依赖IoC容器来动态注入对象需要的外部依赖。 最直观的表达就是，以前创建对象的主动权和时机都是由自己把控的，IOC让对象的创建不用去new了，可以由spring自动生产，使用java的反射机制，根据配置文件在运行时动态的去创建对象以及管理对象，并调用对象的方法的。 Spring的IOC有三种注入方式 ：构造器注入、setter方法注入、根据注解注入。 AOP AOP，一般称为面向切面，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，提高系统的可维护性。可用于权限认证、日志、事务处理。AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。 AOP相关概念 这里还是先给出一个比较专业的概念定义： Aspect（切面）： Aspect 声明类似于 Java 中的类声明，在 Aspect 中会包含着一些 Pointcut 以及相应的 Advice。 Joint point（连接点）：表示在程序中明确定义的点，典型的包括方法调用，对类成员的访问以及异常处理程序块的执行等等，它自身还可以嵌套其它 joint point。 Pointcut（切点）：表示一组 joint point，这些 joint point 或是通过逻辑关系组合起来，或是通过通配、正则表达式等方式集中起来，它定义了相应的 Advice 将要发生的地方。 Advice（增强）：Advice 定义了在 Pointcut 里面定义的程序点具体要做的操作，它通过 before、after 和 around 来区别是在每个 joint point 之前、之后还是代替执行的代码。 Target（目标对象）：织入 Advice 的目标对象.。 Weaving（织入）：将 Aspect 和其他对象连接起来, 并创建 Adviced object 的过程 然后举一个容易理解的例子： 下面我以一个简单的例子来比喻一下 AOP 中 Aspect, Joint point, Pointcut 与 Advice之间的关系. 让我们来假设一下, 从前有一个叫爪哇的小县城, 在一个月黑风高的晚上, 这个县城中发生了命案. 作案的凶手十分狡猾, 现场没有留下什么有价值的线索. 不过万幸的是, 刚从隔壁回来的老王恰好在这时候无意中发现了凶手行凶的过程, 但是由于天色已晚, 加上凶手蒙着面, 老王并没有看清凶手的面目, 只知道凶手是个男性, 身高约七尺五寸. 爪哇县的县令根据老王的描述, 对守门的士兵下命令说: 凡是发现有身高七尺五寸的男性, 都要抓过来审问. 士兵当然不敢违背县令的命令, 只好把进出城的所有符合条件的人都抓了起来. 来让我们看一下上面的一个小故事和 AOP 到底有什么对应关系. 首先我们知道, 在 Spring AOP 中 Joint point 指代的是所有方法的执行点, 而 point cut 是一个描述信息, 它修饰的是 Joint point, 通过 point cut, 我们就可以确定哪些 Joint point 可以被织入 Advice. 对应到我们在上面举的例子, 我们可以做一个简单的类比, Joint point 就相当于 爪哇的小县城里的百姓,pointcut 就相当于 老王所做的指控, 即凶手是个男性, 身高约七尺五寸, 而 Advice 则是施加在符合老王所描述的嫌疑人的动作: 抓过来审问. 为什么可以这样类比呢? Joint point ： 爪哇的小县城里的百姓: 因为根据定义, Joint point 是所有可能被织入 Advice 的候选的点, 在 Spring AOP中, 则可以认为所有方法执行点都是 Joint point. 而在我们上面的例子中, 命案发生在小县城中, 按理说在此县城中的所有人都有可能是嫌疑人. Pointcut ：男性, 身高约七尺五寸: 我们知道, 所有的方法(joint point) 都可以织入 Advice, 但是我们并不希望在所有方法上都织入 Advice, 而 Pointcut 的作用就是提供一组规则来匹配joinpoint, 给满足规则的 joinpoint 添加 Advice. 同理, 对于县令来说, 他再昏庸, 也知道不能把县城中的所有百姓都抓起来审问, 而是根据凶手是个男性, 身高约七尺五寸, 把符合条件的人抓起来. 在这里 凶手是个男性, 身高约七尺五寸 就是一个修饰谓语, 它限定了凶手的范围, 满足此修饰规则的百姓都是嫌疑人, 都需要抓起来审问. Advice ：抓过来审问, Advice 是一个动作, 即一段 Java 代码, 这段 Java 代码是作用于 point cut 所限定的那些 Joint point 上的. 同理, 对比到我们的例子中, 抓过来审问 这个动作就是对作用于那些满足 男性, 身高约七尺五寸 的爪哇的小县城里的百姓. Aspect:：Aspect 是 point cut 与 Advice 的组合, 因此在这里我们就可以类比: “根据老王的线索, 凡是发现有身高七尺五寸的男性, 都要抓过来审问” 这一整个动作可以被认为是一个 Aspect. 静态代理 AspectJ是静态代理，也称为编译时增强，AOP框架会在编译阶段生成AOP代理类，并将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。 动态代理 Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理： JDK动态代理：只提供接口的代理，不支持类的代理，要求被代理类实现接口。JDK动态代理的核心是InvocationHandler接口和Proxy类，在获取代理对象时，使用Proxy类来动态创建目标类的代理类（即最终真正的代理类，这个类继承自Proxy并实现了我们定义的接口），当代理对象调用真实对象的方法时， InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起。 InvocationHandler的invoke(Object proxy,Method method,Object[] args)： proxy：是最终生成的代理对象 method：是被代理目标实例的某个具体方法; args：是被代理目标实例某个方法的具体入参, 在方法反射调用时使用 CGLIB动态代理：如果被代理类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 静态代理与动态代理区别？ 生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。 应用场景 AOP 主要应用场景有： Authentication 权限 Caching 缓存 Context passing 内容传递 Error handling 错误处理 Lazy loading 懒加载 Debugging 调试 logging, tracing, profiling and monitoring 记录跟踪 优化 校准 Performance optimization 性能优化 Persistence 持久化 Resource pooling 资源池 Synchronization 同步 Transactions 事务 3.Spring事务 概念 事务在逻辑上是一组操作，要么执行，要不都不执行。主要是针对数据库而言的，比如说 MySQL。 事务的四个属性ACID **原子性(Automicity)：**事务中的诸多操作，要么都做，要么都不做 一致性(Consistency): 事务必须使数据库从一个一致性状态到另一个一致性状态 **隔离性(lsolation)😗*一个事务的执行不能被其他事务干扰，一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰 **持久性(Durability)😗*一个事务一旦提交，数据库中数据的改变应该是永久性的 实现方式 Spring 支持两种事务方式，分别是编程式事务和声明式事务，后者最常见，通常情况下只需要一个 **@Transactional **就搞定了（代码侵入性降到了最低）。而编程式事务管理，需要在代码中调用 commit()、rollback()等事务管理相关的方法来完成事务的实现。 Spring事务的传播机制 事务的传播，是指一个方法调用另一个方法并将事务传递给它。事务的转播机制主要针对被调用者而言，控制它是否被传播或者被怎样传播。spring事务的传播机制有七种: 传播行为 描述 PROPAGATION_REQUIRED 默认的Spring事物传播级别，若当前存在事务，则加入该事务，若不存在事务，则新建一个事务 PROPAGATION_REQUIRE_NEW 若当前没有事务，则新建一个事务。若当前存在事务，则新建 一个事务，新老事务相互独立。外部事务抛出异常回滚不会影响内部事务的正常提交 PROPAGATION_NESTED 如果当前存在事务，则嵌套在当前事务中执行。如果当前没有事务， 则新建一个事务，类似于REQUIRE_NEW PROPAGATION_SUPPORTS 支持当前事务，若当前不存在事务，以非事务的方式执行 PROPAGATION_NOT_SUPPORTED 以非事务的方式执行，若当前存在事务，则把当前事务挂起 PROPAGATION_MANDATORY 强制事务执行，若当前不存在事务，则抛出异常 PROPAGATION_NEVER 以非事务的方式执行，如果当前存在事务，则抛出异常 传播级别一般不需要定义，默认就是PROPAGATION_REQUIRED，除非在嵌套事务的情况。上述描述表格的描述还是比较抽象，下面我们使用一个例子来说明这个传播机制。假定方法A调用方法B： 方法B定义的事务类型 A方法有事务时 A方法无事务 @Transactional(propagation = Propagation.REQUIRED) B和A事务合并成一个事务 B新建一个事务 @Transactional(propagation = Propagation.REQUIRES_NEW) B新建一个事务，和A事务无关，互不影响 B新建一个事务 @Transactional(propagation = Propagation.NESTED) B新建一个A的子事务，A异常影响B，B异常不影响A B新建一个事务 @Transactional(propagation = Propagation.SUPPORTS) B加入到A事务中 B无事务 @Transactional(propagation = Propagation.NOT_SUPPORTED) 挂起A事务，B以无事务方式执行 B无事务 @Transactional(propagation = Propagation.MANDATORY) B加入到A事务中 B抛异常 @Transactional(propagation = Propagation.NEVER) B抛异常 B无事务 4.SpringBoot的启动步骤 springboot的启动经过了一些一系列的处理，我们先看看整体过程的流程图 你别说，步骤还挺多，但是不要紧，为了帮助大家理解，接下来将上面的编号一个个地讲解，以通俗易懂的方式告诉大家，里面都做了什么事情，废话不多说，开整 4.1、运行 SpringApplication.run() 方法 可以肯定的是，所有的标准的springboot的应用程序都是从run方法开始的 12345678910111213141516package com.spring;import org.springframework.beans.factory.config.ConfigurableListableBeanFactory;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext; @SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; // 启动springboot ConfigurableApplicationContext run = SpringApplication.run(App.class, args); &#125; &#125;123456789101112131415 进入run方法后，会 new 一个SpringApplication 对象，创建这个对象的构造函数做了一些准备工作，编号第2~5步就是构造函数里面所做的事情 123456789101112 /** * Static helper that can be used to run a &#123;@link SpringApplication&#125; from the * specified sources using default settings and user supplied arguments. * @param primarySources the primary sources to load * @param args the application arguments (usually passed from a Java main method) * @return the running &#123;@link ApplicationContext&#125; */ public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args); &#125;1234567891011 另外在说明一下，springboot启动有三种方式，其他的启动方式可参照我的另一个帖子： SpringBoot的三种启动方式 4.2、确定应用程序类型 在SpringApplication的构造方法内，首先会通过 WebApplicationType.deduceFromClasspath()； 方法判断当前应用程序的容器，默认使用的是Servlet 容器，除了servlet之外，还有NONE 和 REACTIVE （响应式编程）； 4.3、加载所有的初始化器 这里加载的初始化器是springboot自带初始化器，从从 META-INF/spring.factories 配置文件中加载的，那么这个文件在哪呢？自带有2个，分别在源码的jar包的 spring-boot-autoconfigure 项目 和 spring-boot 项目里面各有一个 spring.factories文件里面，看到开头是 org.springframework.context.ApplicationContextInitializer 接口就是初始化器了 ， 当然，我们也可以自己实现一个自定义的初始化器：实现 ApplicationContextInitializer接口既可 MyApplicationContextInitializer.java 1234567891011121314package com.spring.application; import org.springframework.context.ApplicationContextInitializer;import org.springframework.context.ConfigurableApplicationContext;/** * 自定义的初始化器 */public class MyApplicationContextInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext configurableApplicationContext) &#123; System.out.println(&quot;我是初始化的 MyApplicationContextInitializer...&quot;); &#125;&#125;12345678910111213 在resources目录下添加 META-INF/spring.factories 配置文件，内容如下，将自定义的初始化器注册进去； 123org.springframework.context.ApplicationContextInitializer=\\com.spring.application.MyApplicationContextInitializer12 启动springboot后，就可以看到控制台打印的内容了，在这里我们可以很直观的看到它的执行顺序，是在打印banner的后面执行的； 4.4、加载所有的监听器 加载监听器也是从 META-INF/spring.factories 配置文件中加载的，与初始化不同的是，监听器加载的是实现了 ApplicationListener 接口的类 自定义监听器也跟初始化器一样，依葫芦画瓢就可以了，这里不在举例； 4.5、设置程序运行的主类 deduceMainApplicationClass(); 这个方法仅仅是找到main方法所在的类，为后面的扫包作准备，deduce是推断的意思，所以准确地说，这个方法作用是推断出主方法所在的类； 4.6、开启计时器 程序运行到这里，就已经进入了run方法的主体了，第一步调用的run方法是静态方法，那个时候还没实例化SpringApplication对象，现在调用的run方法是非静态的，是需要实例化后才可以调用的，进来后首先会开启计时器，这个计时器有什么作用呢？顾名思义就使用来计时的嘛，计算springboot启动花了多长时间；关键代码如下： 12345// 实例化计时器StopWatch stopWatch = new StopWatch(); // 开始计时stopWatch.start();1234 run方法代码段截图 4.7、将java.awt.headless设置为true 这里将java.awt.headless设置为true，表示运行在服务器端，在没有显示器器和鼠标键盘的模式下照样可以工作，模拟输入输出设备功能。 做了这样的操作后,SpringBoot想干什么呢?其实是想设置该应用程序,即使没有检测到显示器,也允许其启动.对于服务器来说,是不需要显示器的,所以要这样设置. 方法主体如下： 12345 private void configureHeadlessProperty() &#123; System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty( SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless))); &#125;1234 通过方法可以看到，setProperty()方法里面又有个getProperty()；这不多此一举吗？其实getProperty()方法里面有2个参数， 第一个key值，第二个是默认值，意思是通过key值查找属性值，如果属性值为空，则返回默认值 true；保证了一定有值的情况； 4.8、获取并启用监听器 这一步 通过监听器来实现初始化的的基本操作，这一步做了2件事情 创建所有 Spring 运行监听器并发布应用启动事件 启用监听器 4.9、设置应用程序参数 将执行run方法时传入的参数封装成一个对象 仅仅是将参数封装成对象，没啥好说的，对象的构造函数如下 123456 public DefaultApplicationArguments(String[] args) &#123; Assert.notNull(args, &quot;Args must not be null&quot;); this.source = new Source(args); this.args = args; &#125;12345 那么问题来了，这个参数是从哪来的呢？其实就是main方法里面执行静态run方法传入的参数， 4.10、准备环境变量 准备环境变量，包含系统属性和用户配置的属性，执行的代码块在 prepareEnvironment 方法内 打了断点之后可以看到，它将maven和系统的环境变量都加载进来了 4.11、忽略bean信息 这个方法configureIgnoreBeanInfo() 这个方法是将 spring.beaninfo.ignore 的默认值值设为true，意思是跳过beanInfo的搜索，其设置默认值的原理和第7步一样； 12345678910 private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) &#123; if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) &#123; Boolean ignore = environment.getProperty(&quot;spring.beaninfo.ignore&quot;, Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); &#125; &#125;123456789 当然也可以在配置文件中添加以下配置来设为false 12spring.beaninfo.ignore=false1 目前还不知道这个配置的具体作用，待作者查明后在补上 4.12、打印 banner 信息 显而易见，这个流程就是用来打印控制台那个很大的spring的banner的，就是下面这个东东 那他在哪里打印的呢？他在 SpringBootBanner.java 里面打印的，这个类实现了Banner 接口， 而且banner信息是直接在代码里面写死的； 有些公司喜欢自定义banner信息，如果想要改成自己喜欢的图标该怎么办呢，其实很简单,只需要在resources目录下添加一个 banner.txt 的文件即可，txt文件内容如下 12345678910 _ _ (_) | | _ _ _____ ___ _ __ __| | ___ _ __ __ _| | | |/ _ \\ \\/ / | &#x27;_ \\ / _` |/ _ \\| &#x27;_ \\ / _` || |_| | __/&gt; &lt;| | | | | (_| | (_) | | | | (_| | \\__, |\\___/_/\\_\\_|_| |_|\\__,_|\\___/|_| |_|\\__, | __/ | __/ | |___/ |___/:: yexindong::123456789 一定要添加到resources目录下，别加错了 只需要加一个文件即可，其他什么都不用做，然后直接启动springboot，就可以看到效果了 4.13、创建应用程序的上下文 实例化应用程序的上下文， 调用 createApplicationContext() 方法，这里就是用反射创建对象，没什么好说的； 4.14、实例化异常报告器 异常报告器是用来捕捉全局异常使用的，当springboot应用程序在发生异常时，异常报告器会将其捕捉并做相应处理，在spring.factories 文件里配置了默认的异常报告器， 需要注意的是，这个异常报告器只会捕获启动过程抛出的异常，如果是在启动完成后，在用户请求时报错，异常报告器不会捕获请求中出现的异常， 了解原理了，接下来我们自己配置一个异常报告器来玩玩； MyExceptionReporter.java 继承 SpringBootExceptionReporter 接口 1234567891011121314151617181920212223package com.spring.application; import org.springframework.boot.SpringBootExceptionReporter;import org.springframework.context.ConfigurableApplicationContext; public class MyExceptionReporter implements SpringBootExceptionReporter &#123; private ConfigurableApplicationContext context; // 必须要有一个有参的构造函数，否则启动会报错 MyExceptionReporter(ConfigurableApplicationContext context) &#123; this.context = context; &#125; @Override public boolean reportException(Throwable failure) &#123; System.out.println(&quot;进入异常报告器&quot;); failure.printStackTrace(); // 返回false会打印详细springboot错误信息，返回true则只打印异常信息 return false; &#125;&#125;12345678910111213141516171819202122 在 spring.factories 文件中注册异常报告器 1234# Error Reporters 异常报告器org.springframework.boot.SpringBootExceptionReporter=\\com.spring.application.MyExceptionReporter123 接着我们在application.yml 中 把端口号设置为一个很大的值，这样肯定会报错， 123server: port: 8082888812 启动后，控制台打印如下图 4.15、准备上下文环境 这里准备的上下文环境是为了下一步刷新做准备的，里面还做了一些额外的事情； 4.15.1、实例化单例的beanName生成器 在 postProcessApplicationContext(context); 方法里面。使用单例模式创建 了BeanNameGenerator 对象，其实就是beanName生成器，用来生成bean对象的名称 4.15.2、执行初始化方法 初始化方法有哪些呢？还记得第3步里面加载的初始化器嘛？其实是执行第3步加载出来的所有初始化器，实现了ApplicationContextInitializer 接口的类 4.15.3、将启动参数注册到容器中 这里将启动参数以单例的模式注册到容器中，是为了以后方便拿来使用，参数的beanName 为 ：springApplicationArguments 4.16、刷新上下文 刷新上下文已经是spring的范畴了，自动装配和启动 tomcat就是在这个方法里面完成的，还有其他的spring自带的机制在这里就不一一细说了， 4.17、刷新上下文后置处理 afterRefresh 方法是启动后的一些处理，留给用户扩展使用，目前这个方法里面是空的， 123456789/** * Called after the context has been refreshed. * @param context the application context * @param args the application arguments */ protected void afterRefresh(ConfigurableApplicationContext context, ApplicationArguments args) &#123; &#125;12345678 4.18、结束计时器 到这一步，springboot其实就已经完成了，计时器会打印启动springboot的时长 在控制台看到启动还是挺快的，不到2秒就启动完成了； 4.19、发布上下文准备就绪事件 告诉应用程序，我已经准备好了，可以开始工作了 4.20、执行自定义的run方法 这是一个扩展功能，callRunners(context, applicationArguments) 可以在启动完成后执行自定义的run方法；有2中方式可以实现： 实现 ApplicationRunner 接口 实现 CommandLineRunner 接口 接下来我们验证一把，为了方便代码可读性，我把这2种方式都放在同一个类里面 123456789101112131415161718192021222324package com.spring.init; import org.springframework.boot.ApplicationArguments;import org.springframework.boot.ApplicationRunner;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component; /** * 自定义run方法的2种方式 */@Componentpublic class MyRunner implements ApplicationRunner, CommandLineRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println(&quot; 我是自定义的run方法1，实现 ApplicationRunner 接口既可运行&quot; ); &#125; @Override public void run(String... args) throws Exception &#123; System.out.println(&quot; 我是自定义的run方法2，实现 CommandLineRunner 接口既可运行&quot; ); &#125;&#125;1234567891011121314151617181920212223 启动springboot后就可以看到控制台打印的信息了 完 其实了解springboot启动原理对开发人员还是有好处的，至少你知道哪些东西是可以扩展的，以及怎么扩展，它的内部原理是怎么做的，我相信了解这些思路之后，让你自己写一个springboot出来也是可以的； 但是这里只是列出了启动过程，并不涉及到全部，源码是很负杂，记得一个大牛说过：“我们看源码的时候，只能通过联想或猜测作者是怎么想的，并且小心验证，就像我们小时候学古诗一样，也只能去猜测古人的想法，拿道德经来说，每个人读完后都有不同的看法，这就需要见仁见智了”； 5.Spring中自定义配置文件的读取 我们怎么来将配置文件里面对应的属性注入到类中了，方法有二 过时方法 1.1 在ConfigProperties类上加上如下注解： @ConfigurationProperties(locations=“classpath:config/user.properties”, prefix=“com.chhliu.springboot”) 其中locations属性用来指定配置文件的位置，prefix用来指定properties配置文件中的key前缀 1.2 在主类上加入配置支持 @EnableConfigurationProperties(ConfigProperties.class) 这样就可以将properties配置文件中的值注入到类中对应的属性上去了，但是上面的这种方式已经被标注为过时了，在新的版本中是不可用的。 替代方法 1.1 在ConfigProperties类上加上如下注解： @Component// 以组件的方式使用，使用的时候可以直接注入 @ConfigurationProperties(prefix=“com.chhliu.springboot”)// 用来指定properties配置文件中的key前缀 @PropertySource(“classpath:config/user.properties”)// 用来指定配置文件的位置 1.2 关闭配置属性的支持 这一步就是将主类上的**@EnableConfigurationProperties这个注解注释掉**，不让springboot自动配置，使用我们的手动配置 6.BeanFactory和FactoryBean的区别 区别：BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似。 BeanFactory和FactoryBean都可以用来创建对象，只不过创建的流程和方式不同 当使用BeanFactory的时候，必须要严格的遵守bean的生命周期，经过一系列繁杂的步骤之后可以创建出单例对象，是流水线式的创建过程，而FactoryBean是用户可以自定义bean对象的创建流程，不需要按照bean的生命周期来创建，在此接口中包含了三个方法： isSingleton:判断是否是单例对象 getObjectType:获取对象的类型 getObject:在此方法中可以自己创建对象，使用new的方式或者使用代理的方式都可以，用户可以按照自己的需要随意去创建对象，在很多框架继承的时候都会实现FactoryBean接口，比如Feign BeanFactory ​ BeanFactory定义了IOC容器的最基本形式，并提供了IOC容器应遵守的的最基本的接口，也就是Spring IOC所遵守的最底层和最基本的编程规范。在Spring代码中，BeanFactory只是个接口，并不是IOC容器的具体实现，但是Spring容器给出了很多种实现，如 DefaultListableBeanFactory、XmlBeanFactory、ApplicationContext等，都是附加了某种功能的实现。 使用场景： 从Ioc容器中获取Bean(byName or byType) 检索Ioc容器中是否包含指定的Bean 判断Bean是否为单例 FactoryBean 一般情况下，Spring通过反射机制利用&lt;bean&gt;的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在&lt;bean&gt;中提供大量的配置信息。配置方式的灵活性是受限的，这时采用编码的方式可能会得到一个简单的方案。Spring为此提供了一个org.springframework.bean.factory.FactoryBean的工厂类接口，用户可以通过实现该接口定制实例化Bean的逻辑。 使用场景： FactoryBean在Spring中最为典型的一个应用就是用来创建AOP的代理对象。 我们知道AOP实际上是Spring在运行时创建了一个代理对象，也就是说这个对象，是我们在运行时创建的，而不是一开始就定义好的，这很符合工厂方法模式。更形象地说，AOP代理对象通过Java的反射机制，在运行时创建了一个代理对象，在代理对象的目标方法中根据业务要求织入了相应的方法。这个对象在Spring中就是——ProxyFactoryBean。 所以，FactoryBean为我们实例化Bean提供了一个更为灵活的方式，我们可以通过FactoryBean创建出更为复杂的Bean实例。 spring容器的生命周期 Spring 容器说一下","categories":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"},{"name":"ssb","slug":"ssb","permalink":"http://cloud-tour.github.io/tags/ssb/"}]},{"title":"jvm","slug":"jvm","date":"2023-02-17T13:16:37.239Z","updated":"2023-03-18T10:25:58.314Z","comments":true,"path":"2023/02/17/jvm/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/jvm/","excerpt":"","text":"一.JVM内存模型及其各部分的作用 从宏观上来说JVM 内存区域 分为三部分线程共享区域、线程私有区域、直接内存区域(上图static变量该在堆区)。 线程共享区域 堆区 堆区Heap是JVM中最大的一块内存区域，基本上所有的对象实例都是在堆上分配空间。堆区细分为年轻代和老年代，其中年轻代又分为Eden、S0、S1 三个部分，他们默认的比例是8:1:1的大小。 元空间 方法区： 在 《Java虚拟机规范》中只是规定了有方法区这么个概念跟它的作用。HotSpot在JDK8之前 搞了个永久代把这个概念实现了。用来主要存储类信息、常量池、静态变量、JIT编译后的代码等数据。 PermGen(永久代)中类的元数据信息在每次FullGC的时候可能会被收集，但成绩很难令人满意。而且为PermGen分配多大的空间因为存储上述多种数据很难确定大小。因此官方在JDK8剔除移除永久代。 元空间： 在Java中用永久代来存储类信息，常量，静态变量等数据不是好办法，因为这样很容易造成内存溢出。同时对永久代的性能调优也很困难，因此在JDK8中 把永久代去除了，引入了元空间metaspace，原先的class、field等变量放入到metaspace。 总结： 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过参数来指定元空间的大小。 直接内存区域 直接内存： 一般使用Native函数操作C++代码来实现直接分配堆外内存，不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。这块内存不受Java堆空间大小的限制，但是受本机总内存大小限制所以也会出现OOM异常。分配空间后避免了在Java堆区跟Native堆中来回复制数据，可以有效提高读写效率，但它的创建、销毁却比普通Buffer慢。 PS：如果使用了NIO，本地内存区域会被频繁的使用，此时 jvm内存 ≈ 方法区 ＋ 堆 ＋ 栈＋ 直接内存 线程私有区域 程序计数器、虚拟机栈、本地方法栈跟线程的生命周期是一样的。 程序计数器 课堂上比如你正在看小说《诛仙》，看到1412章节时，老师喊你回答问题，这个时候你肯定要先应付老师的问题，回答完毕后继续接着看，这个时候你可以用书签也可以凭借记忆记住自己在看的位置，通过这样实现继续阅读。 落实到代码运行时候同样道理，程序计数器用于记录当前线程下虚拟机正在执行的字节码的指令地址。它具有如下特性： 线程私有 多线程情况下，在同一时刻所以为了让线程切换后依然能恢复到原位，每条线程都需要有各自独立的程序计数器。 没有规定OutOfMemoryError 程序计数器存储的是字节码文件的行号，而这个范围是可知晓的，在一开始分配内存时就可以分配一个绝对不会溢出的内存。 执行Native方法时值为空 Native方法大多是通过C实现并未编译成需要执行的字节码指令，也就不需要去存储字节码文件的行号了。 虚拟机栈 方法的出入栈：调用的方法会被打包成栈桢，一个栈桢至少需要包含一个局部变量表、操作数栈、桢数据区、动态链接。 动态链接： 当栈帧内部包含一个指向运行时常量池引用前提下，类加载时候会进行符号引用到直接引用的解析跟链接替换。 局部变量表： 局部变量表是栈帧重要组中部分之一。他主要保存函数的参数以及局部的变量信息。局部变量表中的变量作用域是当前调用的函数。函数调用结束后，随着函数栈帧的销毁。局部变量表也会随之销毁，释放空间。 操作数栈： 保存着Java虚拟机执行过程中数据 方法返回地址： 方法被调用的位置，当方法退出时候实际上等同于当前栈帧出栈。 本地方法栈 跟虚拟机栈类似，只是为使用到的Native方法服务而已。 二.new一个对象的过程 一个Java类从编码到最终完成执行，主要包括两个过程，编译、运行。 编译：将我们写好的.java文件通过Javac命令编译成.class文件。 运行：把编译生成的.class文件交由JVM执行。 Jvm运行class类的时候，并不是一次性将所有的类都加载到内存中，而是用到哪个就加载哪个，并且只加载一次。 类的生命周期 加载 加载指的是把class字节码文件从各个来源通过类加载器装载入内存中，这里有两个重点： 字节码来源：一般的加载来源包括从本地路径下编译生成的.class文件，从jar包中的.class文件，从远程网络，以及动态代理实时编译 类加载器：一般包括启动类加载器，扩展类加载器，应用类加载器，以及用户的自定义类加载器(加密解密那种)。 将类的字节码载入方法区，并创建类.class 对象 如果此类的父类没有加载，先加载父类 加载是懒惰执行 验证 主要是为了保证加载进来的字节流符合虚拟机规范，不会造成安全错误。文件格式验证、元数据验证、字节码验证、符号引用验证。 准备 给类静态变量分配内存空间，仅仅是分配空间，比如 public static int age = 14，在准备后age = 0，在初始化阶段 age = 14，如果添加了final则在这个阶段直接赋值为14。 解析 将常量池内的符号引用替换为直接引用。 初始化 前面在加载类阶段用户应用程序可以通过自定义类加载器参与之外 其余动作完全由虚拟机主导和控制。此时才是真正开始执行类中定义的代码 ：静态代码块、static 修饰的变量赋值、static final 修饰的引用类型变量赋值，会被合并成一个 &lt;cinit&gt; 方法，在初始化时被调用，如果存在父类，先对父类进行初始化。 使用 类加载完毕后紧接着就是为对象分配内存空间和初始化了： 为对象分配合适大小的内存空间 为实例变量赋默认值 设置对象的头信息，对象hash码、GC分代年龄、元数据信息等 执行构造函数(init)初始化。 卸载 最终没啥说等，就是通过GC算法回收对象了。 三.类的初始化顺序 （静态变量、静态初始化块：决于它们在类中出现的先后顺序）&gt;（变量、初始化块：决于它们在类中出现的先后顺序）&gt;构造器 原理： 1、加载类信息。在实例化对象之前，类的装载器会找到需要加载的类class文件，进行类的加载（有父类的会先加载父类），一旦加载到最根上的基类，就会对基类的静态变量和静态初始化块进行初始化； 2、当所有类信息加载完毕就会执行main（）主方法，然后执行new class（），对类进行实例化，首先对变量和、初始化块以及类的构造函数进行初始化（有父类的首先会对父类进行初始化，多个父类递归的方式） 有父类的加载顺序： 父类–静态变量 父类–静态初始化块 子类–静态变量 子类–静态初始化块 父类–变量 父类–初始化块 父类–构造器 子类–变量 子类–初始化块 子类–构造器 四.类加载器 在连接阶段一般是无法干预的，大部分干预 类加载阶段，对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，类加载时候重要三个方法： 1、loadClass() ：加载目标类的入口，它首先会查找当前 ClassLoader 以及它的双亲里面是否已经加载了目标类，找到直接返回 2、findClass() ：如果没有找到就会让双亲尝试加载，如果双亲都加载不了，就会调用 findClass() 让自定义加载器自己来加载目标类 3、defineClass() ：拿到这个字节码之后再调用 defineClass() 方法将字节码转换成 Class 对象。 双亲委派机制 定义： 当某个类加载器需要加载某个.class文件时，首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，自己才会去加载这个类。 作用： 1、可以防止重复加载同一个.class。通过委托去向上面问一问，加载过了，就不用再加载一遍。保证数据安全。 2、保证核心*.class不能被篡改，通过委托方式，不会去篡改核心.class。’ 类加载器： 1、BootstrapClassLoader（启动类加载器）：c++编写，加载java核心库 java.*，JAVA_HOME/lib 2、ExtClassLoader （标准扩展类加载器）：java编写的加载扩展库，JAVA_HOME/lib/ext 3、AppClassLoader（系统类加载器）：加载程序所在的目录，如user.dir所在的位置的ClassPath 4、CustomClassLoader（用户自定义类加载器）：用户自定义的类加载器,可加载指定路径的class文件 关于加载机制 双亲委派机制只是Java类加载的一种常见模式，还有别的加载机制哦，比如Tomcat 总是先尝试去加载某个类，如果找不到再用上一级的加载器，跟双亲加载器顺序正好相反。再比如当使用第三方框架JDBC跟具体实现的时候，反而会引发错误，因为JDK自带的JDBC接口由启动类加载，而第三方实现接口由应用类加载。这样相互之间是不认识的，因此JDK引入了SPI机制 线程上下文加载器 来实现加载(跟Dubbo的SPI不一样哦)。 补充问题 同一个类能被不同的类加载器加载吗？ 答：可以加载，虽然类的全限定名一致，但是如果由不同的类加载器加载后，在jvm中会被认为是两个不同的类。这是由于类加载器的单一性，虽然父类型的加载器对于子类加载器是可见的，但是类加载器“邻居”之间，同一类型仍然可以被加载多次，因为相互不可见。 怎么判定两个class是相同的？ 答：Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。 五.java程序运行的过程 创建 JVM，调用类加载子系统加载 class，将类的信息存入方法区 创建 main 线程，使用的内存区域是 JVM 虚拟机栈，开始执行 main 方法代码 如果遇到了未见过的类，会继续触发类加载过程，同样会存入方法区 需要创建对象，会使用堆内存来存储对象 不再使用的对象，会由垃圾回收器在内存不足时回收其内存 调用方法时，方法内的局部变量、方法参数所使用的是 JVM 虚拟机栈中的栈帧内存 调用方法时，先要到方法区获得到该方法的字节码指令，由解释器将字节码指令解释为机器码执行 调用方法时，会将要执行的指令行号读到程序计数器，这样当发生了线程切换，恢复时就可以从中断的位置继续 对于非 java 实现的方法调用，使用内存称为本地方法栈（见说明） 对于热点方法调用，或者频繁的循环代码，由 JIT 即时编译器将这些代码编译成机器码缓存，提高执行性能 六.JVM跨代引用问题如何处理 跨代引用 场景： 年轻代的对象持有着老年代对象的引用、老年代的对象持有着年轻代对象的引用 特点： 互相引用的两个对象几乎总是同生共死： 如果某个新生代对象存在跨代引用，由于老年代对象难以消亡，该引用会使得新生对象在minor gc时得以存活，该对象经历多次minor gc后晋升到老年代，此时跨代引用自然也随着该对象的晋升而消失了。 因此，存在跨代引用的对象较少。 跨代引用带来的问题： 年轻代对象引用老年代对象：minor gc时需要扫描老年代。 老年代对象引用年轻代对象：major gc时需要扫描年轻代。 解决方案： 通过降低扫描对象的范围或数量来降低gc的耗时： 年轻代对象引用老年代对象：借助 卡表/记忆集合 来减小minor gc时扫描老年代的范围，进而降低minor gc的时间。 老年代对象引用年轻代对象： CMS收集器：借助 提前触发minor gc 来减少年轻代中对象的数量，进而降低major gc的时间。 G1收集器：借助 记忆集合 来减少老年代gc时扫描年轻代的范围，进而降低minor gc的时间。 卡表 数据结构： **卡表：**jvm将老年代划分为若干个大小为512字节的区域(card)，并使用一个 字节(byte)数组 来标记老年代中这些区域(card)中的对象是否持有新生代对象的引用。jvm将这个 字节数组 称为卡表(card table)。 卡表中的元素：表示老年代中某块区域(card)中的对象是否持有新生代对象的引用。 卡表属于points-out(我引用了谁的对象)的结构。 说明：之所以使用byte数组而不是bit数组主要是速度上的考量，现代计算机硬件都是最小按字节寻址的，没有直接存储一个bit指令，所以要用bit的话就不得不多消耗几条shift+mask指令。 原理： 当老年代中的某个对象持有了新生代对象的引用时，jvm将卡表中表示该对象所在区域(card)的元素设为1，表示该对象所在区域(card)是一个 dirty card。（注意：新生代对象引用老年代对象时，老年代对象所在的区域(card)不会被标记为dirty card）。 年轻代gc时只扫描dirty card中的对象，而无需扫描整个老年代中的对象，从而减少年轻代gc的停顿时间。 当完成所有脏卡的扫描之后，jvm便会将所有脏卡的标识位清零。 card标记为dirty card的原理 写屏障： 写屏障是一小段将card标记为dirty card的代码：检查对象的引用变更时是否出现了跨代引用(g1是跨region引用)，如果出现，这将对应的card标记为dirty card。 Hotspot VM的字节码解释器和JIT编译器使用写屏障维护卡表： 解释器每次执行更新引用的字节码时，都会执行一段写屏障。 JIT编译器在生成更新引用的代码后，也会生成一段写屏障。 虽然写屏障使得应用线程增加了一些性能开销，但是minor gc的效率提高了很多，进而提高了系统的吞吐量。 记忆集合 数据结构: 每个region都维护着一个记忆集合(Remembered Set / Rset)，收集器在标记跨代引用的对象时只需扫描(CSet中region维护的)RSet即可。 RSet的整体结构是一个哈希表，底层是在卡表的基础上实现的。 key：key记录了引用本region中对象的对象所在region的位置。 value：是一个集合，其元素是：其它region(由key确定是哪个region)中的对象引用本region中对象的引用及引用所在的卡表位置。 Rset属于points-into结构(谁引用了我的对象) RSet、Card和Region的关系 每个region被分成了多个card。 不同region中的card会相互引用。 图示： Region1中的Card中的对象引用了Region2中的Card中的对象，蓝色实线表示的就是points-out的关系，而在Region2的RSet中，记录了Region1的Card，即红色虚线表示的关系，这就是points-into。 七.判断对象是否为垃圾（是否有引用） 引用计数法 引用计数法（Reference Count）会给对象中添加一个引用计数器，每当有一个地方引用它的时候，计数器的值就 +1 ，当引用失效时，计数器值就 -1 ，计数器的值为 0 的对象不可能在被使用，这个时候就可以判定这个对象是垃圾。 当图中的数值变成0时，这个时候使用引用计数算法就可以判定它是垃圾了，但是引用计数法不能解决一个问题，就是当对象是循环引用的时候，计数器值都不为0，这个时候引用计数器无法通知GC收集器来回收他们，如下图所示： 这个时候就需要使用到我们的根可达算法。 可达性分析 根可达算法（Root Searching）的意思是说从根上开始搜索，当一个程序启动后，马上需要的那些个对象就叫做根对象，所谓的根可达算法就是首先找到根对象，然后跟着这根线一直往外找到那些有用的。常见的GC roots如下： 线程栈变量： 第一种是虚拟机栈中的引用的对象，在程序中正常创建一个对象，对象会在堆上开辟一块空间，同时会将这块空间的地址作为引用保存到虚拟机栈中，如果对象生命周期结束了，那么引用就会从虚拟机栈中出栈，因此如果在虚拟机栈中有引用，就说明这个对象还是有用的，这种情况是最常见的。 静态变量：第二种是我们在类中定义了全局的静态的对象，也就是使用了static关键字，由于虚拟机栈是线程私有的，所以这种对象的引用会保存在共有的方法区中，显然将方法区中的静态引用作为GC Roots是必须的。 常量池： 第三种便是常量引用，就是使用了static final关键字，由于这种引用初始化之后不会修改，所以方法区常量池里的引用的对象也应该作为GC Roots。 JNI： 第四种是在使用JNI技术时，有时候单纯的Java代码并不能满足我们的需求，我们可能需要在Java中调用C或C++的代码，因此会使用Native方法，JVM内存中专门有一块本地方法栈，用来保存这些对象的引用，所以本地方法栈中引用的对象也会被作为GC Roots。 八.GC垃圾收集器 GC垃圾收集器的JVM配置参数： -XX:+UseSerialGC：年轻代和老年代都用串行收集器 -XX:+UseParNewGC：年轻代使用 ParNew，老年代使用 Serial Old -XX:+UseParallelGC：年轻代使用 ParallerGC，老年代使用 Serial Old -XX:+UseParallelOldGC：新生代和老年代都使用并行收集器 -XX:+UseConcMarkSweepGC：表示年轻代使用 ParNew，老年代的用 CMS -XX:+UseG1GC：使用 G1垃圾回收器 -XX:+UseZGC：使用 ZGC 垃圾回收器 年轻代收集器 新生代有Serial、ParNew、Parallel Scavenge三种垃圾收集器。 名称 串行/并行/并发 回收算法 使用场景 可以跟CMS配合 Serial 串行 复制 单CPU，Client模式下虚拟机 是 ParNew 并行(Serial的并行版) 复制 多CPU，常在Server模式 是 Parallel Scavenge 并行 复制 多CPU且关注吞吐量 否 Serial收集器 处理GC的只有一条线程，并且在垃圾回收的过程中暂停一切用户线程。最简单的垃圾回收器，但千万别以为它没有用武之地。因为简单，所以高效，它通常用在客户端应用上。因为客户端应用不会频繁创建很多对象，用户也不会感觉出明显的卡顿。相反，它使用的资源更少，也更轻量级。 ParNew收集器 ParNew是Serial的多线程版本。由多条GC线程并行地进行垃圾清理。清理过程依然要停止用户线程。ParNew 追求“低停顿时间”，与 Serial 唯一区别就是使用了多线程进行垃圾收集，在多 CPU 环境下性能比 Serial 会有一定程度的提升；但线程切换需要额外的开销，因此在单 CPU 环境中表现不如 Serial。 Parallel Scavenge收集器 另一个多线程版本的垃圾回收器。它与ParNew的主要区别是： Parallel Scavenge：追求CPU吞吐量，能够在较短时间完成任务，适合没有交互的后台计算。弱交互强计算 ParNew：追求降低用户停顿时间，适合交互式应用。强交互弱计算 老年代收集器 老年代有Serial Old、Parallel Old、CMS 三种垃圾收集器。 名称 串行/并行/并发 回收算法 使用场景 组合年轻代 Serial Old 串行 标记整理 单CPU Serial 、ParNew、Parallel Scavenge Parallel Old 并行 标记整理 多CPU Parallel Scavenge CMS 并发 标记清除 多CPU且关注吞吐量，常用Server端 Serial 、ParNew Serial Old收集器 与年轻代的 Serial 垃圾收集器对应，都是单线程版本，同样适合客户端使用。年轻代的 Serial，使用复制算法。老年代的 Old Serial，使用标记-整理算法。 Parallel Old收集器 Parallel Old 收集器是 Parallel Scavenge 的老年代版本，追求 CPU 吞吐量。 CMS收集器 并发标记清除(Concurrent Mark Sweep,CMS)垃圾回收器，是一款致力于获取最短停顿时间的收集器，使用多个线程来扫描堆内存并标记可被清除的对象，然后清除标记的对象。在下面两种情形下会暂停工作线程： 在老年代中标记引用对象的时候 在做垃圾回收的过程中堆内存中有变化发生 对比与并行垃圾回收器，CMS回收器使用更多的CPU来保证更高的吞吐量。如果我们可以有更多的CPU用来提升性能，那么CMS垃圾回收器是比并行回收器更好的选择。使用 -XX:+UseParNewGCJVM 参数来开启使用CMS垃圾回收器。 主要流程如下： 初始标记(CMS initial mark)：仅标记出GC Roots能直接关联到的对象。需要Stop-the-world 并发标记(CMS concurrenr mark)：进行GC Roots遍历的过程，寻找出所有可达对象 重新标记(CMS remark)：修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录。需要Stop-the-world 并发清除(CMS concurrent sweep)：清出垃圾 CMS触发机制：当老年代的使用率达到80%时，就会触发一次CMS GC。 -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly 优点 并发收集 停顿时间最短 缺点 并发回收导致CPU资源紧张 在并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程而导致应用程序变慢，降低程序总吞吐量。CMS默认启动的回收线程数是：（CPU核数 + 3）/ 4，当CPU核数不足四个时，CMS对用户程序的影响就可能变得很大。 无法清理浮动垃圾 在CMS的并发标记和并发清理阶段，用户线程还在继续运行，就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留到下一次垃圾收集时再清理掉。这一部分垃圾称为“浮动垃圾”。 并发失败（Concurrent Mode Failure） 由于在垃圾回收阶段用户线程还在并发运行，那就还需要预留足够的内存空间提供给用户线程使用，因此CMS不能像其他回收器那样等到老年代几乎完全被填满了再进行回收，必须预留一部分空间供并发回收时的程序运行使用。默认情况下，当老年代使用了 80% 的空间后就会触发 CMS 垃圾回收，这个值可以通过 -XX**😗* CMSInitiatingOccupancyFraction 参数来设置。 这里会有一个风险：要是CMS运行期间预留的内存无法满足程序分配新对象的需要，就会出现一次“并发失败”（Concurrent Mode Failure），这时候虚拟机将不得不启动后备预案：Stop The World，临时启用 Serial Old 来重新进行老年代的垃圾回收，这样一来停顿时间就很长了。 内存碎片问题 CMS是一款基于“标记-清除”算法实现的回收器，这意味着回收结束时会有内存碎片产生。内存碎片过多时，将会给大对象分配带来麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次 Full GC 的情况。 为了解决这个问题，CMS收集器提供了一个 -XX:+UseCMSCompactAtFullCollection 开关参数（默认开启），用于在 Full GC 时开启内存碎片的合并整理过程，由于这个内存整理必须移动存活对象，是无法并发的，这样停顿时间就会变长。还有另外一个参数 -XX:CMSFullGCsBeforeCompaction，这个参数的作用是要求CMS在执行过若干次不整理空间的 Full GC 之后，下一次进入 Full GC 前会先进行碎片整理（默认值为0，表示每次进入 Full GC 时都进行碎片整理）。 作用内存区域：老年代 适用场景：对停顿时间敏感的场合 算法类型：标记-清除 G1收集器 G1（Garbage First）回收器采用面向局部收集的设计思路和基于Region的内存布局形式，是一款主要面向服务端应用的垃圾回收器。G1设计初衷就是替换 CMS，成为一种全功能收集器。G1 在JDK9 之后成为服务端模式下的默认垃圾回收器，取代了 Parallel Scavenge 加 Parallel Old 的默认组合，而 CMS 被声明为不推荐使用的垃圾回收器。G1从整体来看是基于 标记-整理 算法实现的回收器，但从局部（两个Region之间）上看又是基于 标记-复制 算法实现的。G1 回收过程，G1 回收器的运作过程大致可分为四个步骤： 初始标记（会STW）：仅仅只是标记一下 GC Roots 能直接关联到的对象，并且修改TAMS指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。 并发标记：从 GC Roots 开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理在并发时有引用变动的对象。并利用Rset标记跨代引用对象。 最终标记（会STW）：对用户线程做短暂的暂停，处理并发阶段结束后仍有引用变动的对象。 清理阶段（会STW）：更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作涉及存活对象的移动，必须暂停用户线程，由多条回收器线程并行完成的。 G1收集器中的堆内存被划分为多个大小相等的内存块（Region），每个Region是逻辑连续的一段内存，结构如下： 每个Region被标记了E、S、O和H，说明每个Region在运行时都充当了一种角色，其中H是以往算法中没有的，它代表Humongous（巨大的），这表示这些Region存储的是巨型对象（humongous object，H-obj），当新建对象大小超过Region大小一半时，直接在新的一个或多个连续Region中分配，并标记为H。 同时G1中引入了RememberSets、CollectionSets帮助更好的执行GC 。 1、RememberSets： RSet 记录了其他Region中的对象引用本Region中对象的关系，属于points-into结构（谁引用了我的对象） 2、CollectionSets：Csets 是一次GC中需要被清理的regions集合，注意G1每次GC不是全部region都参与的，可能只清理少数几个，这几个就被叫做Csets。在GC的时候，对于old -&gt; young 和old -&gt; old的跨代对象引用，只要扫描对应的CSet中的RSet即可。 G1进行GC的时候一般分为Yang GC跟Mixed GC。 Young GC：CSet 就是所有年轻代里面的Region Mixed GC：CSet 是所有年轻代里的Region加上在全局并发标记阶段标记出来的收益高的Region Region 堆内存中一个Region的大小可以通过 -XX:G1HeapRegionSize参数指定，大小区间只能是1M、2M、4M、8M、16M和32M，总之是2的幂次方。如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实际大小，默认把堆内存按照2048份均分，最后得到一个合理的大小。 GC模式 young gc 发生在年轻代的GC算法，一般对象（除了巨型对象）都是在eden region中分配内存，当所有eden region被耗尽无法申请内存时，就会触发一次young gc，这种触发机制和之前的young gc差不多，执行完一次young gc，活跃对象会被拷贝到survivor region或者晋升到old region中，空闲的region会被放入空闲列表中，等待下次被使用。 -XX:MaxGCPauseMillis：设置G1收集过程目标时间，默认值200ms -XX:G1NewSizePercent：新生代最小值，默认值5% -XX:G1MaxNewSizePercent：新生代最大值，默认值60% mixed gc 当越来越多的对象晋升到老年代old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即mixed gc，该算法并不是一个old gc，除了回收整个young region，还会回收一部分的old region，这里需要注意：是一部分老年代，而不是全部老年代，可以选择哪些old region进行收集，从而可以对垃圾回收的耗时时间进行控制 full gc 如果对象内存分配速度过快，mixed gc来不及回收，导致老年代被填满，就会触发一次full gc，G1的full gc算法就是单线程执行的serial old gc，会导致异常长时间的暂停时间，需要进行不断的调优，尽可能的避免full gc G1垃圾回收器 应用于大的堆内存空间。它将堆内存空间划分为不同的区域，对各个区域并行地做回收工作。G1在回收内存空间后还立即对空闲空间做整合工作以减少碎片。CMS却是在全部停止(stop the world,STW)时执行内存整合工作。对于不同的区域G1根据垃圾的数量决定优先级。使用 -XX:UseG1GCJVM 参数来开启使用G1垃圾回收器。 主要流程如下： 初始标记(Initial Marking)：标记从GC Root可达的对象。会发生STW 并发标记(Concurrenr Marking)：标记出GC Root可达对象衍生出去的存活对象，并收集各个Region的存活对象信息。整个过程gc collector线程与应用线程可以并行执行 最终标记(Final Marking)：标记出在并发标记过程中遗漏的，或内部引用发生变化的对象。会发生STW 筛选回收(Live Data Counting And Evacution)：垃圾清除过程，如果发现一个Region中没有存活对象，则把该Region加入到空闲列表中 -XX:InitiatingHeapOccupancyPercent：当老年代大小占整个堆大小百分比达到该阈值时，会触发一次mixed gc。 回收总结： 1、经过global concurrent marking，collector就知道哪些Region有存活的对象。并将那些完全可回收的Region(没有存活对象)收集起来加入到可分配Region队列，实现对该部分内存的回收。对于有存活对象的Region，G1会根据统计模型找处收益最高、开销不超过用户指定的上限的若干Region进行对象回收。这些选中被回收的Region组成的集合就叫做collection set 简称Cset！ 2、在MIX GC中的Cset = 所有年轻代里的region + 根据global concurrent marking统计得出收集收益高的若干old region。 3、在YGC中的Cset = 所有年轻代里的region + 通过控制年轻代的region个数来控制young GC的开销。 4、YGC 与 MIXGC 都是采用多线程复制清理，整个过程会STW。G1的低延迟原理在于其回收的区域变得精确并且范围变小了。 优点： 1、**并行与并发：**G1能充分利用多CPU、多核环境下的硬件优势，可以通过并发的方式让Java程序继续执行。 2、**分代收集：**分代概念在G1中依然得以保留，它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象来获得更好的收集效果。 3、**空间整合：**G1从整体上看是基于标记-整理算法实现的，从局部(两个Region之间)上看是基于复制算法实现的，G1运行期间不会产生内存空间碎片。 4、**可预测停顿：**G1比CMS牛在能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。 缺点： 需要记忆集来记录新生代和老年代之间的引用关系 需要占用大量的内存，可能达到整个堆内存容量的20%甚至更多 作用内存区域：跨代 适用场景：作为关注停顿时间的场景的收集器备选方案 算法类型：整体来看基于”标记-整理算法“，局部采用&quot;复制算法&quot; 九.new对象时的堆抢占问题 问题：jvm 里 new 对象时，堆会不会发生抢占？怎么设计jvm的堆的线程安全？ TLAB 什么是TLAB？它是干什么的？咋们先抛开这个问题，一切的开始得从new对象到指针碰撞开始讲起。 new对象与指针碰撞 new对象怎么就出问题了呢？ java中我们要创建一个对象,用关键字new就可以了。但是，在我们日常中，有很多生命周期很短的对象。比如： 1234public void dome()&#123; User user=new user(); user.sayhi();&#125; 什么是指针碰撞呢？ 假设JVM虚拟机上，堆内存都是规整的。堆内存被一个指针一分为二。指针的左边都被塞满了对象，指针的右变是未使用的区域。每一次有新的对象创建，指针就会向右移动一个对象size的距离。这就被称为指针碰撞。 image 好，问题来了。如果我们多线程执行刚才那个dome方法，一个线程正在给A对象分配内存，指针还没有来的及修改，其它为B对象分配内存的线程，而且还是引用这之前的指针指向。这样就出现毛病了。如果想避免冲突的话,我们可以进行加锁,但是加锁会导致性能比较低,因此JVM里没有采用这种方式; TLAB的出现 我们现在已经搞清楚，我们出现了哪些问题。我在为大家介绍一下今天的主角。 TLAB的全称是Thread Local Allocation Buffer，即**线程本地分配缓存区，这是一个线程专用的内存分配区域。** 如果设置了虚拟机参数**-XX:UseTLAB，在线程初始化时，同时也会申请一块指定大小的内存，只给当前线程使用，这样每个线程都单独拥有一个空间**，如果需要分配内存，就在自己的空间上分配，这样就不存在竞争的情况，可以大大提升分配效率。 TLAB空间的内存非常小，缺省情况下仅占有整个Eden空间的1%，也可以通过选项-XX:TLABWasteTargetPercent设置TLAB空间所占用Eden空间的百分比大小。 TLAB的本质其实是三个指针管理的区域：start，top 和 end，每个线程都会从Eden分配一块空间，例如说100KB，作为自己的TLAB，其中 start 和 end 是占位用的，标识出 eden 里被这个 TLAB 所管理的区域，卡住eden里的一块空间不让其它线程来这里分配。 TLAB只是让每个线程有私有的分配指针，但底下存对象的内存空间还是给所有线程访问的，只是其它线程无法在这个区域分配而已。从这一点看，它被翻译为 线程私有分配区 更为合理一点 当一个TLAB用满（分配指针top撞上分配极限end了），就新申请一个TLAB，而在老TLAB里的对象还留在原地什么都不用管——它们无法感知自己是否是曾经从TLAB分配出来的，而只关心自己是在eden里分配的。 TLAB的缺点 事务总不是完美的，TLAB也又自己的缺点。因为TLAB通常很小，所以放不下大对象。 1，TLAB空间大小是固定的，但是这时候一个大对象，我TLAB剩余的空间已经容不下它了。(比如100kb的TLAB，来了个110KB的对象) 2，TLAB空间还剩一点点没有用到，有点舍不得。(比如100kb的TLAB，装了80KB，又来了个30KB的对象) 所以JVM开发人员做了以下处理，设置了最大浪费空间。 当剩余的空间小于最大浪费空间，那该TLAB属于的线程在重新向Eden区申请一个TLAB空间。进行对象创建，还是空间不够，那你这个对象太大了，去Eden区直接创建吧！ 当剩余的空间大于最大浪费空间，那这个大对象请你直接去Eden区创建，我TLAB放不下没有使用完的空间。 当然，又回造成新的病垢。 3，Eden空间够的时候，你再次申请TLAB没问题，我不够了，Heap的Eden区要开始GC， 4，TLAB允许浪费空间，导致Eden区空间不连续，积少成多。以后还要人帮忙打理。 补充: 上面说了对象可能分配到TLAB上,其实还可能分配到栈上,这涉及到逃逸分析以及标量替换(HotSpot中默认就开启这俩) 关于开启逃逸分析后,确认不会逃逸出方法的对象可以分配到栈上随着方法调用结束而结束,不用走JVM堆处理,可以看https://www.jianshu.com/p/4fd825568b9e https://blog.csdn.net/xiaomingdetianxia/article/details/77688945 栈上对象 java中我们要创建一个对象,用关键字new就可以了。但是，在我们日常中，有很多生命周期很短的对象。比如： 1234public void dome()&#123; User user=new user(); user.sayhi();&#125; 这种对象的作用域都不会逃逸出方法外，也就是说该对象的生命周期会随着方法的调用开始而开始，方法的调用结束而结束。 假设JVM所有的对象都放在堆内存中(为什么用假设，因为JVM并不是这样)一旦方法结束，没有了指向该对象的引用，该对象就需要被GC回收，如果存在很多这样的情况，对GC来说压力山大呀。 如果确定一个对象的作用域不会逃逸出方法之外，那可以将这个对象分配在栈上，这样，对象所占用的内存空间就可以随栈帧出栈而销毁。在一般应用中，不会逃逸的局部对象所占的比例很大，如果能使用栈上分配，那大量的对象就会随着方法的结束而自动销毁了，无须通过垃圾收集器回收，可以减小垃圾收集器的负载。 JVM允许将线程私有的对象打散分配在栈上，而不是分配在堆上。分配在栈上的好处是可以在函数调用结束后自行销毁，而不需要垃圾回收器的介入，从而提高系统性能。 栈上分配的技术基础： 一是**逃逸分析**：逃逸分析的目的是判断对象的作用域是否有可能逃逸出函数体。 二是**标量替换**：允许将对象打散分配在栈上，比如若一个对象拥有两个字段，会将这两个字段视作局部变量进行分配。 只能在server模式下才能启用逃逸分析，参数-XX:DoEscapeAnalysis启用逃逸分析，参数-XX:+EliminateAllocations开启标量替换（默认打开）。Java SE 6u23版本之后，HotSpot中默认就开启了逃逸分析，可以通过选项-XX:+PrintEscapeAnalysis查看逃逸分析的筛选结果。 十.JVM性能调优 磁盘不足排查 其实，磁盘不足排查算是系统、程序层面的问题排查，并不算是JVM，但是另一方面考虑过来就是，系统磁盘的不足，也会导致JVM的运行异常，所以也把磁盘不足算进来了。并且排查磁盘不足，是比较简单，就是几个命令，然后就是逐层的排查，首先第一个命令就是df -h，查询磁盘的状态： 从上面的显示中其中第一行使用的2.8G最大，然后是挂载在 / 目录下，我们直接cd /。然后通过执行： 1du -sh * 查看各个文件的大小，找到其中最大的，或者说存储量级差不多的并且都非常大的文件，把那些没用的大文件删除就好。 然后，就是直接cd到对应的目录也是执行：du -sh *，就这样一层一层的执行，找到对应的没用的，然后文件又比较大的，可以直接删除。 堆外内存排查与解决 Metaspace 属于堆外内存，但由于它是单独管理的，所以排查起来没什么难度。 你平常可能见到的使用堆外内存的场景还有下面这些： JNI 或者 JNA 程序，直接操纵了本地内存，比如一些加密库； 使用了Java 的 Unsafe 类，做了一些本地内存的操作； Netty 的直接内存（Direct Memory），底层会调用操作系统的 malloc 函数。 使用堆外内存可以调用一些功能完备的库函数，而且减轻了 GC 的压力。 这些代码，有可能是你了解的人写的，也有可能隐藏在第三方的 jar 包里。虽然有一些好处，但是问题排查起来通常会比较的困难。 MaxDirectMemorySize 控制直接内存的申请 其实，通过这个参数，仍然限制不住所有堆外内存的使用，它只是限制了使用 DirectByteBuffer 的内存申请。 很多时候（比如直接使用了 sun.misc.Unsafe 类），堆外内存会一直增长，直到机器物理内存爆满，被 oom killer。 123456789101112131415import sun.misc.Unsafe;import java.lang.reflect.Field;public class UnsafeDemo &#123; public static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception &#123; Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); Unsafe unsafe = (Unsafe) field.get(null); for (; ; ) &#123; unsafe.allocateMemory(_1MB); &#125; &#125; 上面这段代码，就会持续申请堆外内存，但它返回的是 long 类型的地址句柄，所以堆内内存的使用会很少。 我们使用下面的命令去限制堆内和直接内存的使用，结果发现程序占用的操作系统内存在一直上升，这两个参数在这种场景下没有任何效果。这段程序搞死了我的机器很多次，运行的时候要小心。 1java -XX:MaxDirectMemorySize=10M -Xmx10M UnsafeDemo 相信这种情况也困扰了你，因为使用一些 JDK 提供的工具，根本无法发现这部分内存的使用。 我们需要一些更加底层的工具来发现这些游离的内存分配。 其实，很多内存和性能问题，都逃不过下面要介绍的这些工具的联合分析。本课时将会结合一个实际的例子，来看一下一个堆外内存的溢出情况，了解常见的套路。 1. 现象 我们有一个服务，非常的奇怪，在某个版本之后，占用的内存开始增长，直到虚拟机分配的内存上限，但是并不会 OOM。 如果你开启了 SWAP，会发现这个应用也会毫不犹豫的将它吞掉，有多少吞多少。 说它的内存增长，是通过 top 命令去观察的，看它的 RES 列的数值；反之，如果使用 jmap 命令去看内存占用，得到的只是堆的大小，只能看到一小块可怜的空间。 使用 ps 也能看到相同的效果。我们观测到，除了虚拟内存比较高，达到了 17GB 以外，实际使用的内存 RSS 也夸张的达到了 7 GB，远远超过了 -Xmx 的设定。 12[root]$ ps -p 75 -o rss,vsz RSS VSZ 7152568 17485844 使用 jps 查看启动参数，发现分配了大约 3GB 的堆内存。实际内存使用超出了最大内存设定的一倍还多，这明显是不正常的，肯定是使用了堆外内存。 2. 模拟程序 为了能够使用这些工具实际观测这个内存泄漏的过程，我这里准备了一份小程序。 程序将会持续的使用 Java 的 Zip 函数进行压缩和解压，这种操作在一些对传输性能较高的的场景经常会用到。 程序将会申请 1kb 的随机字符串，然后持续解压。 为了避免让操作系统陷入假死状态，我们每次都会判断操作系统内存使用率，在达到 60% 的时候，我们将挂起程序；通过访问 8888 端口，将会把内存阈值提高到 85%。 我们将分析这两个处于相对静态的虚拟快照。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116import com.sun.management.OperatingSystemMXBean;import com.sun.net.httpserver.HttpContext;import com.sun.net.httpserver.HttpServer;import java.io.*;import java.lang.management.ManagementFactory;import java.net.InetSocketAddress;import java.util.Random;import java.util.concurrent.ThreadLocalRandom;import java.util.zip.GZIPInputStream;import java.util.zip.GZIPOutputStream;public class LeakExample &#123; /** * 构造随机的字符串 */ public static String randomString(int strLength) &#123; Random rnd = ThreadLocalRandom.current(); StringBuilder ret = new StringBuilder(); for (int i = 0; i &lt; strLength; i++) &#123; boolean isChar = (rnd.nextInt(2) % 2 == 0); if (isChar) &#123; int choice = rnd.nextInt(2) % 2 == 0 ? 65 : 97; ret.append((char) (choice + rnd.nextInt(26))); &#125; else &#123; ret.append(rnd.nextInt(10)); &#125; &#125; return ret.toString(); &#125; public static int copy(InputStream input, OutputStream output) throws IOException &#123; long count = copyLarge(input, output); return count &gt; 2147483647L ? -1 : (int) count; &#125; public static long copyLarge(InputStream input, OutputStream output) throws IOException &#123; byte[] buffer = new byte[4096]; long count = 0L; int n; for (; -1 != (n = input.read(buffer)); count += (long) n) &#123; output.write(buffer, 0, n); &#125; return count; &#125; public static String decompress(byte[] input) throws Exception &#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); copy(new GZIPInputStream(new ByteArrayInputStream(input)), out); return new String(out.toByteArray()); &#125; public static byte[] compress(String str) throws Exception &#123; ByteArrayOutputStream bos = new ByteArrayOutputStream(); GZIPOutputStream gzip = new GZIPOutputStream(bos); try &#123; gzip.write(str.getBytes()); gzip.finish(); byte[] b = bos.toByteArray(); return b; &#125;finally &#123; try &#123; gzip.close(); &#125;catch (Exception ex )&#123;&#125; try &#123; bos.close(); &#125;catch (Exception ex )&#123;&#125; &#125; &#125; private static OperatingSystemMXBean osmxb = (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean(); public static int memoryLoad() &#123; double totalvirtualMemory = osmxb.getTotalPhysicalMemorySize(); double freePhysicalMemorySize = osmxb.getFreePhysicalMemorySize(); double value = freePhysicalMemorySize / totalvirtualMemory; int percentMemoryLoad = (int) ((1 - value) * 100); return percentMemoryLoad; &#125; private static volatile int RADIO = 60; public static void main(String[] args) throws Exception &#123; HttpServer server = HttpServer.create(new InetSocketAddress(8888), 0); HttpContext context = server.createContext(&quot;/&quot;); context.setHandler(exchange -&gt; &#123; try &#123; RADIO = 85; String response = &quot;OK!&quot;; exchange.sendResponseHeaders(200, response.getBytes().length); OutputStream os = exchange.getResponseBody(); os.write(response.getBytes()); os.close(); &#125; catch (Exception ex) &#123; &#125; &#125;); server.start(); //1kb int BLOCK_SIZE = 1024; String str = randomString(BLOCK_SIZE / Byte.SIZE); byte[] bytes = compress(str); for (; ; ) &#123; int percent = memoryLoad(); if (percent &gt; RADIO) &#123; Thread.sleep(1000); &#125; else &#123; decompress(bytes); Thread.sleep(1); &#125;&#125; 程序将使用下面的命令行进行启动。为了简化问题，这里省略了一些无关的配置。 1java -Xmx1G -Xmn1G -XX:+AlwaysPreTouch -XX:MaxMetaspaceSize=10M -XX:MaxDirectMemorySize=10M -XX:NativeMemoryTracking=detail LeakExample 3. NMT 首先介绍一下上面的几个 JVM 参数，分别使用 Xmx、MaxMetaspaceSize、MaxDirectMemorySize 这三个参数限制了堆、元空间、直接内存的大小。 然后，使用 AlwaysPreTouch 参数： 作用 服务启动的时候真实的分配物理内存给jvm。默认情况下，通过参数指定了 JVM 大小，只有在 JVM 真正使用的时候，才会分配给它。 如果没有此参数，则jvm启动的时候，分配的只是虚拟内存，当真正使用的时候才会分配物理内存 如果没有此参数，则代码运行的时候，实时分配物理内存，导致代码运行速度变慢 如果有此参数，则jvm启动的时候速度会下降很多 在堆比较大的时候，会加大启动时间，但在这个场景中，我们为了减少内存动态分配的影响，把这个值设置为 True。 接下来的 NativeMemoryTracking，是用来追踪 Native 内存的使用情况。通过在启动参数上加入 -XX:NativeMemoryTracking=detail 就可以启用。使用 jcmd 命令，就可查看内存分配。 1jcmd $pid VM.native_memory summary 我们在一台 4GB 的虚拟机上使用上面的命令。启动程序之后，发现进程使用的内存迅速升到 2.4GB。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# jcmd 2154 VM.native_memory summary2154:Native Memory Tracking:Total: reserved=2370381KB, committed=1071413KB- Java Heap (reserved=1048576KB, committed=1048576KB) (mmap: reserved=1048576KB, committed=1048576KB)- Class (reserved=1056899KB, committed=4995KB) (classes #432) (malloc=131KB #328) (mmap: reserved=1056768KB, committed=4864KB)- Thread (reserved=10305KB, committed=10305KB) (thread #11) (stack: reserved=10260KB, committed=10260KB) (malloc=34KB #52) (arena=12KB #18)- Code (reserved=249744KB, committed=2680KB) (malloc=144KB #502) (mmap: reserved=249600KB, committed=2536KB)- GC (reserved=2063KB, committed=2063KB) (malloc=7KB #80) (mmap: reserved=2056KB, committed=2056KB)- Compiler (reserved=138KB, committed=138KB) (malloc=8KB #38) (arena=131KB #5)- Internal (reserved=789KB, committed=789KB) (malloc=757KB #1272) (mmap: reserved=32KB, committed=32KB)- Symbol (reserved=1535KB, committed=1535KB) (malloc=983KB #114) (arena=552KB #1)- Native Memory Tracking (reserved=159KB, committed=159KB) (malloc=99KB #1399) (tracking overhead=60KB)- Arena Chunk (reserved=174KB, committed=174KB) (mall 可惜的是，这个名字让人振奋的工具并不能如它描述的一样，看到我们这种泄漏的场景。下图这点小小的空间，是不能和 2GB 的内存占用相比的。 NMT 能看到堆内内存、Code 区域或者使用 unsafe.allocateMemory 和 DirectByteBuffer 申请的堆外内存，虽然是个好工具但问题并不能解决。 使用 jmap 工具，dump 一份堆快照，然后使用 MAT 分析，依然不能找到这部分内存。 4. pmap 像是 EhCache 这种缓存框架，提供了多种策略，可以设定将数据存储在非堆上，我们就是要排查这些影响因素。 如果能够在代码里看到这种可能性最大的代码块，是最好的。 为了进一步分析问题，我们使用 pmap 命令查看进程的内存分配，通过 RSS 升序序排列。 结果发现除了地址 00000000c0000000 上分配的 1GB 堆以外（也就是我们的堆内存），还有数量非常多的 64M 一块的内存段，还有巨量小的物理内存块映射到不同的虚拟内存段上。 但到现在为止，我们不知道里面的内容是什么，是通过什么产生的。 1234567891011121314151617181920212223242526272829303132333435363738# pmap -x 2154 | sort -n -k3Address Kbytes RSS Dirty Mode Mapping---------------- ------- ------- -------0000000100080000 1048064 0 0 ----- [ anon ]00007f2d4fff1000 60 0 0 ----- [ anon ]00007f2d537fb000 8212 0 0 ----- [ anon ]00007f2d57ff1000 60 0 0 ----- [ anon ].....省略N行00007f2e3c000000 65524 22064 22064 rw--- [ anon ]00007f2e00000000 65476 22068 22068 rw--- [ anon ]00007f2e18000000 65476 22072 22072 rw--- [ anon ]00007f2e30000000 65476 22076 22076 rw--- [ anon ]00007f2dc0000000 65520 22080 22080 rw--- [ anon ]00007f2dd8000000 65520 22080 22080 rw--- [ anon ]00007f2da8000000 65524 22088 22088 rw--- [ anon ]00007f2e8c000000 65528 22088 22088 rw--- [ anon ]00007f2e64000000 65520 22092 22092 rw--- [ anon ]00007f2e4c000000 65520 22096 22096 rw--- [ anon ]00007f2e7c000000 65520 22096 22096 rw--- [ anon ]00007f2ecc000000 65520 22980 22980 rw--- [ anon ]00007f2d84000000 65476 23368 23368 rw--- [ anon ]00007f2d9c000000 131060 43932 43932 rw--- [ anon ]00007f2d50000000 57324 56000 56000 rw--- [ anon ]00007f2d4c000000 65476 64160 64160 rw--- [ anon ]00007f2d5c000000 65476 64164 64164 rw--- [ anon ]00007f2d64000000 65476 64164 64164 rw--- [ anon ]00007f2d54000000 65476 64168 64168 rw--- [ anon ]00007f2d7c000000 65476 64168 64168 rw--- [ anon ]00007f2d60000000 65520 64172 64172 rw--- [ anon ]00007f2d6c000000 65476 64172 64172 rw--- [ anon ]00007f2d74000000 65476 64172 64172 rw--- [ anon ]00007f2d78000000 65520 64176 64176 rw--- [ anon ]00007f2d68000000 65520 64180 64180 rw--- [ anon ]00007f2d80000000 65520 64184 64184 rw--- [ anon ]00007f2d58000000 65520 64188 64188 rw--- [ anon ]00007f2d70000000 65520 64192 64192 rw--- [ anon ]00000000c0000000 1049088 1049088 1049088 rw--- [ anon ]total kB 8492740 3511008 3498584 通过 Google，找到以下资料 Linux glibc &gt;= 2.10 (RHEL 6) malloc may show excessive virtual memory usage) 。 文章指出造成应用程序大量申请 64M 大内存块的原因是由 Glibc 的一个版本升级引起的，通过 export MALLOC_ARENA_MAX=4 可以解决 VSZ 占用过高的问题。 虽然这也是一个问题，但却不是我们想要的，因为我们增长的是物理内存，而不是虚拟内存，程序在这一方面表现是正常的。 5. gdb 非常好奇 64M 或者其他小内存块中是什么内容，接下来可以通过 gdb 工具将其 dump 出来。 读取 /proc 目录下的 maps 文件，能精准地知晓目前进程的内存分布。 以下脚本通过传入进程 id，能够将所关联的内存全部 dump 到文件中。 注意，这个命令会影响服务，要慎用。 1pid=$1;grep rw-p /proc/$pid/maps | sed -n &#x27;s/^([0-9a-f]*)-([0-9a-f]*) .*$/1 2/p&#x27; | while read start stop; do gdb --batch --pid $pid -ex &quot;dump memory $1-$start-$stop.dump 0x$start 0x$stop&quot;; done 这个命令十分霸道，甚至把加载到内存中的 class 文件、堆文件一块给 dump 下来。这是机器的原始内存，大多数文件我们打不开。 更多时候，只需要 dump 一部分内存就可以。再次提醒操作会影响服务，注意 dump 的内存块大小，线上一定要慎用。 我们复制 pman 的一块 64M 内存，比如 00007f2d70000000，然后去掉前面的 0，使用下面代码得到内存块的开始和结束地址。 12cat /proc/2154/maps | grep 7f2d700000007f2d6fff1000-7f2d70000000 ---p 00000000 00:00 0 7f2d70000000-7f2d73ffc000 rw-p 00000000 00:00 0 接下来就 dump 这 64MB 的内存。 1gdb --batch --pid 2154 -ex &quot;dump memory a.dump 0x7f2d70000000 0x7f2d73ffc000&quot; 使用 du 命令查看具体的内存块大小，不多不少正好 64M。 12# du -h a.dump64M a.dump 是时候查看里面的内容了，使用 strings 命令可以看到内存块里一些可以打印的内容。 1234# strings -10 a.dump0R4f1Qej1ty5GT8V1R8no6T44564wz499E6Y582q2R9h8CC175GJ3yeJ1Q3P5Vt757Mcf6378kM36hxZ5U8uhg2A26T5l7f68719WQK6vZ2BOdH9lH5C7838qf1... 等等？这些内容不应该在堆里面么？为何还会使用额外的内存进行分配？那么还有什么地方在分配堆外内存呢？ 这种情况，只可能是 native 程序对堆外内存的操作。 6. perf 下面介绍一个神器 perf，除了能够进行一些性能分析，它还能帮助我们找到相应的 native 调用。这么突出的堆外内存使用问题，肯定能找到相应的调用函数。 使用 perf record -g -p 2154 开启监控栈函数调用，然后访问服务器的 8888 端口，这将会把内存使用的阈值增加到 85%，我们的程序会逐渐把这部分内存占满，你可以手工观察这个过程。 perf 运行一段时间后 Ctrl+C 结束，会生成一个文件 perf.data。 执行 perf report -i perf.data 查看报告。 如图，一般第三方 JNI 程序，或者 JDK 内的模块，都会调用相应的本地函数，在 Linux 上，这些函数库的后缀都是 so。 我们依次浏览用的可疑资源，发现了“libzip.so”，还发现了不少相关的调用。搜索 zip（输入 / 进入搜索模式），结果如下： 查看 JDK 代码，发现 bzip 大量使用了 native 方法。也就是说，有大量内存的申请和销毁，是在堆外发生的。 进程调用了Java_java_util_zip_Inflater_inflatBytes() 申请了内存，却没有调用 Deflater 释放内存。与 pmap 内存地址相比对，确实是 zip 在搞鬼。 7. gperftools google 还有一个类似的、非常好用的工具，叫做 gperftools，我们主要用到它的 Heap Profiler，功能更加强大。 它的启动方式有点特别，安装成功之后，你只需要输出两个环境变量即可。 mkdir -p /opt/test export LD_PRELOAD=/usr/lib64/libtcmalloc.so export HEAPPROFILE=/opt/test/heap 在同一个终端，再次启动我们的应用程序，可以看到内存申请动作都被记录到了 opt 目录下的 test 目录。 接下来，我们就可以使用 pprof 命令分析这些文件。 12cd /opt/testpprof -text *heap | head -n 200 使用这个工具，能够一眼追踪到申请内存最多的函数。Java_java_util_zip_Inflater_init 这个函数立马就被发现了。 123456789101112131415161718Total: 25205.3 MB 20559.2 81.6% 81.6% 20559.2 81.6% inflateBackEnd 4487.3 17.8% 99.4% 4487.3 17.8% inflateInit2_ 75.7 0.3% 99.7% 75.7 0.3% os::malloc@8bbaa0 70.3 0.3% 99.9% 4557.6 18.1% Java_java_util_zip_Inflater_init 7.1 0.0% 100.0% 7.1 0.0% readCEN 3.9 0.0% 100.0% 3.9 0.0% init 1.1 0.0% 100.0% 1.1 0.0% os::malloc@8bb8d0 0.2 0.0% 100.0% 0.2 0.0% _dl_new_object 0.1 0.0% 100.0% 0.1 0.0% __GI__dl_allocate_tls 0.1 0.0% 100.0% 0.1 0.0% _nl_intern_locale_data 0.0 0.0% 100.0% 0.0 0.0% _dl_check_map_versions 0.0 0.0% 100.0% 0.0 0.0% __GI___strdup 0.0 0.0% 100.0% 0.1 0.0% _dl_map_object_deps 0.0 0.0% 100.0% 0.0 0.0% nss_parse_service_list 0.0 0.0% 100.0% 0.0 0.0% __new_exitfn 0.0 0.0% 100.0% 0.0 0.0% getpwuid 0.0 0.0% 100.0% 0.0 0.0% expand_dynamic_string_token 8. 解决 这就是我们模拟内存泄漏的整个过程，到此问题就解决了。 GZIPInputStream 使用 Inflater 申请堆外内存、Deflater 释放内存，调用 close() 方法来主动释放。 如果忘记关闭，Inflater 对象的生命会延续到下一次 GC，有一点类似堆内的弱引用。在此过程中，堆外内存会一直增长。 把 decompress 函数改成如下代码，重新编译代码后观察，问题解决。 1234567891011public static String decompress(byte[] input) throws Exception &#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); GZIPInputStream gzip = new GZIPInputStream(new ByteArrayInputStream(input)); try &#123; copy(gzip, out); return new String(out.toByteArray()); &#125;finally &#123; try&#123; gzip.close(); &#125;catch (Exception ex)&#123;&#125; try&#123; out.close(); &#125;catch (Exception ex)&#123;&#125; &#125; &#125; 9. 小结 本课时使用了非常多的工具和命令来进行堆外内存的排查，可以看到，除了使用 jmap 获取堆内内存，还对堆外内存的获取也有不少办法。 现在，我们可以把堆外内存进行更加细致地划分了。 元空间属于堆外内存，主要是方法区和常量池的存储之地，使用数“MaxMetaspaceSize”可以限制它的大小，我们也能观测到它的使用。 直接内存主要是通过 DirectByteBuffer 申请的内存，可以使用参数“MaxDirectMemorySize”来限制它的大小（参考第 10 课时）。 其他堆外内存，主要是指使用了 Unsafe 或者其他 JNI 手段直接直接申请的内存。这种情况，就没有任何参数能够阻挡它们，要么靠它自己去释放一些内存，要么等待操作系统对它的审判了。 还有一种情况，和内存的使用无关，但是也会造成内存不正常使用，那就是使用了 Process 接口，直接调用了外部的应用程序，这些程序对操作系统的内存使用一般是不可预知的。 本课时介绍的一些工具，很多高级研发，包括一些面试官，也是不知道的；即使了解这个过程，不实际操作一遍，也很难有深刻的印象。通过这个例子，你可以看到一个典型的堆外内存问题的排查思路。 堆外内存的泄漏是非常严重的，它的排查难度高、影响大，甚至会造成宿主机的死亡。在排查内存问题时，不要忘了这一环。 CPU过高排查 排查过程 使用top查找进程id 使用top -Hp &lt;pid&gt;查找进程中耗cpu比较高的线程id 使用printf %x &lt;pid&gt;将线程id十进制转十六进制 使用 jstack -pid | grep -A 20 &lt;pid&gt;过滤出线程id锁关联的栈信息 根据栈信息中的调用链定位业务代码 案例代码如下： 1234567891011121314151617181920212223242526public class CPUSoaring &#123; public static void main(String[] args) &#123; Thread thread1 = new Thread(new Runnable()&#123; @Override public void run() &#123; for (;;)&#123; System.out.println(&quot;I am children-thread1&quot;); &#125; &#125; &#125;,&quot;children-thread1&quot;); Thread thread2 = new Thread(new Runnable()&#123; @Override public void run() &#123; for (;;)&#123; System.out.println(&quot;I am children-thread2&quot;); &#125; &#125; &#125;,&quot;children-thread2&quot;); thread1.start(); thread2.start(); System.err.println(&quot;I am is main thread!!!!!!!!&quot;); &#125;&#125; 第一步：首先通过top命令可以查看到id为3806的进程所占的CPU最高： 第二步：然后通过top -Hp pid命令，找到占用CPU最高的线程： 第三步：接着通过：printf ‘%x\\n’ tid命令将线程的tid转换为十六进制：xid： 第四步：最后通过：jstack pid|grep xid -A 30命令就是输出线程的堆栈信息，线程所在的位置： 第五步：还可以通过jstack -l pid &gt; 文件名称.txt 命令将线程堆栈信息输出到文件，线下查看。 这就是一个CPU飙高的排查过程，目的就是要找到占用CPU最高的线程所在的位置，然后就是review你的代码，定位到问题的所在。使用Arthas的工具排查也是一样的，首先要使用top命令找到占用CPU最高的Java进程，然后使用Arthas进入该进程内，使用dashboard命令排查占用CPU最高的线程。，最后通过thread命令线程的信息。 OOM异常排查 OOM的异常排查也比较简单，首先服务上线的时候，要先设置这两个参数： 1-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;目录&#125; 指定项目出现OOM异常的时候自动导出堆转储文件，然后通过内存分析工具（Visual VM）来进行线下的分析。 首先我们来聊一聊，哪些原因会导致OOM异常，站在JVM的分区的角度： Java堆 方法区 虚拟机栈 本地方法栈 程序计数器 直接内存 只有程序计数器区域不会出现OOM，在Java 8及以上的元空间（本地内存）都会出现OOM。 而站在程序代码的角度来看，总结了大概有以下几点原因会导致OOM异常： 内存泄露 对象过大、过多 方法过长 过度使用代理框架，生成大量的类信息 接下来我们屋来看看OOM的排查，出现OOM异常后dump出了堆转储文件，然后打开jdk自带的Visual VM工具，导入堆转储文件，首先我使用的OOM异常代码如下： 12345678910111213141516171819202122232425import java.util.ArrayList;import java.util.List;class OOM &#123; static class User&#123; private String name; private int age; public User(String name, int age)&#123; this.name = name; this.age = age; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; List&lt;User&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; Thread.sleep(1000); User user = new User(&quot;zhangsan&quot;+i,i); list.add(user); &#125; &#125;&#125; 代码很简单，就是往集合里面不断地add对象，带入堆转储文件后，在类和实例那栏就可以看到实例最多的类： 这样就找到导致OOM异常的类，还可以通过下面的方法查看导致OOM异常的线程堆栈信息，找到对应异常的代码段。 上面的方法是排查已经出现了OOM异常的方法，肯定是防线的最后一步，那么在此之前怎么防止出现OOM异常呢？ 一般大厂都会有自己的监控平台，能够实施的监控测试环境、预览环境、线上实施的服务健康状况（CPU、内存） 等信息，对于频繁GC，并且GC后内存的回收率很差的，就要引起我们的注意了。 因为一般方法的长度合理，95%以上的对象都是朝生夕死，在Minor GC后只剩少量的存活对象，所以在代码层面上应该避免方法过长、大对象的现象。 每次自己写完代码，自己检查后，都可以提交给比自己高级别的工程师review自己的代码，就能及时的发现代码的问题，基本上代码没问题，百分之九十以上的问题都能避免，这也是大厂注重代码质量，并且时刻review代码的习惯。 Jvisualvm 项目频繁YGC 、FGC问题排查 内存问题 对象内存占用、实例个数监控 对象内存占用、年龄值监控 通过上面两张图发现这些对象占用内存比较大而且存活时间也是比较常，所以survivor 中的空间被这些对象占用，而如果缓存再次刷新则会创建同样大小对象来替换老数据，这时发现eden内存空间不足，就会触发yonggc 如果yonggc 结束后发现eden空间还是不够则会直接放到老年代，所以这样就产生了大对象的提前晋升，导致fgc增加…… 优化办法：优化两个缓存对象，将缓存对象大小减小。优化一下两个对象，缓存关键信息！ CPU耗时问题排查 Cpu使用耗时监控： 耗时、调用次数监控： 从上面监控图可以看到主要耗时还是在网络请求，没有看到具体业务代码消耗过错cpu…… 调优参数 堆 -Xms1024m 设置堆的初始大小 -Xmx1024m 设置堆的最大大小 -XX:NewSize=1024m 设置年轻代的初始大小 -XX:MaxNewSize=1024m 设置年轻代的最大值 -XX:SurvivorRatio=8 Eden和S区的比例 -XX:NewRatio=4 设置老年代和新生代的比例 -XX:MaxTenuringThreshold=10 设置晋升老年代的年龄条件 栈 -Xss128k 元空间 -XX:MetasapceSize=200m 设置初始元空间大小 -XX:MaxMatespaceSize=200m 设置最大元空间大小 默认无限制 直接内存 -XX:MaxDirectMemorySize 设置直接内存的容量，默认与堆最大值一样 内存收缩 -XX:MaxHeapFreeRatio 空闲内存达到多少比值时，开始收缩JVM进程占用的内存空间。默认应该是70%，若当java进程占用内存过大，该值应该适当调小。 -XX:MinHeapFreeRatio 空闲内存最少保留的比例值。我这里设置的是8%。此参数意思是无论如何进程内存会保留一定比例的空闲内存，进程内存收缩过程中，剩余内存达到这个阈值后，就会停止收缩。 日志 -Xloggc:/opt/app/ard-user/ard-user-gc-%t.log 设置日志目录和日志名称 -XX:+UseGCLogFileRotation 开启滚动生成日志 -XX:NumberOfGCLogFiles=5 滚动GC日志文件数，默认0，不滚动 -XX:GCLogFileSize=20M GC文件滚动大小，需开 UseGCLogFileRotation -XX:+PrintGCDetails 开启记录GC日志详细信息（包括GC类型、各个操作使用的时间）,并且在程序运行结束打印出JVM的内存占用情况 -XX:+ PrintGCDateStamps 记录系统的GC时间 -XX:+PrintGCCause 产生GC的原因(默认开启) GC Serial垃圾收集器（新生代） 开启 -XX:+UseSerialGC 关闭： -XX:-UseSerialGC //新生代使用Serial 老年代则使用SerialOld Parallel Scavenge收集器（新生代）开启 -XX:+UseParallelOldGC 关闭 -XX:-UseParallelOldGC 新生代使用功能Parallel Scavenge 老年代将会使用Parallel Old收集器 ParallelOl垃圾收集器（老年代）开启 -XX:+UseParallelGC 关闭 -XX:-UseParallelGC 新生代使用功能Parallel Scavenge 老年代将会使用Parallel Old收集器 ParNew垃圾收集器（新生代）开启 -XX:+UseParNewGC 关闭 -XX:-UseParNewGC //新生代使用功能ParNew 老年代则使用功能CMS CMS垃圾收集器（老年代）开启 -XX:+UseConcMarkSweepGC 关闭 -XX:-UseConcMarkSweepGC -XX:MaxGCPauseMillis GC停顿时间，垃圾收集器会尝试用各种手段达到这个时间，比如减小年轻代 -XX:+UseCMSCompactAtFullCollection 用于在CMS收集器不得不进行FullGC时开启内存碎片的合并整理过程，由于这个内存整理必须移动存活对象，（在Shenandoah和ZGC出现前）是无法并发的 -XX：CMSFullGCsBefore-Compaction 多少次FullGC之后压缩一次，默认值为0，表示每次进入FullGC时都进行碎片整理） -XX:CMSInitiatingOccupancyFraction 当老年代使用达到该比例时会触发FullGC，默认是92 -XX:+UseCMSInitiatingOccupancyOnly 这个参数搭配上面那个用，表示是不是要一直使用上面的比例触发FullGC，如果设置则只会在第一次FullGC的时候使用-XX:CMSInitiatingOccupancyFraction的值，之后会进行自动调整 -XX:+CMSScavengeBeforeRemark 在FullGC前启动一次MinorGC，目的在于减少老年代对年轻代的引用，降低CMSGC的标记阶段时的开销，一般CMS的GC耗时80%都在标记阶段 -XX:+CMSParallellnitialMarkEnabled 默认情况下初始标记是单线程的，这个参数可以让他多线程执行，可以减少STW -XX:+CMSParallelRemarkEnabled 使用多线程进行重新标记，目的也是为了减少STW G1垃圾收集器开启 -XX:+UseG1GC 关闭 -XX:-UseG1GC -XX：G1HeapRegionSize 设置每个Region的大小，取值范围为1MB～32MB -XX：MaxGCPauseMillis 设置垃圾收集器的停顿时间，默认值是200毫秒，通常把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的 堆 gc频次正常 时间正常。 出现内存泄漏是什么问题？","categories":[{"name":"jvm","slug":"jvm","permalink":"http://cloud-tour.github.io/categories/jvm/"}],"tags":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://cloud-tour.github.io/tags/jvm/"}]},{"title":"juc","slug":"juc","date":"2023-02-17T13:02:02.380Z","updated":"2023-02-17T13:11:55.182Z","comments":true,"path":"2023/02/17/juc/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/juc/","excerpt":"","text":"1.进程和线程 进程 进程就是用来加载指令、管理内存、管理 IO 的 当一个程序被运行，从磁盘加载这个程序的代码至内存，这时就开启了一个进程。 线程 一个进程之内可以分为一到多个线程。 一个线程就是一个指令流，将指令流中的一条条指令以一定的顺序交给 CPU 执行 Java 中，线程作为最小调度单位，进程作为资源分配的最小单位。 在 windows 中进程是不活动的，只是作为线程的容器 区别 进程基本上相互独立的，而线程存在于进程内，是进程的一个子集 进程拥有共享的资源，如内存空间等，供其内部的线程共享 进程间通信较为复杂 同一台计算机的进程通信称为 IPC（Inter-process communication） 不同计算机之间的进程通信，需要通过网络，并遵守共同的协议，例如 HTTP 线程通信相对简单，因为它们共享进程内的内存，一个例子是多个线程可以访问同一个共享变量 线程更轻量，线程上下文切换成本一般上要比进程上下文切换低 其他区别 1、因为进程拥有独立的堆栈空间和数据段，所以每当启动一个新的进程必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这对于多进程来说十分“奢侈”，系统开销比较大，而线程不一样，线程拥有独立的堆栈空间，但是共享数据段，它们彼此之间使用相同的地址空间，共享大部分数据，比进程更节俭，开销比较小，切换速度也比进程快，效率高，但是正由于进程之间独立的特点，使得进程安全性比较高，也因为进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。一个线程死掉就等于整个进程死掉。 2、体现在通信机制上面，正因为进程之间互不干扰，相互独立，进程的通信机制相对很复杂，譬如管道，信号，消息队列，共享内存，套接字等通信机制，而线程由于共享数据段所以通信机制很方便。。 3、体现在CPU系统上面，线程使得CPU系统更加有效，因为操作系统会保证当线程数不大于CPU数目时，不同的线程运行于不同的CPU上。 4、体现在程序结构上，举一个简明易懂的列子：当我们使用进程的时候，我们不自主的使用if else嵌套来判断pid，使得程序结构繁琐，但是当我们使用线程的时候，基本上可以甩掉它，当然程序内部执行功能单元需要使用的时候还是要使用，所以线程对程序结构的改善有很大帮助。 2.synchronized原理 java对象头 普通对象 数组对象 64 位虚拟机 Mark Word Monitor Monitor 被翻译为监视器或管程 每个 Java 对象都可以关联一个 Monitor 对象，如果使用 synchronized 给对象上锁（重量级）之后，该对象头的 Mark Word 中就被设置指向 Monitor 对象的指针 Monitor 结构如下 刚开始 Monitor 中 Owner 为 null 当 Thread-2 执行 synchronized(obj) 就会将 Monitor 的所有者 Owner 置为 Thread-2，Monitor中只能有一 个 Owner 在 Thread-2 上锁的过程中，如果 Thread-3，Thread-4，Thread-5 也来执行 synchronized(obj)，就会进入 EntryList BLOCKED Thread-2 执行完同步代码块的内容，然后唤醒 EntryList 中等待的线程来竞争锁，竞争的时是非公平的 图中 WaitSet 中的 Thread-0，Thread-1 是之前获得过锁，但条件不满足进入 WAITING 状态的线程，后面讲 wait-notify 时会分析 注意： synchronized 必须是进入同一个对象的 monitor 才有上述的效果 不加 synchronized 的对象不会关联监视器，不遵从以上规则 轻量级锁 轻量级锁的使用场景：如果一个对象虽然有多线程要加锁，但加锁的时间是错开的（也就是没有竞争），那么可以 使用轻量级锁来优化。 轻量级锁对使用者是透明的，即语法仍然是 synchronized 假设有两个方法同步块，利用同一个对象加锁 123456789101112static final Object obj = new Object();public static void method1() &#123; synchronized( obj ) &#123; // 同步块 A method2(); &#125;&#125;public static void method2() &#123; synchronized( obj ) &#123; // 同步块 B &#125;&#125; 创建锁记录（Lock Record）对象，每个线程都的栈帧都会包含一个锁记录的结构，内部可以存储锁定对象的 Mark Word 让锁记录中 Object reference 指向锁对象，并尝试用 cas 替换 Object 的 Mark Word，将 Mark Word 的值存 入锁记录 如果 cas 替换成功，对象头中存储了 锁记录地址和状态 00 ，表示由该线程给对象加锁，这时图示如下 如果 cas 失败，有两种情况 如果是其它线程已经持有了该 Object 的轻量级锁，这时表明有竞争，进入锁膨胀过程 如果是自己执行了 synchronized 锁重入，那么再添加一条 Lock Record 作为重入的计数 当退出 synchronized 代码块（解锁时）如果有取值为 null 的锁记录，表示有重入，这时重置锁记录，表示重 入计数减一 当退出 synchronized 代码块（解锁时）锁记录的值不为 null，这时使用 cas 将 Mark Word 的值恢复给对象头 成功，则解锁成功 失败，说明轻量级锁进行了锁膨胀或已经升级为重量级锁，进入重量级锁解锁流程 锁膨胀 如果在尝试加轻量级锁的过程中，CAS 操作无法成功，这时一种情况就是有其它线程为此对象加上了轻量级锁（有 竞争），这时需要进行锁膨胀，将轻量级锁变为重量级锁。 当 Thread-1 进行轻量级加锁时，Thread-0 已经对该对象加了轻量级锁 这时 Thread-1 加轻量级锁失败，进入锁膨胀流程 即为 Object 对象申请 Monitor 锁，让 Object 指向重量级锁地址 然后自己进入 Monitor 的 EntryList BLOCKED 当 Thread-0 退出同步块解锁时，使用 cas 将 Mark Word 的值恢复给对象头，失败。这时会进入重量级解锁 流程，即按照 Monitor 地址找到 Monitor 对象，设置 Owner 为 null，唤醒 EntryList 中 BLOCKED 线程 自旋优化 重量级锁竞争的时候，还可以使用自旋来进行优化，如果当前线程自旋成功（即这时候持锁线程已经退出了同步 块，释放了锁），这时当前线程就可以避免阻塞。 自旋重试成功的情况 自旋重试失败的情况 自旋会占用 CPU 时间，单核 CPU 自旋就是浪费，多核 CPU 自旋才能发挥优势。 在 Java 6 之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会 高，就多自旋几次；反之，就少自旋甚至不自旋，总之，比较智能。 Java 7 之后不能控制是否开启自旋功能 偏向锁 轻量级锁在没有竞争时（就自己这个线程），每次重入仍然需要执行 CAS 操作。 Java 6 中引入了偏向锁来做进一步优化：只有第一次使用 CAS 将线程 ID 设置到对象的 Mark Word 头，之后发现 这个线程 ID 是自己的就表示没有竞争，不用重新 CAS。以后只要不发生竞争，这个对象就归该线程所有 偏向状态 对象头格式 一个对象创建时： 如果开启了偏向锁（默认开启），那么对象创建后，markword 值为 0x05 即最后 3 位为 101，这时它的 thread、epoch、age 都为 0 偏向锁是默认是延迟的，不会在程序启动时立即生效，如果想避免延迟，可以加 VM 参数 - XX:BiasedLockingStartupDelay=0 来禁用延迟 如果没有开启偏向锁，那么对象创建后，markword 值为 0x01 即最后 3 位为 001，这时它的 hashcode、 age 都为 0，第一次用到 hashcode 时才会赋值 3.wait、notify原理 Owner 线程发现条件不满足，调用 wait 方法，即可进入 WaitSet 变为 WAITING 状态 BLOCKED 和 WAITING 的线程都处于阻塞状态，不占用 CPU 时间片 BLOCKED 线程会在 Owner 线程释放锁时唤醒 WAITING 线程会在 Owner 线程调用 notify 或 notifyAll 时唤醒，但唤醒后并不意味者立刻获得锁，仍需进入 EntryList 重新竞争 4.park unpark 原理 每个线程都有自己的一个 Parker 对象，由三部分组成 _counter ， _cond 和 _mutex 打个比喻 _ 线程就像一个旅人，Parker 就像他随身携带的背包，条件变量就好比背包中的帐篷。_counter 就好比背包中 的备用干粮（0 为耗尽，1 为充足） 调用 park 就是要看需不需要停下来歇息 如果备用干粮耗尽，那么钻进帐篷歇息 如果备用干粮充足，那么不需停留，继续前进 调用 unpark，就好比令干粮充足 如果这时线程还在帐篷，就唤醒让他继续前进 如果这时线程还在运行，那么下次他调用 park 时，仅是消耗掉备用干粮，不需停留继续前进 因为背包空间有限，多次调用 unpark 仅会补充一份备用干粮 5.ReentrantLock 相对于 synchronized 它具备如下特点 可中断 可以设置超时时间 可以设置为公平锁 支持多个条件变量 与 synchronized 一样，都支持可重入 基本用法： 12345678// 获取锁reentrantLock.lock();try &#123; // 临界区&#125; finally &#123; // 释放锁 reentrantLock.unlock();&#125; 条件变量 synchronized 中也有条件变量，就是我们讲原理时那个 waitSet 休息室，当条件不满足时进入 waitSet 等待 ReentrantLock 的条件变量比 synchronized 强大之处在于，它是支持多个条件变量的，这就好比 synchronized 是那些不满足条件的线程都在一间休息室等消息 而 ReentrantLock 支持多间休息室，有专门等烟的休息室、专门等早餐的休息室、唤醒时也是按休息室来唤醒 使用要点： await 前需要获得锁 await 执行后，会释放锁，进入 conditionObject 等待 await 的线程被唤醒（或打断、或超时）取重新竞争 lock 锁 竞争 lock 锁成功后，从 await 后继续执行 非公平锁实现原理 先从构造器开始看，默认为非公平锁实现 123public ReentrantLock() &#123; sync = new NonfairSync();&#125; NonfairSync 继承自 AQS 没有竞争时 第一个竞争出现时 Thread-1 执行了 CAS 尝试将 state 由 0 改为 1，结果失败 进入 tryAcquire 逻辑，这时 state 已经是1，结果仍然失败 接下来进入 addWaiter 逻辑，构造 Node 队列 图中黄色三角表示该 Node 的 waitStatus 状态，其中 0 为默认正常状态 Node 的创建是懒惰的 其中第一个 Node 称为 Dummy（哑元）或哨兵，用来占位，并不关联线程 当前线程进入 acquireQueued 逻辑 acquireQueued 会在一个死循环中不断尝试获得锁，失败后进入 park 阻塞 如果自己是紧邻着 head（排第二位），那么再次 tryAcquire 尝试获取锁，当然这时 state 仍为 1，失败 进入 shouldParkAfterFailedAcquire 逻辑，将前驱 node，即 head 的 waitStatus 改为 -1，这次返回 false shouldParkAfterFailedAcquire 执行完毕回到 acquireQueued ，再次 tryAcquire 尝试获取锁，当然这时 state 仍为 1，失败 当再次进入 shouldParkAfterFailedAcquire 时，这时因为其前驱 node 的 waitStatus 已经是 -1，这次返回 true 进入 parkAndCheckInterrupt， Thread-1 park（灰色表示） 再次有多个线程经历上述过程竞争失败，变成这个样子 Thread-0 释放锁，进入 tryRelease 流程，如果成功 设置 exclusiveOwnerThread 为 null state = 0 当前队列不为 null，并且 head 的 waitStatus = -1，进入 unparkSuccessor 流程 找到队列中离 head 最近的一个 Node（没取消的），unpark 恢复其运行，本例中即为 Thread-1 回到 Thread-1 的 acquireQueued 流程 如果加锁成功（没有竞争），会设置 exclusiveOwnerThread 为 Thread-1，state = 1 head 指向刚刚 Thread-1 所在的 Node，该 Node 清空 Thread 原本的 head 因为从链表断开，而可被垃圾回收 如果这时候有其它线程来竞争（非公平的体现），例如这时有 Thread-4 来了 如果不巧又被 Thread-4 占了先 Thread-4 被设置为 exclusiveOwnerThread，state = 1 Thread-1 再次进入 acquireQueued 流程，获取锁失败，重新进入 park 阻塞 可重入原理 条件变量实现原理 每个条件变量其实就对应着一个等待队列，其实现类是 ConditionObject await 流程 开始 Thread-0 持有锁，调用 await，进入 ConditionObject 的 addConditionWaiter 流程 创建新的 Node 状态为 -2（Node.CONDITION），关联 Thread-0，加入等待队列尾部 接下来进入 AQS 的 fullyRelease 流程，释放同步器上的锁 unpark AQS 队列中的下一个节点，竞争锁，假设没有其他竞争线程，那么 Thread-1 竞争成功 park 阻塞 Thread-0 signal 流程 假设 Thread-1 要来唤醒 Thread-0 进入 ConditionObject 的 doSignal 流程，取得等待队列中第一个 Node，即 Thread-0 所在 Node 执行 transferForSignal 流程，将该 Node 加入 AQS 队列尾部，将 Thread-0 的 waitStatus 改为 0，Thread-3 的 waitStatus 改为 -1 Thread-1 释放锁，进入 unlock 流程，略 6.volatile原理 volatile 的底层实现原理是内存屏障，Memory Barrier（Memory Fence） 对 volatile 变量的写指令后会加入写屏障 对 volatile 变量的读指令前会加入读屏障 保证可见性 写屏障（sfence）保证在该屏障之前的，对共享变量的改动，都同步到主存当中 public void actor2(I_Result r) &#123; num = 2; ready = true; // ready 是 volatile 赋值带写屏障 // 写屏障 &#125; &lt;!--code￼3--&gt; 保证有序性 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后 public void actor2(I_Result r) &#123; num = 2; ready = true; // ready 是 volatile 赋值带写屏障 // 写屏障 &#125; &lt;!--code￼4--&gt; 还是那句话，不能解决指令交错(字节码层面)： 写屏障仅仅是保证之后的读能够读到最新的结果，但不能保证读跑到它前面去 而有序性的保证也只是保证了本线程内相关代码不被重排序 7.线程池 线程池（thread pool）：一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，对线程统一管理。 线程池就是存放线程的池子，池子里存放了很多可以复用的线程。 创建线程和销毁线程的花销是比较大的（手动new Thread 类），创建和消耗线程的时间有可能比处理业务的时间还要长。这样频繁的创建线程和销毁线程是比较消耗资源的。（我们可以把创建和销毁的线程的过程去掉）。 线程池状态 ThreadPoolExecutor 使用 int 的高 3 位来表示线程池状态，低 29 位表示线程数量 从数字上比较，TERMINATED &gt; TIDYING &gt; STOP &gt; SHUTDOWN &gt; RUNNING 这些信息存储在一个原子变量 ctl 中，目的是将线程池状态与线程个数合二为一，这样就可以用一次 cas 原子操作 进行赋值 running：初始化后的状态，表示线程池可以处理任务。 shutdown：调用线程池的shutdown方法会使线程进入shutdown状态，从而调用execute的时候会抛出异常。但如果阻塞队列中还有任务，则会先将阻塞队列中的认为执行完，才会后收所有线程。 stop：调用线程池的shutdownnow方法会使线程进入stop状态，既不能接受新的任务，也不能把阻塞队列中的任务执行完。 tidying：在执行玩shutdownnow方法的时候，关闭完所有线程的时候，就会调用tryTerminate（）方法 terminated：线程池处于TIDYING状态后，会执行terminated（）方法，执行完后就i进入terminated状态，在ThreadPoolExecutor中的terminated（）是一个空方法，可以自定义线程池重写这个方法，实现自定义的业务逻辑。 工作流程 提交任务 当工作线程数小于核心线程数时，直接创建新的核心工作线程 当工作线程数不小于核心线程数时，就需要尝试将任务添加到阻塞队列中去 如果能够加入成功，说明队列还没有满，那么需要做以下的二次验证来保证添加进去的任务能够成功被执行 验证当前线程池的运行状态，如果是非RUNNING状态，则需要将任务从阻塞队列中移除，然后拒绝该任务 验证当前线程池中的工作线程的个数，如果为0，则需要主动添加一个空工作线程来执行刚刚添加到阻塞队列中的任务 如果加入失败，则说明队列已经满了，那么这时就需要创建新的“临时”工作线程来执行任务 如果创建成功，则直接执行该任务 如果创建失败，则说明工作线程数已经等于最大线程数了，则只能拒绝该任务了 构造方法 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize 核心线程数目 (最多保留的线程数) CPU密集型：corePoolSize = CPU核数 + 1 IO密集型：corePoolSize = CPU核数 * 2 maximumPoolSize 最大线程数目 keepAliveTime 生存时间 - 针对救急线程 unit 时间单位 - 针对救急线程 workQueue 阻塞队列 threadFactory 线程工厂 - 可以为线程创建时起个好名字 handler 拒绝策略（可通过实现RejectedExecutionHandler 接口自定义拒绝策略） 工作队列 1、无界队列 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列作为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而博主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 当然这种队列，maximumPoolSize 的值也就无效了。当每个任务完全独立于其他任务，即任务执行互不影响时，适合于使用无界队列；例如，在 Web 页服务器中。这种排队可用于处理瞬态突发请求，当命令以超过队列所能处理的平均数连续到达时，此策略允许无界线程具有增长的可能性。 2、有界队列 当使用有限的 maximumPoolSizes 时，有界队列有助于防止资源耗尽，但是可能较难调整和控制。常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。 使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 3、同步移交队列 如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 newFixedThreadPool 123456public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 特点 核心线程数 == 最大线程数（没有救急线程被创建），因此也无需超时时间 阻塞队列是无界的，可以放任意数量的任务 评价 适用于任务量已知，相对耗时的任务 newCachedThreadPool 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 特点 核心线程数是 0， 最大线程数是 Integer.MAX_VALUE，救急线程的空闲生存时间是 60s，意味着 全部都是救急线程（60s 后可以回收） 救急线程可以无限创建 队列采用了 SynchronousQueue 实现特点是，它没有容量，没有线程来取是放不进去的（一手交钱、一手交 货） 评价 整个线程池表现为线程数会根据任务量不断增长，没有上限，当任务执行完毕，空闲 1分钟后释放线 程。 适合任务数比较密集，但每个任务执行时间较短的情况 newSingleThreadExecutor 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 使用场景： 希望多个任务排队执行。线程数固定为 1，任务数多于 1 时，会放入无界队列排队。任务执行完毕，这唯一的线程 也不会被释放。 区别： 自己创建一个单线程串行执行任务，如果任务执行失败而终止那么没有任何补救措施，而线程池还会新建一 个线程，保证池的正常工作 Executors.newSingleThreadExecutor() 线程个数始终为1，不能修改 FinalizableDelegatedExecutorService 应用的是装饰器模式，只对外暴露了 ExecutorService 接口，因 此不能调用 ThreadPoolExecutor 中特有的方法 Executors.newFixedThreadPool(1) 初始时为1，以后还可以修改 对外暴露的是 ThreadPoolExecutor 对象，可以强转后调用 setCorePoolSize 等方法进行修改 任务提交 123456789101112131415161718192021222324// 执行任务void execute(Runnable command);// 提交任务 task，用返回值 Future 获得任务执行结果&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);// 提交 tasks 中所有任务&lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException;// 提交 tasks 中所有任务，带超时时间&lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException;// 提交 tasks 中所有任务，哪个任务先成功执行完毕，返回此任务执行结果，其它任务取消&lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException;// 提交 tasks 中所有任务，哪个任务先成功执行完毕，返回此任务执行结果，其它任务取消，带超时时间&lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; 关闭线程池 123456789101112131415161718192021222324/*线程池状态变为 SHUTDOWN - 不会接收新任务 - 但已提交任务会执行完 - 此方法不会阻塞调用线程的执行*/void shutdown();/*线程池状态变为 STOP - 不会接收新任务 - 会将队列中的任务返回 - 并用 interrupt 的方式中断正在执行的任务*/List&lt;Runnable&gt; shutdownNow();// 不在 RUNNING 状态的线程池，此方法就返回 trueboolean isShutdown();// 线程池状态是否是 TERMINATEDboolean isTerminated();// 调用 shutdown 后，由于调用线程并不会等待所有任务运行结束，因此如果它想在线程池 TERMINATED 后做些事情，可以利用此方法等待 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; 任务调度线程池 使用 ScheduledExecutorService 123456789ScheduledExecutorService executor = Executors.newScheduledThreadPool(2);// 添加两个任务，希望它们都在 1s 后执行executor.schedule(() -&gt; &#123; System.out.println(&quot;任务1，执行时间：&quot; + new Date()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125;&#125;, 1000, TimeUnit.MILLISECONDS);executor.schedule(() -&gt; &#123; System.out.println(&quot;任务2，执行时间：&quot; + new Date());&#125;, 1000, TimeUnit.MILLISECONDS); scheduleAtFixedRate 循环执行例子 12345ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleAtFixedRate(() -&gt; &#123; log.debug(&quot;running...&quot;);&#125;, 1, 1, TimeUnit.SECONDS); 输出： scheduleAtFixedRate 例子（任务执行时间超过了间隔时间）： 123456ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleAtFixedRate(() -&gt; &#123; log.debug(&quot;running...&quot;); sleep(2);&#125;, 1, 1, TimeUnit.SECONDS); 输出分析：一开始，延时 1s，接下来，由于任务执行时间 &gt; 间隔时间，间隔被『撑』到了 2s scheduleWithFixedDelay 例子： 123456ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleWithFixedDelay(()-&gt; &#123; log.debug(&quot;running...&quot;); sleep(2);&#125;, 1, 1, TimeUnit.SECONDS); 输出分析：一开始，延时 1s，scheduleWithFixedDelay 的间隔是 上一个任务结束 &lt;-&gt; 延时 &lt;-&gt; 下一个任务开始 所 以间隔都是 3s 评价 整个线程池表现为：线程数固定，任务数多于线程数时，会放入无界队列排队。任务执行完毕，这些线 程也不会被释放。用来执行延迟或反复执行的任务 Fork/Join Fork/Join 是 JDK 1.7 加入的新的线程池实现，它体现的是一种分治思想，适用于能够进行任务拆分的 cpu 密集型 运算 所谓的任务拆分，是将一个大任务拆分为算法上相同的小任务，直至不能拆分可以直接求解。跟递归相关的一些计算，如归并排序、斐波那契数列、都可以用分治思想进行求解 Fork/Join 在分治的基础上加入了多线程，可以把每个任务的分解和合并交给不同的线程来完成，进一步提升了运算效率 Fork/Join 默认会创建与 cpu 核心数大小相同的线程池 使用： 提交给 Fork/Join 线程池的任务需要继承 RecursiveTask（有返回值）或 RecursiveAction（没有返回值），例如下 面定义了一个对 1~n 之间的整数求和的任务 123456789101112131415161718192021222324252627282930@Slf4j(topic = &quot;c.AddTask&quot;)class AddTask1 extends RecursiveTask&lt;Integer&gt; &#123; int n; public AddTask1(int n) &#123; this.n = n; &#125; @Override public String toString() &#123; return &quot;&#123;&quot; + n + &#x27;&#125;&#x27;; &#125; @Override protected Integer compute() &#123; // 如果 n 已经为 1，可以求得结果了 if (n == 1) &#123; log.debug(&quot;join() &#123;&#125;&quot;, n); return n; &#125; // 将任务进行拆分(fork) AddTask1 t1 = new AddTask1(n - 1); t1.fork(); log.debug(&quot;fork() &#123;&#125; + &#123;&#125;&quot;, n, t1); // 合并(join)结果 int result = n + t1.join(); log.debug(&quot;join() &#123;&#125; + &#123;&#125; = &#123;&#125;&quot;, n, t1, result); return result; &#125;&#125; 然后提交给 ForkJoinPool 来执行 12345public static void main(String[] args) &#123; ForkJoinPool pool = new ForkJoinPool(4); System.out.println(pool.invoke(new AddTask1(5)));&#125; 结果： 8.AQS 概述 全称是 AbstractQueuedSynchronizer，是阻塞式锁和相关的同步器工具的框架 AQS定义了一套多线程访问共享资源的同步模板，解决了实现同步器时涉及的大量细节问题，能够极大地减少实现工作 AQS的组成结构 三部分组成：volatile int state同步状态、Node组成的CLH队列、ConditionObject条件变量（包含Node组成的条件单向队列）。 特点： 用 state 属性来表示资源的状态（分独占模式和共享模式），子类需要定义如何维护这个状态，控制如何获取锁和释放锁 getState - 获取 state 状态 setState - 设置 state 状态 compareAndSetState - cas 机制设置 state 状态 独占模式是只有一个线程能够访问资源，而共享模式可以允许多个线程访问资源 ReentrantLock的state用来表示是否有锁资源 ReentrantReadWriteLock的state高16位代表读锁状态，低16位代表写锁状态 Semaphore的state用来表示可用信号的个数 CountDownLatch的state用来表示计数器的值 提供了基于 FIFO （先进先出）的等待队列，类似于 Monitor 的 EntryList 条件变量来实现等待、唤醒机制，支持多个条件变量，类似于 Monitor 的 WaitSet 子类主要实现这样一些方法（默认抛出 UnsupportedOperationException） tryAcquire tryRelease tryAcquireShared tryReleaseShared isHeldExclusively 获取锁的姿势： 1234// 如果获取锁失败if (!tryAcquire(arg)) &#123; // 入队, 可以选择阻塞当前线程 park unpark&#125; 释放锁的姿势 1234// 如果释放锁成功if (tryRelease(arg)) &#123; // 让阻塞线程恢复运行&#125; 例子 实现不可重入锁 自定义同步器 1234567891011121314151617181920212223242526272829303132333435final class MySync extends AbstractQueuedSynchronizer &#123; @Override protected boolean tryAcquire(int acquires) &#123; if (acquires == 1)&#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; &#125; return false; &#125; @Override protected boolean tryRelease(int acquires) &#123; if(acquires == 1) &#123; if(getState() == 0) &#123; throw new IllegalMonitorStateException(); &#125; setExclusiveOwnerThread(null); setState(0); return true; &#125; return false; &#125; protected Condition newCondition() &#123; return new ConditionObject(); &#125; @Override protected boolean isHeldExclusively() &#123; return getState() == 1; &#125;&#125; 自定义锁 有了自定义同步器，很容易复用 AQS ，实现一个功能完备的自定义锁 12345678910111213141516171819202122232425262728293031323334353637383940class MyLock implements Lock &#123; static MySync sync = new MySync(); @Override // 尝试，不成功，进入等待队列 public void lock() &#123; sync.acquire(1); &#125; @Override // 尝试，不成功，进入等待队列，可打断 public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; @Override // 尝试一次，不成功返回，不进入队列 public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; @Override // 尝试，不成功，进入等待队列，有时限 public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(time)); &#125; @Override // 释放锁 public void unlock() &#123; sync.release(1); &#125; @Override // 生成条件变量 public Condition newCondition() &#123; return sync.newCondition(); &#125;&#125; CLH队列 CLH是AQS内部维护的FIFO（先进先出）双端双向队列（方便尾部节点插入），基于链表数据结构，当一个线程竞争资源失败，就会将等待资源的线程封装成一个Node节点，通过CAS原子操作插入队列尾部，最终不同的Node节点连接组成了一个CLH队列，所以说AQS通过CLH队列管理竞争资源的线程，个人总结CLH队列具有如下几个优点： 先进先出保证了公平性 非阻塞的队列，通过自旋锁和CAS保证节点插入和移除的原子性，实现无锁快速插入 采用了自旋锁思想，所以CLH也是一种基于链表的可扩展、高性能、公平的自旋锁 流程 线程获取资源失败，封装成Node节点从CLH队列尾部入队并阻塞线程，某线程释放资源时会把CLH队列首部Node节点关联的线程唤醒（此处的首部是指第二个节点，后面会细说），再次获取资源。， 入队 获取资源失败的线程需要封装成Node节点，接着尾部入队，在AQS中提供addWaiter函数完成Node节点的创建与入队。 添加节点的时候，如果从CLH队列已经存在，通过CAS快速将当前节点添加到队列尾部，如果添加失败或队列不存在，则指向enq函数自旋入队。 通过自旋CAS尝试往队列尾部插入节点，直到成功，自旋过程如果发现CLH队列不存在时会初始化CLH队列，入队过程流程如下图： 第一次循环 刚开始C L H队列不存在，head与tail都指向null 要初始化C L H队列，会创建一个哨兵节点，head与tail都指向哨兵节点 第二次循环 当前线程节点的前驱节点指向尾部节点（哨兵节点） 设置当前线程节点为尾部，tail指向当前线程节点 前尾部节点的后驱节点指向当前线程节点（当前尾部节点） 最后结合addWaiter与enq函数的入队流程图如下 出队 CLH队列中的节点都是获取资源失败的线程节点，当持有资源的线程释放资源时，会将head.next指向的线程节点唤醒（CLH队列的第二个节点），如果唤醒的线程节点获取资源成功，线程节点清空信息设置为头部节点（新哨兵节点），原头部节点出队（原哨兵节点）acquireQueued函数中的部分代码 假设获取资源成功，更换头部节点，并把头部节点的信息清除变成哨兵节点，注意这个过程是不需要使用CAS来保证，因为只有一个线程能够成功获取到资源。 条件变量 Object的wait、notify函数是配合Synchronized锁实现线程间同步协作的功能，A Q S的ConditionObject条件变量也提供这样的功能，通过ConditionObject的await和signal两类函数完成。不同于Synchronized锁，一个A Q S可以对应多个条件变量，而Synchronized只有一个 如上图所示，ConditionObject内部维护着一个单向条件队列，不同于C H L队列，条件队列只入队执行await的线程节点，并且加入条件队列的节点，不能在C H L队列， 条件队列出队的节点，会入队到C H L队列。 当某个线程执行了ConditionObject的await函数，阻塞当前线程，线程会被封装成Node节点添加到条件队列的末端，其他线程执行ConditionObject的signal函数，会将条件队列头部线程节点转移到C H L队列参与竞争资源，具体流程如下图 共享方式 AQS定义两种资源共享方式。无论是独占锁还是共享锁，本质上都是对AQS内部的一个变量state的获取。state是一个原子的int变量，用来表示锁状态、资源数等。 ① 独占锁(Exclusive)模式：只能被一个线程获取到(Reentrantlock)。 ② 共享锁(Share)模式：可以被多个线程同时获取(Semaphore/CountDownLatch/ReadWriteLock)。 state机制 提供volatile变量state，用于同步线程之间的共享状态。通过 CAS 和 volatile 保证其原子性和可见性。核心要点： state 用 volatile 修饰，保证多线程中的可见性 getState() 和 setState() 方法采用final修饰，限制AQS的子类重写它们两 compareAndSetState() 方法采用乐观锁思想的CAS算法，也是采用final修饰的，不允许子类重写 state应用案例： 案例 描述 Semaphore 使用AQS同步状态来保存信号量的当前计数。tryRelease会增加计数，acquireShared会减少计数 CountDownLatch 使用AQS同步状态来表示计数。计数为0时，所有的Acquire操作（CountDownLatch的await方法）才可以通过 ReentrantReadWriteLock 使用AQS同步状态中的16位保存写锁持有的次数，剩下的16位用于保存读锁的持有次数 ThreadPoolExecutor Worker利用AQS同步状态实现对独占线程变量的设置（tryAcquire和tryRelease） ReentrantLock 使用AQS保存锁重复持有的次数。当一个线程获取锁时，ReentrantLock记录当前获得锁的线程标识，用于检测是否重复获取，以及错误线程试图解锁操作时异常情况的处理 9.Semaphore 用来限制能同时访问共享资源的线程上限。 Semaphore是一个计数信号量，它的本质是一个&quot;共享锁&quot;。信号量维护了一个信号量许可集。线程可以通过调用acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以通过release()来释放它所持有的信号量许可。 流程 Semaphore 有点像一个停车场，permits 就好像停车位数量，当线程获得了 permits 就像是获得了停车位，然后 停车场显示空余车位减一 刚开始，permits（state）为 3，这时 5 个线程来获取资源 假设其中 Thread-1，Thread-2，Thread-4 cas 竞争成功，而 Thread-0 和 Thread-3 竞争失败，进入 AQS 队列 park 阻塞 这时 Thread-4 释放了 permits，状态如下 接下来 Thread-0 竞争成功，permits 再次设置为 0，设置自己为 head 节点，断开原来的 head 节点，unpark 接 下来的 Thread-3 节点，但由于 permits 是 0，因此 Thread-3 在尝试不成功后再次进入 park 状态 10. CountDownLatch 用来进行线程同步协作，等待所有线程完成倒计时。 其中构造参数用来初始化等待计数值，await() 用来等待计数归零，countDown() 用来让计数减一 CountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。 CountDownLatch和CyclicBarrier的区别 CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待 CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier 11.CyclicBarrier 循环栅栏，用来进行线程协作，等待线程满足某个计数。构造时设置『计数个数』，每个线程执 行到某个需要“同步”的时刻调用 await() 方法进行等待，当等待的线程数满足『计数个数』时，继续执行 CyclicBarrier是一个同步辅助类，允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 12.多线程通信 通信 多线程通讯的方式主要包括以下几种： 使用volatile关键词：基于共享内存的思想 使用Synchronized+Object类的wait()/notify()/notifyAll()方法 使用JUC工具类CountDownLatch：基于共享变量state实现 使用Lock（ReentrantLock）结合Condition 基于LockSupport实现线程间的阻塞和唤醒 线程协作 sleep/yield/join sleep() 让当前线程暂停指定的时间（毫秒） wait方法依赖于同步，而sleep方法可以直接调用 sleep方法只是暂时让出CPU的执行权，并不释放锁，而wait方法则需要释放锁 yield() 暂停当前线程，让出当前CPU的使用权，以便其它线程有机会执行 不能指定暂停的时间，并且也不能保证当前线程马上停止 会让当前线程从运行状态转变为就绪状态 yield只能使同优先级或更高优先级的线程有执行的机会 join() 等待调用 join 方法的线程执行结束，才执行后面的代码 其调用一定要在 start 方法之后（看源码可知） 作用是父线程等待子线程执行完成后再执行（即将异步执行的线程合并为同步的线程） wait/notify/notifyAll 一般需要配合synchronized一起使用。Object的主要方法如下： wait()：阻塞当前线程，直到 notify 或者 notifyAll 来唤醒 notify()：只能唤醒一个处于 wait 的线程 notifyAll()：唤醒全部处于 wait 的线程 await/signal/signalAll 使用显式的 Lock 和 Condition 对象： await()：当前线程进入等待状态，直到被通知（signal/signalAll）、中断或超时 signal()：唤醒一个等待在Condition上的线程，将该线程从等待队列中转移到同步队列中 signalAll()：能够唤醒所有等待在Condition上的线程 13.ThreadLocal 在多线程访问共享资源时会采取一定的线程同步方式（如：加锁）来解决带来的并发问题。（如图） 使用ThreadLocal对共享资源的访问也可以解决并发问题 **作用：**ThreadLocal提供了线程的本地变量，即当创建一个变量后，每个线程对其进行访问的时候访问的是自己线程的变量。 这里的本地内存并不是线程的工作内存，而是Thread类中的一个变量，而不是放在不是存放在ThreadLocal实例里面 这样做的好处： 线程安全，可以避免多线程访问同一个共享变量导致的并发问题。 不需要加锁，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 原理 ThreadLocalMap 前面提到：每个线程的本地变量是放在调用线程Thread类中的一个变量threadLocals中，而不是放在不是存放在ThreadLocal实例里面 ThreadLocal是基于每个线程对象内部的一个叫做threadLocals的属性来实现的，它的类型是ThreadLocalMap（说白了就是一个Map对象）。它以ThreadLocal本本身作为键值（注意：这里的引用为弱引用），副本对象作为value存储，这样当每个线程调用该对象时就可以直接从自身的threadLocals属性中获取变量副本来进行操作。 为什么要用map？ 这是因为在实际使用中可能会有多个ThreadLocal变量，因此需要将这些ThreadLocal添加到map中。 为什么要设置为弱引用？ 如果为强引用： 由于ThreadLocalMap是属于线程的，而我们创建多线程时一般是使用线程池进行创建，线程池中的部分线程在任务结束后是不会关闭的，那么这部分线程中的ThreadLocalMap将会一直持有对ThreadLocal对象的强引用，导致ThreadLocal对象无法被垃圾回收，从而造成内存泄漏。 因此设置为弱引用：在下一次垃圾回收时，无论内存空间是否足够，只有弱引用指向的对象都会被直接回收。所以将ThreadLocalMap对ThreadLocal对象的引用设置成弱引用，就能避免ThreadLocal对象无法回收导致内存泄漏的问题。 内存泄露解决 解决：在finally中remove即可。 14.线程间异步传参 使用阿里开源的TransmittableThreadLocal（TTL） 详见官网：https://gitcode.net/mirrors/alibaba/transmittable-thread-local?utm_source=csdn_github_accelerator 15.进程间的通信方式 管道 消息队列 共享内存 消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那共享内存的方式，就很好的解决了这一问题。 现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程A和 进程B的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。 共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去， 大大提高了进程间通信的速度。 信号量 为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信号量就实现了这一保护机制。 信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据 信号 信号一般用于一些异常情况下的进程间通信，是一种异步通信，它的数据结构一般就是一个数字。 socket 前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想跨网络与不同主机上的进程之间通信，就需要Socket通信了。 实际上，Socket通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。 16.其他问题 内核态和用户态的区别 内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。 **string stringbuffer stringbuilder的区别，各自的使用场景 ** 1、String是一个final类，代表不可变的字符序列，也就是String引用的字符串是不能改变的 2、StringBuffer/StringBuilder表示的字符串对象可以直接进行修改，而且方法也一样 3、StringBuilder是java5中引入的，和StringBuffer的方法完全相同。区别在与它是单线程环境下使用的，因为他的所有方法都没有synchronized修饰，他的效率理论上比StringBuffer要高 类 字符序列类型 效率 线程是否安全 String 不可变字符序列 效率低，但是复用率高 StringBuffer 可变字符序列 效率较高（在增删情况下） 安全 StringBuilder 可变字符序列 效率最高 不安全 ​ 任务事件中 如何保障多线程情况下线程安全的进行上层的writeAndFlush？ synchronized方法 加锁机制（ReentrantLock等） 使用Atomic对象 使用无状态对象（同样的输入返回一致的结果） 使用不可变对象（final）","categories":[{"name":"juc","slug":"juc","permalink":"http://cloud-tour.github.io/categories/juc/"}],"tags":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"},{"name":"juc","slug":"juc","permalink":"http://cloud-tour.github.io/tags/juc/"}]},{"title":"javase","slug":"javase","date":"2023-02-17T12:58:55.387Z","updated":"2023-02-17T13:01:44.155Z","comments":true,"path":"2023/02/17/javase/","link":"","permalink":"http://cloud-tour.github.io/2023/02/17/javase/","excerpt":"","text":"1.equals和hashcode的区别 在Java中任何一个对象都具备equals(Object obj)和hashCode()这两个方法，因为他们是在Object类中定义的。 equals(Object obj)方法用来判断两个对象是否“相同”，如果“相同”则返回true，否则返回false。 hashCode()方法返回一个int数，在Object类中的默认实现是“将该对象的内部地址转换成一个整数返回”。 如果只重写equals不重写hashcode会有问题吗？ 会有问题，如果我们想按照其他规则来判断两个对象相等与否，此时只重写equals而不重写hashcode，那么在将数据存储入set、list等散列表中时，java会默认采用目标地址进行hashcode，这是，我们认为的两个相同的对象就可能放入不同的数组位置上了，这样就造成了数据的不唯一性。 补充： 当集合要添加新的元素时，可分为两个步骤： 先调用这个元素的 hashCode 方法，然后根据所得到的值计算出元素应该在数组的位置。如果这个位置上没有元素，那么直接将它存储在这个位置上； 如果这个位置上已经有元素了，那么调用它的equals方法与新元素进行比较：相同的话就不存了，否则，将其存在这个位置对应的链表中（Java 中 HashSet, HashMap 和 Hashtable的实现总将元素放到链表的表头）。 2.树 二叉搜索树（二叉查找树） 二叉查找树具有的特性： 左子树上所有结点的值均小于或等于它的根结点的值。 右子树上所有结点的值均大于或等于它的根结点的值。 左、右子树也分别为二叉排序树。 二叉平衡树（ALV） 什么是二叉平衡树呢？ 1.具有二叉查找树的全部特性。 2.每个节点的左子树和右子树的高度差至多为1。 平衡树基于这种特点就可以保证不会出现大量节点偏向于一边的情况了!（插入或者删除时，会发生左旋、右旋操作，使这棵树再次左右保持一定的平衡) 为什么有了平衡树还需要红黑树呢？ 虽然平衡树解决了二叉查找树退化为近似链表的缺点，能够把查找时间控制在O(logn)，不过却不是最佳的。 因为平衡树要求每个节点的左子树和右子树的高度差至多等于1，这个要求实在是太严了，导致每次进行插入删除节点的时候几乎都会破坏平衡树的第二个规则，进而我们都需要通过左旋和右旋来进行调整，使之再次成为一颗符合要求的平衡树。 显然，如果在那种插入、删除很频繁的场景中，平衡树需要频繁着进行调整，这会使平衡树的性能大打折扣，为了解决这个问题，于是有了红黑树! 红黑树 红黑树一种自平衡的二叉查找树。除了具有二叉查找的特性外，还具有的特性： 节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点(NIL节点)。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 hashmap为什么不使用AVL树而使用红黑树？ 红黑树和AVL树都是最常用的平衡二叉搜索树，它们的查找、删除、修改都是O(lgn) time AVL树和红黑树有几点比较和区别： （1）AVL树是更加严格的平衡，因此可以提供更快的查找速度，一般读取查找密集型任务，适用AVL树。 （2）红黑树更适合于插入修改密集型任务。 （3）通常，AVL树的旋转比红黑树的旋转更加难以平衡和调试。 在CurrentHashMap中是加锁了的，实际上是读写锁，如果写冲突就会等待， 如果插入时间过长必然等待时间更长，而红黑树相对AVL树他的插入更快！ 3.java异常体系 java异常体系 Thorwable类（表示可抛出）是所有异常和错误的超类，两个直接子类为Error和Exception，分别表示错误和异常。 其中异常类Exception又分为运行时异常(RuntimeException)和非运行时异常， 这两种异常有很大的区别，也称之为不检查异常（Unchecked Exception）和检查异常（Checked Exception）。 Error与Exception Error错误：（这种错误无法处理）描述了Java运行时系统的内部错误和资源耗尽错误。一般是指虚拟机（JVM）相关的问题，如系统崩溃，虚拟机出错误等，这种错误无法恢复或不可能捕获，将导致应用程序中断，通常不处理。因为如果出现这样的内部错误，除了通告用户，并尽力使程序安全地终止之外，再也无能为力了。 Exception异常：Java的异常分为两种，checked Exception（编译时异常也叫非运行时异常）和 RuntimeException（运行时异常）。 运行时异常与非运行时异常 运行时异常（逻辑方面）都是RuntimeException类及其子类异常，如NullPointerException、IndexOutOfBoundsException等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。 非运行时异常（程序语法）是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 Java异常处理方法有 抛出异常，捕捉异常。主要依赖于try、catch、finally、throw、throws五个关键字。 4.面向对象 面向对象理解 面向对象是模型化的，你只需抽象出一个类，这是一个封闭的盒子，在这里你拥有数据也拥有解决问题的方法。需要什么功能直接使用就可以了，不必去一步一步的实现，至于这个功能是如何实现的，管我们什么事？我们会用就 可以了。 面向对象的底层其实还是面向过程，把面向过程抽象成类，然后封装，方便我们使用的就是面向对象了。 面向过程理解 面向过程是具体化的，流程化的，解决一个问题，你需要一步一步的分析，一步一步的实现。 面向对象和面向过程的区别 面向过程 优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、 嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。 缺点：没有面向对象易维护、易复用、易扩展 面向对象 优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出 低耦合的系统，使系统更加灵活、更加易于维护 缺点：性能比面向过程低 5.接口与抽象类 成员区别 抽象类 构造方法：有构造方法，用于子类实例化使用。 成员变量：可以是变量，也可以是常量。 成员方法：可以是抽象的，也可以是非抽象的。 接口 构造方法：没有构造方法 成员变量：只能是常量。默认修饰符：public static final 成员方法：jdk1.7只能是抽象的。默认修饰符：public abstract (推荐：默认修饰符请自己永远手动给出) jdk1.8可以写以default和static开头的具体方法 类与接口的关系区别 类与类 继承关系,只能单继承。可以多层继承。 类与接口 实现关系,可以单实现,也可以多实现。 类还可以在继承一个类的同时实现多个接口。 接口与接口 继承关系,可以单继承,也可以多继承。 体现的理念不同 抽象类里面定义的都是一个继承体系中的共性内容。 接口是功能的集合,是一个体系额外的功能，是暴露出来的规则。 选择抽象类还是接口的依据 当你关注一个事物的本质的时候，用抽象类；当你关注一个操作的时候，用接口。 抽象类的功能要远超过接口，但是，定义抽象类的代价高。因为高级语言来说（从实际设计上来说也是）每个类只能继承一个类。在这个类中，你必须继承或编写出其所有子类的所有共性。虽然接口在功能上会弱化许多，但是它只是针对一个动作的描述。而且你可以在一个类中同时实现多个接口。在设计阶段会降低难度。 6.静态变量和普通变量的区别 所属目标不同 静态变量属于类的变量，普通变量属于对象的变量。 存储区域不同 静态变量存储在方法区的静态区，普通变量存储在堆区。 加载时间不同 静态变量是随时类的加载而加载的，随着类的消失而消失。 普通变量是随着对象的加载而加载，随着对象的消失而消失。 调用方式不同 静态变量只能通过类名，对象调用。 普通变量只能通过对象调用。 static可以修饰局部变量么？ 不能是局部变量，可以是内部类，全局变量，方法，代码块。 7.有了int为什么还要Integer 主要是因为面向对象的思想，因为Java语言是面向对象的，这也是它只所以流行的原因之一，对象封装有很多好处，可以把属性也就是数据跟处理这些数据的方法结合在一起，比如Integer就有parseInt()等方法来专门处理int型相关的数据， 另一个非常重要的原因就是在Java中绝大部分方法或类都是用来处理类类型对象的，如ArrayList集合类就只能以类作为他的存储对象，而这时如果想把一个int型的数据存入list是不可能的，必须把它包装成类，也就是Integer才能被List所接受。所以Integer的存在是很必要的。 8.Java泛型的好处及底层原理 泛型好处 保证了类型的安全性，泛型可以使编译器知道一个对象的限定类型是什么，这样编译器就可以在一个高的程度上验证这个类型 消除了强制类型转换 使得代码可读性好，减少了很多出错的机会 泛型的好处是在编译的时候检查类型安全，并且所有的强制转换都是自动和隐式的，提高代码的重用率。 避免了不必要的装箱、拆箱操作，提高程序的性能 实现原理 泛型的实现是靠类型擦除技术 类型擦除是在编译期完成的 也就是在编译期 编译器会将泛型的类型参数都擦除成它的限定类型，如果没有则擦除为object类型之后在获取的时候再强制类型转换为对应的类型。 如果构建泛型实例时使用了泛型语法，那么编译器将标记该实例并关注该实例后续所有方法的调用，每次调用前都进行安全检查，非指定类型的方法都不能调用成功。 9.java定时器实现 使用Timer和和TimerTask类 1、Timer和TimerTask是java.util包下的类，用于实现定时任务 步骤1：创建TimerTask定时器任务，可以通过匿名内部类的方式创建 步骤2：创建Timer定时器，调用定时器的方法执行定时器任务 2、Timer的两个方法schedule()和scheduleAtFixedRate()及其重载方法： void schedule(TimerTask task, long delay)：在指定时间后执行1次任务，其中delay表示时延，单位是毫秒，设置为1000，则表示1秒后执行一次定时器任务； void schedule(TimerTask task, long delay, long period)：指定延迟指定时间后周期性地执行任务（delay毫秒后，每period毫秒执行一次） void scheduleAtFixedRate(TimerTask task, long delay, long period)：指定延迟指定时间后周期性地执行任务（delay毫秒后，每period毫秒执行一次） void scheduleAtFixedRate(TimerTask task, Date firstTime,long period) ：从指定日期firstTime开始，每period毫秒执行一次任务 案例： 12345678910111213141516171819public class TimerExample &#123; public static void main(String[] args) &#123; // 创建定时器 Timer timer = new Timer(); // 创建定时器任务 TimerTask timerTask = new TimerTask() &#123; @Override public void run() &#123; System.out.println(&quot;Hello world!&quot;); &#125; &#125;; timer.schedule(timerTask, 1000); // 1秒后执行一次 timer.schedule(timerTask, 2000, 2000); // 两秒后每两秒执行一次 timer.scheduleAtFixedRate(timerTask, 3000, 3000); // 3秒后每3秒执行一次 timer.scheduleAtFixedRate(task, new Date(), 4000); // 每4秒执行一次 &#125; &#125; 使用线程池 案例： 1234567891011121314151617public class TimerExample &#123; public static void main(String[] args) &#123; // 创建定时器任务 TimerTask timerTask = new TimerTask() &#123; @Override public void run() &#123; System.out.println(&quot;Hello world!&quot;); &#125; &#125;; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(2); scheduledThreadPool.schedule(timerTask, 1000, TimeUnit.MILLISECONDS); scheduledThreadPool.scheduleAtFixedRate(timerTask, 1000, 1000, TimeUnit.MILLISECONDS); &#125; &#125; 使用Spring Task 步骤1：在springBoot启动类上添加@EnableScheduling注解 12345678@EnableScheduling@SpringBootApplicationpublic class SpringbootApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootApplication.class, args); &#125;&#125; 步骤2：创建一个定时任务类的bean，在类的方法上使用@Schedule注解，通过注解的cron属性设置定时器的属性 12345678@Componentpublic class TimerTask &#123; @Scheduled(cron = &quot;0 7 2 26 7 *&quot;) public void task() &#123; System.out.println(&quot;定时任务...&quot;); &#125;&#125; 以上代码指定在2022年7月26日02:07:00执行一次定时任务 通过Quartz任务调度工具 步骤1：在pom.xml中添加quartz的依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;&lt;/dependency&gt; 步骤2：创建quartz的配置类 1234567891011121314151617181920212223242526272829@Configurationpublic class QuartzConfig &#123; // 创建一个JobDetail(工作详情)类对象,保存到Spring容器中，这个类用于封装我们编写的job接口实现类 @Bean public JobDetail jobDetail()&#123; System.out.println(&quot;showTime方法运行&quot;); return JobBuilder.newJob(QuartzJob.class) // 绑定要运行的任务类的类对象 .withIdentity(&quot;job&quot;) // 设置job的名称 .storeDurably() // 信息持久 // 设置storeDurably之后,当没有触发器指向这个JobDetail时,JobDetail也不会从 // Spring容器中删除,如果不设置这行,就会自动从Spring容器中删除 .build(); &#125; // 声明触发器，触发器决定我们的工作\\任务何时触发@Beanpublic Trigger trigger()&#123; System.out.println(&quot;showTime触发器运行&quot;); // 定义Cron表达式，每分钟触发一次 CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(&quot;0/10 * * * * ?&quot;); return TriggerBuilder.newTrigger() .forJob(jobDetail()) // 绑定JobDetail对象 .withIdentity(&quot;trigger&quot;) // 定义触发器名称 .withSchedule(cronScheduleBuilder) // 绑定Cron表达式 .build();&#125; } 步骤:3：定义Job 1234567public class QuartzJob implements Job &#123; @Override public void execute(JobExecutionContext jobExecutionContext) &#123; // 输出当前时间 System.out.println(LocalDateTime.now()); &#125;&#125; 10.java中有哪些队列 Java 中的队列可以从不同的维度进行分类，例如可以从阻塞和非阻塞进行分类，也可以从有界和无界进行分类，而这里将从队列的功能上进行分类，例如：优先队列、普通队列、双端队列、延迟队列等。 普通队列 普通队列（Queue）是指实现了先进先出的基本队列，例如 ArrayBlockingQueue 和 LinkedBlockingQueue，其中 ArrayBlockingQueue 是用数组实现的普通队列，而 LinkedBlockingQueue 是使用链表实现的普通队列 双端队列 双端队列（Deque）是指队列的头部和尾部都可以同时入队和出队的数据结构。例如：LinkedBlockingDeque 优先队列 优先队列（PriorityQueue）是一种特殊的队列，它并不是先进先出的，而是优先级高的元素先出队。 优先队列是根据二叉堆实现的，二叉堆的数据结构如下图所示： 二叉堆分为两种类型：一种是最大堆一种是最小堆。以上展示的是最大堆，在最大堆中，任意一个父节点的值都大于等于它左右子节点的值。 因为优先队列是基于二叉堆实现的，因此它可以将优先级最好的元素先出队。 优先队列的出队是不考虑入队顺序的，它始终遵循的是优先级高的元素先出队。 延迟队列 延迟队列（DelayQueue）是基于优先队列 PriorityQueue 实现的，它可以看作是一种以时间为度量单位的优先的队列，当入队的元素到达指定的延迟时间之后方可出队。 其他队列（例如SynchronousQueue同步移交队列） 在 Java 的队列中有一个比较特殊的队列 SynchronousQueue，它的特别之处在于它内部没有容器，每次进行 put() 数据后（添加数据），必须等待另一个线程拿走数据后才可以再次添加数据 11.获取类中私有属性 通过反射获得 1234567891011121314//获取学生类的字节码对象Class clazzClass=Class.forName(&quot;com.test2.Student&quot;);//获取学生对象Object stuObject=clazzClass.newInstance();//获取私有的字段对象Field field=clazzClass.getDeclaredField(&quot;nameString&quot;);field.setAccessible(true);//设置发射时取消Java的访问检查，暴力访问System.out.println(field);field.set(stuObject, &quot;桂贤松&quot;);System.out.println(stuObject);//获取的是地址//使其获取到值Object nameObject=field.get(stuObject);System.out.println(nameObject); 12.分布式事务 满足ACID（原子性、一致性、隔离性、持久性）的一组操作，可以被称为一个事务。随着计算机系统的发展，越来越多的采用分布式的架构来对外提供服务，但是，不同的机器的处理性能、存储性能、网络状态等各有不同，让分布式集群始终对外提供可用的一致性服务一直是需要处理的问题。 为了保证数据变更请求在整个分布式环境下正确地执行，不会导致部分服务器暂时崩溃导致整个集群提供的服务和数据不再相同，在整个分布式系统处理数据变更请求的过程中，需要引入分布式事务的概念。常见的提交方式有二阶段提交（Two-phase Commit，2PC）和三阶段提交（Three-phase commit，3PC）。 二阶段提交 2PC，两阶段提交，将事务的提交过程分为资源准备和资源提交两个阶段，并且由事务协调者来协调所有事务参与者，如果准备阶段所有事务参与者都预留资源成功，则进行第二阶段的资源提交，否则事务协调者回滚资源。 第一阶段： 在XA分布式事务的第一阶段，作为事务协调者的节点会首先向所有的参与者节点发送Prepare请求。 在接到Prepare请求之后，每一个参与者节点会各自执行与事务有关的数据更新，写入Undo Log和Redo Log。如果参与者执行成功，暂时不提交事务，而是向事务协调节点返回“完成”消息。 当事务协调者接到了所有参与者的返回消息，整个分布式事务将会进入第二阶段。 第二阶段： 在XA分布式事务的第二阶段，如果事务协调节点在之前所收到都是正向返回，那么它将会向所有事务参与者发出Commit请求。 接到Commit请求之后，事务参与者节点会各自进行本地的事务提交，并释放锁资源。当本地事务完成提交后，将会向事务协调者返回“完成”消息。 当事务协调者接收到所有事务参与者的“完成”反馈，整个分布式事务完成。 以上所描述的是XA两阶段提交的正向流程，接下来我们看一看失败情况的处理流程： 第一阶段： 第二阶段： 在XA的第一阶段，如果某个事务参与者反馈失败消息，说明该节点的本地事务执行不成功，必须回滚。 于是在第二阶段，事务协调节点向所有的事务参与者发送Abort请求。接收到Abort请求之后，各个事务参与者节点需要在本地进行事务的回滚操作，回滚操作依照Undo Log来进行。 两阶段提交的不足 1.性能问题 XA协议遵循强一致性。在事务执行过程中，各个节点占用着数据库资源，只有当所有节点准备完毕，事务协调者才会通知提交，参与者提交后释放资源。这样的过程有着非常明显的性能问题。 2.协调者单点故障问题 事务协调者是整个XA模型的核心，一旦事务协调者节点挂掉，参与者收不到提交或是回滚通知，参与者会一直处于中间状态无法完成事务。 3.丢失消息导致的不一致问题。 在XA协议的第二个阶段，如果发生局部网络问题，一部分事务参与者收到了提交消息，另一部分事务参与者没收到提交消息，那么就导致了节点之间数据的不一致。 二阶段无法解决的问题 协调者在发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了，那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 三阶段提交 3PC，三阶段提交协议，是二阶段提交协议的改进版本，三阶段提交有两个改动点： （1）在协调者和参与者中都引入超时机制 （2）在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。 所以3PC会分为3个阶段，CanCommit 准备阶段、PreCommit 预提交阶段、DoCommit 提交阶段，处理流程如下： 1、阶段一：CanCommit 准备阶段 ​ 协调者向参与者发送 canCommit 请求，参与者如果可以提交就返回Yes响应，否则返回No响应，具体流程如下： （1）事务询问：协调者向所有参与者发出包含事务内容的 canCommit 请求，询问是否可以提交事务，并等待所有参与者答复。 （2）响应反馈：参与者收到 canCommit 请求后，如果认为可以执行事务操作，则反馈 yes 并进入预备状态，否则反馈 no。 2、阶段二：PreCommit 阶段 ​ 协调者根据参与者的反应情况来决定是否可以进行事务的 PreCommit 操作。根据响应情况，有以下两种可能： （1）执行事务： 假如所有参与者均反馈 yes，协调者预执行事务，具体如下： ① 发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入准备阶段 ② 事务预提交 ：参与者接收到 PreCommit 请求后，会执行本地事务操作，并将 undo 和 redo 信息记录到事务日志中（但不提交事务） ③ 响应反馈 ：如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 （2）中断事务： 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断，流程如下： ① 发送中断请求 ：协调者向所有参与者发送 abort 请求。 ② 中断事务 ：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 3、阶段三：doCommit阶段 该阶段进行真正的事务提交，也可以分为以下两种情况： （1）提交事务： ① 发送提交请求：协调接收到所有参与者发送的ACK响应，那么他将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求 ② 本地事务提交：参与者接收到doCommit请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源 ③ 响应反馈：事务提交完之后，向协调者发送ack响应。 ④ 完成事务：协调者接收到所有参与者的ack响应之后，完成事务。 （2）中断事务： 任何一个参与者反馈 no，或者等待超时后协调者尚无法收到所有参与者的反馈，即中断事务 ① 发送中断请求：如果协调者处于工作状态，向所有参与者发出 abort 请求 ② 事务回滚：参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 ③ 反馈结果：参与者完成事务回滚之后，向协调者反馈ACK消息 ④ 中断事务：协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 进入doCommit阶段后，无论协调者出现问题，或者协调者与参与者之间的网络出现问题，都会导致参与者无法接收到协调者发出的 doCommit 请求或 abort 请求。此时，参与者都会在等待超时之后，继续执行事务提交。这其实基于概率来决定的，当进入第三阶段时，说明第一阶段收到所有参与者的CanCommit响应都是Yes，意味着大家都同意修改了，并且第二阶段所有的参与者对协调者的PreCommit请求也都是同意的。所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 3PC的优缺点： 与2PC相比，3PC降低了阻塞范围，并且在等待超时后，协调者或参与者会中断事务，避免了协调者单点问题，阶段三中协调者出现问题时，参与者会继续提交事务。 ​ 数据不一致问题依然存在，当在参与者收到 preCommit 请求后等待 doCommit 指令时，此时如果协调者请求中断事务，而协调者因为网络问题无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。 12PC和3PC都无法保证数据绝对的一致性，一般为了预防这种问题，可以添加一个报警，比如监控到事务异常的时候，通过脚本自动补偿差异的信息。 TTC 1、什么是TCC： TCC（Try Confirm Cancel）是应用层的两阶段提交，所以对代码的侵入性强，其核心思想是：针对每个操作，都要实现对应的确认和补偿操作，也就是业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，第一阶段由业务代码编排来调用Try接口进行资源预留，当所有参与者的 Try 接口都成功了，事务协调者提交事务，并调用参与者的 confirm 接口真正提交业务操作，否则调用每个参与者的 cancel 接口回滚事务，并且由于 confirm 或者 cancel 有可能会重试，因此对应的部分需要支持幂等。 2、TCC的执行流程： ​ TCC的执行流程可以分为两个阶段，分别如下： （1）第一阶段：Try，业务系统做检测并预留资源 (加锁，锁住资源)，比如常见的下单，在try阶段，我们不是真正的减库存，而是把下单的库存给锁定住。 （2）第二阶段：根据第一阶段的结果决定是执行confirm还是cancel Confirm：执行真正的业务（执行业务，释放锁） Cancle：是对Try阶段预留资源的释放（出问题，释放锁） 3、TCC如何保证最终一致性： TCC 事务机制以 Try 为中心的，Confirm 确认操作和 Cancel 取消操作都是围绕 Try 而展开。因此，Try 阶段中的操作，其保障性是最好的，即使失败，仍然有 Cancel 取消操作可以将其执行结果撤销。 Try阶段执行成功并开始执行 Confirm 阶段时，默认 Confirm 阶段是不会出错的，也就是说只要 Try 成功，Confirm 一定成功（TCC设计之初的定义） Confirm 与 Cancel 如果失败，由TCC框架进行重试补偿存在极低概率在CC环节彻底失败，则需要定时任务或人工介入 4、TCC的注意事项： （1）允许空回滚： 空回滚出现的原因是 Try 超时或者丢包，导致 TCC 分布式事务二阶段的 回滚，触发 Cancel 操作，此时事务参与者未收到Try，但是却收到了Cancel 请求。 所以 cancel 接口在实现时需要允许空回滚，也就是 Cancel 执行时如果发现没有对应的事务 xid 或主键时，需要返回回滚成功，让事务服务管理器认为已回滚。 （2）防悬挂控制： 悬挂指的是二阶段的 Cancel 比 一阶段的Try 操作先执行，出现该问题的原因是 Try 由于网络拥堵而超时，导致事务管理器生成回滚，触发 Cancel 接口，但之后拥堵在网络的 Try 操作又被资源管理器收到了，但是 Cancel 比 Try 先到。但按照前面允许空回滚的逻辑，回滚会返回成功，事务管理器认为事务已回滚成功，所以此时应该拒绝执行空回滚之后到来的 Try 操作，否则会产生数据不一致。因此我们可以在 Cancel 空回滚返回成功之前，先记录该条事务 xid 或业务主键，标识这条记录已经回滚过，Try 接口执行前先检查这条事务xid或业务主键是否已经标记为回滚成功，如果是则不执行 Try 的业务操作。 （3）幂等控制： 由于网络原因或者重试操作都有可能导致 Try - Confirm - Cancel 3个操作的重复执行，所以使用 TCC 时需要注意这三个操作的幂等控制，通常我们可以使用事务 xid 或业务主键判重来控制。 5、TCC方案的优缺点： （1）TCC 事务机制相比于上面介绍的 XA 事务机制，有以下优点： 性能提升：具体业务来实现，控制资源锁的粒度变小，不会锁定整个资源。 数据最终一致性：基于 Confirm 和 Cancel 的幂等性，保证事务最终完成确认或者取消，保证数据的一致性。 可靠性：解决了 XA 协议的协调者单点故障问题，由主业务方发起并控制整个业务活动，业务活动管理器也变成多点，引入集群。 （2）缺点 TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。 Saga事务 1、什么是Saga事务： ​ Saga 事务核心思想是将长事务拆分为多个本地短事务并依次正常提交，如果所有短事务均执行成功，那么分布式事务提交；如果出现某个参与者执行本地事务失败，则由 Saga 事务协调器协调根据相反顺序调用补偿操作，回滚已提交的参与者，使分布式事务回到最初始的状态。Saga 事务基本协议如下： （1）每个 Saga 事务由一系列幂等的有序子事务(sub-transaction) Ti 组成。 （2）每个 Ti 都有对应的幂等补偿动作 Ci，补偿动作用于撤销 Ti 造成的结果。 与TCC事务补偿机制相比，TCC有一个预留(Try)动作，相当于先报存一个草稿，然后才提交；Saga事务没有预留动作，直接提交。 2、Saga的恢复策略： 对于事务异常，Saga提供了两种恢复策略，分别如下： （1）向后恢复(backward recovery)： 当执行事务失败时，补偿所有已完成的事务，是“一退到底”的方式，这种做法的效果是撤销掉之前所有成功的子事务，使得整个 Saga 的执行结果撤销。如下图： 从上图可知事务执行到了支付事务T3，但是失败了，因此事务回滚需要从C3,C2,C1依次进行回滚补偿，对应的执行顺序为：T1,T2,T3,C3,C2,C1。 （2）向前恢复(forward recovery)： ​ 对于执行不通过的事务，会尝试重试事务，这里有一个假设就是每个子事务最终都会成功，这种方式适用于必须要成功的场景，事务失败了重试，不需要补偿。流程如下图： 3、Saga事务的实现方式： Saga事务有两种不同的实现方式，分别如下： 命令协调（Order Orchestrator） 事件编排（Event Choreographyo） （1）命令协调： ​ 中央协调器（Orchestrator，简称 OSO）以命令/回复的方式与每项服务进行通信，全权负责告诉每个参与者该做什么以及什么时候该做什么。整体流程如下图： ① 事务发起方的主业务逻辑请求 OSO 服务开启订单事务 ② OSO 向库存服务请求扣减库存，库存服务回复处理结果。 ③ OSO 向订单服务请求创建订单，订单服务回复创建结果。 ④ OSO 向支付服务请求支付，支付服务回复处理结果。 ⑤ 主业务逻辑接收并处理 OSO 事务处理结果回复。 ​ 中央协调器 OSO 必须事先知道执行整个事务所需的流程，如果有任何失败，它还负责通过向每个参与者发送命令来撤销之前的操作来协调分布式的回滚，基于中央协调器协调一切时，回滚要容易得多，因为协调器默认是执行正向流程，回滚时只要执行反向流程即可。 （2）事件编排： ​ 命令协调方式基于中央协调器实现，所以有单点风险，但是事件编排方式没有中央协调器。事件编排的实现方式中，每个服务产生自己的时间并监听其他服务的事件来决定是否应采取行动。 ​ 在事件编排方法中，第一个服务执行一个事务，然后发布一个事件，该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何 Saga 参与者听到都意味着事务结束。 ① 事务发起方的主业务逻辑发布开始订单事件。 ② 库存服务监听开始订单事件，扣减库存，并发布库存已扣减事件。 ③ 订单服务监听库存已扣减事件，创建订单，并发布订单已创建事件。 ④ 支付服务监听订单已创建事件，进行支付，并发布订单已支付事件。 ⑤ 主业务逻辑监听订单已支付事件并处理。 如果事务涉及 2 至 4 个步骤，则非常合适使用事件编排方式，它是实现 Saga 模式的自然方式，它很简单，容易理解，不需要太多的代码来构建。 4、Saga事务的优缺点： （1）命令协调设计的优缺点： ① 优点： 服务之间关系简单，避免服务间循环依赖，因为 Saga 协调器会调用 Saga 参与者，但参与者不会调用协调器。 程序开发简单，只需要执行命令/回复(其实回复消息也是一种事件消息)，降低参与者的复杂性。 易维护扩展，在添加新步骤时，事务复杂性保持线性，回滚更容易管理，更容易实施和测试。 ② 缺点： 中央协调器处理逻辑容易变得庞大复杂，导致难以维护。 存在协调器单点故障风险。 （2）事件编排设计的优缺点： ① 优点： 避免中央协调器单点故障风险。 当涉及的步骤较少服务开发简单，容易实现。 ② 缺点： 服务之间存在循环依赖的风险。 当涉及的步骤较多，服务间关系混乱，难以追踪调测。 1由于 Saga 模型没有 Prepare 阶段，因此事务间不能保证隔离性。当多个 Saga 事务操作同一资源时，就会产生更新丢失、脏数据读取等问题，这时需要在业务层控制并发，例如：在应用层面加锁，或者应用层面预先冻结资源。 本地消息表 1、什么是本地消息表： 本地消息表的核心思路就是将分布式事务拆分成本地事务进行处理，在该方案中主要有两种角色：事务主动方和事务被动方。事务主动发起方需要额外新建事务消息表，并在本地事务中完成业务处理和记录事务消息，并轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。 ​ 这样可以避免以下两种情况导致的数据不一致性： 业务处理成功、事务消息发送失败 业务处理失败、事务消息发送成功 2、本地消息表的执行流程： ① 事务主动方在同一个本地事务中处理业务和写消息表操作 ② 事务主动方通过消息中间件，通知事务被动方处理事务消息。消息中间件可以基于 Kafka、RocketMQ 消息队列，事务主动方主动写消息到消息队列，事务消费方消费并处理消息队列中的消息。 ③ 事务被动方通过消息中间件，通知事务主动方事务已处理的消息。 ④ 事务主动方接收中间件的消息，更新消息表的状态为已处理。 一些必要的容错处理如下： 当①处理出错，由于还在事务主动方的本地事务中，直接回滚即可 当②、③处理出错，由于事务主动方本地保存了消息，只需要轮询消息重新通过消息中间件发送，通知事务被动方重新读取消息处理业务即可。 如果是业务上处理失败，事务被动方可以发消息给事务主动方回滚事务 如果事务被动方已经消费了消息，事务主动方需要回滚事务的话，需要发消息通知事务主动方进行回滚事务。 3、本地消息表的优缺点： （1）优点： 从应用设计开发的角度实现了消息数据的可靠性，消息数据的可靠性不依赖于消息中间件，弱化了对 MQ 中间件特性的依赖。 方案轻量，容易实现。 （2）缺点： 与具体的业务场景绑定，耦合性强，不可公用 消息数据与业务数据同库，占用业务系统资源 业务系统在使用关系型数据库的情况下，消息服务性能会受到关系型数据库并发性能的局限 ## MQ事务消息 1、MQ事务消息的执行流程： ​ 基于MQ的分布式事务方案本质上是对本地消息表的封装，整体流程与本地消息表一致，唯一不同的就是将本地消息表存在了MQ内部，而不是业务数据库中，如下图： ​ 由于将本地消息表存在了MQ内部，那么MQ内部的处理尤为重要，下面主要基于 RocketMQ4.3 之后的版本介绍 MQ 的分布式事务方案 2、RocketMQ事务消息： ​ 在本地消息表方案中，保证事务主动方发写业务表数据和写消息表数据的一致性是基于数据库事务，而 RocketMQ 的事务消息相对于普通 MQ提供了 2PC 的提交接口，方案如下： （1）正常情况： 在事务主动方服务正常，没有发生故障的情况下，发消息流程如下： 步骤①：发送方向 MQ Server(MQ服务方)发送 half 消息 步骤②：MQ Server 将消息持久化成功之后，向发送方 ack 确认消息已经发送成功 步骤③：发送方开始执行本地事务逻辑 步骤④：发送方根据本地事务执行结果向 MQ Server 提交二次确认（commit 或是 rollback）。 最终步骤：MQ Server 如果收到的是 commit 操作，则将半消息标记为可投递，MQ订阅方最终将收到该消息；若收到的是 rollback 操作则删除 half 半消息，订阅方将不会接受该消息 （2）异常情况： 在断网或者应用重启等异常情况下，图中的步骤④提交的二次确认超时未到达 MQ Server，此时的处理逻辑如下： 步骤⑤：MQ Server 对该消息发起消息回查 步骤⑥：发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果 步骤⑦：发送方根据检查得到的本地事务的最终状态再次提交二次确认。 最终步骤：MQ Server基于 commit/rollback 对消息进行投递或者删除。 3、MQ事务消息的优缺点： （1）优点：相比本地消息表方案，MQ 事务方案优点是： 消息数据独立存储 ，降低业务系统与消息系统之间的耦合 吞吐量大于使用本地消息表方案 （2）缺点： 一次消息发送需要两次网络请求(half 消息 + commit/rollback 消息) 。 业务处理服务需要实现消息状态回查接口。 最大努力通知 最大努力通知也称为定期校对，是对MQ事务方案的进一步优化。它在事务主动方增加了消息校对的接口，如果事务被动方没有接收到主动方发送的消息，此时可以调用事务主动方提供的消息校对的接口主动获取 ​ 在可靠消息事务中，事务主动方需要将消息发送出去，并且让接收方成功接收消息，这种可靠性发送是由事务主动方保证的；但是最大努力通知，事务主动方仅仅是尽最大努力（重试，轮询…）将事务发送给事务接收方，所以存在事务被动方接收不到消息的情况，此时需要事务被动方主动调用事务主动方的消息校对接口查询业务消息并消费，这种通知的可靠性是由事务被动方保证的。 ​ 所以最大努力通知适用于业务通知类型，例如微信交易的结果，就是通过最大努力通知方式通知各个商户，既有回调通知，也有交易查询接口。 各方案常见使用场景 2PC/3PC：依赖于数据库，能够很好的提供强一致性和强事务性，但延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。 TCC：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。 本地消息表/MQ 事务：适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。 Saga 事务：由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。Saga 由于缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。所以，Saga 事务较适用于补偿动作容易处理的场景 13.其他 字符串数据结构在C语言的底层实现，是字节数组吗。 jdk1.8及以前String底层使用是char[]，1.9开始使用byte[] 原子变量的实现原理 底层用到都是cas，类中用了unsafe来实现cas Unsafe 对象提供了非常底层的，操作内存、线程的方法，Unsafe 对象不能直接调用，只能通过反射获得 有没有更好的计数器解决策略 LongAdder","categories":[{"name":"javase","slug":"javase","permalink":"http://cloud-tour.github.io/categories/javase/"}],"tags":[{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"}]},{"title":"debian系统下安装nginx","slug":"debian系统下安装nginx","date":"2022-10-08T04:14:18.807Z","updated":"2022-10-08T04:29:18.082Z","comments":true,"path":"2022/10/08/debian系统下安装nginx/","link":"","permalink":"http://cloud-tour.github.io/2022/10/08/debian%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%89%E8%A3%85nginx/","excerpt":"","text":"​ 博主在工作学习时，经常性使用到nginx，但是某些时候有一些指令总是忘记，去网上收缩有甚是繁琐，因此用本篇博客记录安装的指令，与最后各种文件存放的位置 debian下安装nginx 在安装nginx之前，需要先安装一些插件，使得后面nginx能够正常运行 gcc 1apt install -y build-essential 正则库 1apt install -y libpcre3 libpcre3-dev zlib库 1apt install -y zlib1g-dev OpenSSL库 1apt install -y openssl libssl-dev 下载nginx源码 1234567891011# 下载源码wget http://nginx.org/download/nginx-1.20.2.tar.gz# 解压源码tar -xf nginx-1.20.2.tar.gz# 进入源代码内cd nginx-1.20.2 配置 1234567891011121314151617181920212223242526272829303132333435363738./configure \\--prefix=/usr/local/nginx \\--user=www \\--group=www \\--sbin-path=/usr/local/nginx/sbin/nginx \\--conf-path=/usr/local/nginx/nginx.conf \\--error-log-path=/var/log/nginx/error.log \\--http-log-path=/var/log/nginx/access.log \\--pid-path=/var/run/nginx.pid \\--lock-path=/var/run/nginx.lock \\--http-client-body-temp-path=/var/cache/nginx/client_temp \\--http-proxy-temp-path=/var/cache/nginx/proxy_temp \\--http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp \\--http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp \\--http-scgi-temp-path=/var/cache/nginx/scgi_temp \\--with-file-aio \\--with-threads \\--with-http_addition_module \\--with-http_auth_request_module \\--with-http_dav_module \\--with-http_flv_module \\--with-http_gunzip_module \\--with-http_gzip_static_module \\--with-http_mp4_module \\--with-http_random_index_module \\--with-http_realip_module \\--with-http_secure_link_module \\--with-http_slice_module \\--with-http_ssl_module \\--with-http_stub_status_module \\--with-http_sub_module \\--with-http_v2_module \\--with-mail \\--with-mail_ssl_module \\--with-stream \\--with-stream_realip_module \\--with-stream_ssl_module \\--with-stream_ssl_preread_module 其中： --prefix：Nginx主要安装路径，后续Nginx子目录依照这个变量展开 --user：设置Nginx进程启动时，所属的用户 --group：设置Nginx进程启动时，所属的用户组 编译 1make 安装 1make install 创建systemctl守护，管理Nginx： 1vim /usr/lib/systemd/system/nginx.service 12345678910111213[Unit]Description=nginxAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true [Install]WantedBy=multi-user.target 启动各服务 123456789systemctl daemon-reload-------------------systemctl start nginx-------------------#观察nginx启动状态systemctl status nginx.service-------------------#查看80线程状态lsof -i:80 nginx命令 如果你是按我的方法编译，那么，需要注意。 /usr/local/nginx：为Nginx编译安装的地址。 /usr/local/nginx/conf/nginx.conf：Nginx默认配置文件。 同时，我们使用systemctl对Nginx进行管理： systemctl start nginx：启动Nginx服务。 systemctl reload nginx：Nginx配置重载。 systemctl stop nginx：停止Nginx服务。 本片文章参考https://www.php.cn/nginx/488924.html","categories":[{"name":"linux","slug":"linux","permalink":"http://cloud-tour.github.io/categories/linux/"}],"tags":[{"name":"debian","slug":"debian","permalink":"http://cloud-tour.github.io/tags/debian/"},{"name":"nginx","slug":"nginx","permalink":"http://cloud-tour.github.io/tags/nginx/"}]},{"title":"debian系统配置java11环境","slug":"debian系统配置java11环境","date":"2022-10-08T03:31:36.963Z","updated":"2022-10-08T03:47:19.584Z","comments":true,"path":"2022/10/08/debian系统配置java11环境/","link":"","permalink":"http://cloud-tour.github.io/2022/10/08/debian%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AEjava11%E7%8E%AF%E5%A2%83/","excerpt":"","text":"​ 博主最近在工作中使用到debian系统，在初始系统中要进行一些配置，使后续的程序能正常运行，因此用该篇文章记录一些配置的步骤。 博主用的debian系统为11.2版本 下载jdk11 分别输入下列两条指令 123apt search openjdk------------------------------sudo apt install default-jdk 输入查看java版本 1java -version 出现下列信息即为安装成功 其他操作 ​ 其实在上述操作后应该就可以正常运行java程序了，但是为了以防万一，再贴出其他操作 debian的系统环境变量 debian系统的环境变量位置位于/etc/profile 入若需要进行配置环境变量，直接vim即可 配置方式： ​ 配置环境变量时，需要先在windows中下载jdk11的安装包，然后传到debian的/etc/java中，再解压。具体信息步骤博主就不在这贴出了。自行百度，很简单的。","categories":[{"name":"linux","slug":"linux","permalink":"http://cloud-tour.github.io/categories/linux/"}],"tags":[{"name":"debian","slug":"debian","permalink":"http://cloud-tour.github.io/tags/debian/"},{"name":"java11","slug":"java11","permalink":"http://cloud-tour.github.io/tags/java11/"}]},{"title":"proteus8.9与keil4的简单使用","slug":"proteus8.9与keil4的简单使用","date":"2022-10-07T13:45:21.243Z","updated":"2022-10-07T14:28:39.430Z","comments":true,"path":"2022/10/07/proteus8.9与keil4的简单使用/","link":"","permalink":"http://cloud-tour.github.io/2022/10/07/proteus8.9%E4%B8%8Ekeil4%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","excerpt":"","text":"[toc] 最近博主在学51单片机，因此接触到proteus和keil，本篇文章简单介绍一下这两款软件的使用以及如何结合起来，并用这两款软件简单操作一个51单片机实现一个LED灯的闪烁操作。 keil4 ​ 对于keil4，简单来说就是用来写代码的，将代码编译后生成**.hex**文件，将此文件烧入单片机中便可驱动单片机实现某些特定的操作。 创建工程 新建项目 new一个project 选择Atmel下的AT89C51（即51单片机） 下面这个是提醒是否使用汇编，直接选择否，因为我们使用的是c语言 新建.c文件 接下来new一个file文件 ctrl+s保存为.c文件 将该c文件假如到project中 配置项目编译时自动生成.hex文件 编写功能代码 编写代码 下面写的代码是控制p2的0口间隔输出高低电平 1234567891011121314151617181920212223#include&quot;reg51.h&quot;//代表p2口的0处sbit LED0=P2^0;//睡眠函数void sleep(unsigned int n)&#123; unsigned int i = 0,j=0; for(i=0;i&lt;n;i++)&#123; for(j=0;j&lt;120;j++); &#125;&#125;void main()&#123; while(1)&#123; //输出低电平 LED0=0; sleep(5); //输出高电平 LED0=1; sleep(5); &#125;&#125; 写完编译 无error即可 观察项目路径，已生成,hex文件 proteus8.9 ​ 对于proteus来说，可简单理解为是一个用来对单片机仿真的软件，可以用.hex文件驱动单片机进行特定的功能并显现出来。 创建工程 new一个新的工程，不用选择什么，一直下一步直至完成 制作仿真图 添加元器件仓库 添加AT89C51 选中双击即可 同理添加RES和LED-BIBY 置放元器件（单击后即可在图中置放元器件）添加一个电源 连线 修改电阻值（电阻的值必须大于250，否则可能出错） 烧入程序 将.hex文件烧入程序中。双击单片机 烧入成功后即可仿真 end ​ 这样就利用proteus和keil完成一个简单的单片机电路，实现对led控制闪烁。","categories":[{"name":"51单片机","slug":"51单片机","permalink":"http://cloud-tour.github.io/categories/51%E5%8D%95%E7%89%87%E6%9C%BA/"}],"tags":[{"name":"proteus","slug":"proteus","permalink":"http://cloud-tour.github.io/tags/proteus/"},{"name":"keil","slug":"keil","permalink":"http://cloud-tour.github.io/tags/keil/"}]}],"categories":[{"name":"kafka","slug":"kafka","permalink":"http://cloud-tour.github.io/categories/kafka/"},{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/categories/java/"},{"name":"redis","slug":"redis","permalink":"http://cloud-tour.github.io/categories/redis/"},{"name":"mysql","slug":"mysql","permalink":"http://cloud-tour.github.io/categories/mysql/"},{"name":"jvm","slug":"jvm","permalink":"http://cloud-tour.github.io/categories/jvm/"},{"name":"juc","slug":"juc","permalink":"http://cloud-tour.github.io/categories/juc/"},{"name":"javase","slug":"javase","permalink":"http://cloud-tour.github.io/categories/javase/"},{"name":"linux","slug":"linux","permalink":"http://cloud-tour.github.io/categories/linux/"},{"name":"51单片机","slug":"51单片机","permalink":"http://cloud-tour.github.io/categories/51%E5%8D%95%E7%89%87%E6%9C%BA/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://cloud-tour.github.io/tags/kafka/"},{"name":"java","slug":"java","permalink":"http://cloud-tour.github.io/tags/java/"},{"name":"juc","slug":"juc","permalink":"http://cloud-tour.github.io/tags/juc/"},{"name":"jvm","slug":"jvm","permalink":"http://cloud-tour.github.io/tags/jvm/"},{"name":"ssm","slug":"ssm","permalink":"http://cloud-tour.github.io/tags/ssm/"},{"name":"redis","slug":"redis","permalink":"http://cloud-tour.github.io/tags/redis/"},{"name":"mysql","slug":"mysql","permalink":"http://cloud-tour.github.io/tags/mysql/"},{"name":"ssb","slug":"ssb","permalink":"http://cloud-tour.github.io/tags/ssb/"},{"name":"debian","slug":"debian","permalink":"http://cloud-tour.github.io/tags/debian/"},{"name":"nginx","slug":"nginx","permalink":"http://cloud-tour.github.io/tags/nginx/"},{"name":"java11","slug":"java11","permalink":"http://cloud-tour.github.io/tags/java11/"},{"name":"proteus","slug":"proteus","permalink":"http://cloud-tour.github.io/tags/proteus/"},{"name":"keil","slug":"keil","permalink":"http://cloud-tour.github.io/tags/keil/"}]}